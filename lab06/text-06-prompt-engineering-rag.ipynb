{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanhnv2303/polio_llm/blob/main/lab06/text-06-prompt-engineering-rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXLBEDJtu3nI"
      },
      "source": [
        "## Exercise 1: Prompt Engineering\n",
        "\n",
        "Let's consider LLAMA as our starting point. In the following, we see a typical prompt feeding and text generation with LLAMA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "token = userdata.get('hg_key')\n",
        "login(token=token)"
      ],
      "metadata": {
        "id": "nuIWiK-0wkCC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/thanhnv2303/polio_llm/11919c6e1f59e4034b0fa1ea3d8327eeb05fd33f/lab06/example2.pdf -O example.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRCM-qjqxzzH",
        "outputId": "2a72ebb3-9a07-40d8-e656-f4f60661e6c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-19 11:02:11--  https://raw.githubusercontent.com/thanhnv2303/polio_llm/11919c6e1f59e4034b0fa1ea3d8327eeb05fd33f/lab06/example2.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29327 (29K) [application/octet-stream]\n",
            "Saving to: ‘example.pdf’\n",
            "\n",
            "example.pdf         100%[===================>]  28.64K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-11-19 11:02:12 (132 MB/s) - ‘example.pdf’ saved [29327/29327]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/thanhnv2303/polio_llm.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xgv72qFzSQW",
        "outputId": "63127b9c-c7ee-43f4-c23f-cdb3a42d1c2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'polio_llm'...\n",
            "remote: Enumerating objects: 425, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 425 (delta 40), reused 44 (delta 33), pack-reused 356 (from 1)\u001b[K\n",
            "Receiving objects: 100% (425/425), 19.11 MiB | 15.88 MiB/s, done.\n",
            "Resolving deltas: 100% (187/187), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai pymupdf faiss-cpu scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x6fllWOyCJe",
        "outputId": "c0b6de79-af58-4a21-af6c-9e65424dca17"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf, faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.0 pymupdf-1.26.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804,
          "referenced_widgets": [
            "6860ea42986f413c8a5b93527bff4fb9",
            "eacdb2d59149466fade7a2d3fba63d67",
            "8012e0d489a54bf79122b6cdb26b08e9",
            "d027f2d320754793800c54a0cb423c37",
            "eb686a80e527485f9c6fb29b52fe895e",
            "e3069f4cf4d34353b421ec054c356e3c",
            "26fa74a01b5e4161b9131e91e2e131d1",
            "536d443e03cf487c8a64d466111adef5",
            "8a61ccc15ce24a04b8ca1282d7a5173e",
            "a05c8e9689254c198ade062316f973dc",
            "f4ba8faf11f8449a97da39555203944d",
            "ffbf91d8e91c4c279183430209fd1bde",
            "9369d314368b4e16b2a68df3e0c4eb7f",
            "fc001ba4a1954e2bb96c70a3aa71ce89",
            "dca3fa0d7b5a485c8346a1ae45c06fe9",
            "e7eba09b70d74f72b924e52f798fbc7f",
            "c95287884cde46869063b050cf8fe756",
            "42f8220280e740c4b48d7c0de4063bbf",
            "b879fdefefd247cc8dc609a1833382da",
            "fdc282f8d7814007a193c2fc1e3d3920",
            "06a812d974c24ed5aaef7427347ccbd3",
            "3c2f840876234229b1b7be06d2323835",
            "2498c8a6fc694e3a941101cb9914ac5a",
            "cdc9e721b69d4a7f82940773a1ec9bd4",
            "8b2845493b60465eb74248ac0b843e62",
            "883ca4415f274ca0ae47a8576bdc93fc",
            "e83b81d3db0f4adab79daea9d2ab7af9",
            "564c8eb9160e46afb895cb29778ce657",
            "78370d2507124e20ab1b8e5d38d08d1b",
            "9411c360a26c4e91b8a8289b14549f08",
            "2023172a76614616b0c07be2355b6fb5",
            "274500f519a34775b461638a90001d60",
            "3844a75cdbaf4d9c9310138611f49422",
            "7e564a1e26d3401a8113448ee064af9e",
            "9a103b5c0cd34c81935722b3dc8f3f5a",
            "219e191ab5ef4ba192b8dc4dbdfc05f6",
            "f5ec43deac6b4051827acf46a8c4bc20",
            "ae33c01e553d42d18e10f4a456962b7c",
            "c795d4b9c84f4eab82d214bf71e6c615",
            "3f05c13c04e344da86230acd72439996",
            "ff14f785c1d7436cae5e76f95d51b1b9",
            "66ef8c549722466f8f09de9cae1f8ea5",
            "f0156862e6324271846af87976d97437",
            "b5f723174cec4d8cbec2bd6e83f3d9b8",
            "68750819911c44e1bc078a5a6e63d6a3",
            "380bac766a1e4d12aacb53565819dbba",
            "77dc678e8aac47cd8be1d16df3585541",
            "f285ac43b6c0464faa6b5bb0f3380b05",
            "2551ceeac64742769ed649003e87df3f",
            "0b9f04d03b4443ea914756a05c30b2af",
            "6fa4b183ab2c4748a597a2169ce5d991",
            "09b7e1a74db74a96948604401af8982a",
            "0ee9048a83eb49adb7e2d7b03f65725a",
            "8ee494d3a4e04abe909cabc5752b0877",
            "fd5fb4b5bd5d4e9eaf555fbbd8e8ac79",
            "5a11e8e269f345788386a95c3dc26bc0",
            "7071b56892284b6fbd4a1f6c0393132c",
            "cc539511904d43a38c65484d3b45912a",
            "111cb8d96e7c411ab41dc074580430a6",
            "477950761c0a48e596d07b4bb9b1847a",
            "11b67898018245db9fd0d9625ea640da",
            "b4332756b86643e88b4863cb76eef863",
            "f716ff3d5a324a03a722e2f3eecd807a",
            "bc54e1a5e2aa4c2d97e0363306329d21",
            "10834c496b9b42539348efe0f1e1d53b",
            "1a0c83f8df404dcba59b657c7edbf397"
          ]
        },
        "id": "9rxwuPxbu3nK",
        "outputId": "5d64fbde-3e1f-466e-c7d0-127be07b35d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6860ea42986f413c8a5b93527bff4fb9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffbf91d8e91c4c279183430209fd1bde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2498c8a6fc694e3a941101cb9914ac5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e564a1e26d3401a8113448ee064af9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68750819911c44e1bc078a5a6e63d6a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a11e8e269f345788386a95c3dc26bc0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: System: You are an expert on world capitals.\n",
            "Respond with only the capital city of the given country. Do not repeat the question.\n",
            "\n",
            "Query: What is the capital of France?\n",
            "Answer:\n",
            "Paris\n",
            "\n",
            "Query: What is the capital of the USA?\n",
            "Answer:\n",
            "Washington DC\n",
            "\n",
            "Query: What is the capital of Russia?\n",
            "Answer:\n",
            "Moscow\n",
            "\n",
            "Query: What is the capital of China?\n",
            "Answer:\n",
            "Beijing\n",
            "\n",
            "Query: What is the capital of India?\n",
            "Answer:\n",
            "New Delhi\n",
            "\n",
            "Query\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Assuming model and tokenizer are already loaded\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to the device (GPU if available)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# Input prompt - Make it clear that you want only the direct answer without any explanations or options\n",
        "prompt = \"\"\"\n",
        "System: You are an expert on world capitals.\n",
        "Respond with only the capital city of the given country. Do not repeat the question.\n",
        "\n",
        "Query: What is the capital of France?\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "# Generate a response\n",
        "output = model.generate(\n",
        "    inputs['input_ids'],  # Tokenized input\n",
        "    max_length=100,         # Limit response length to avoid extra text\n",
        "    temperature=0.7,        # Lower temperature to reduce randomness\n",
        "    do_sample=True,        # Disable sampling for deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id  # Ensure the model doesn't go beyond the end token\n",
        "\n",
        ")\n",
        "\n",
        "# Decode the response into human-readable text\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "answer = response.split(\"query:\")[-1].strip()\n",
        "print(\"Response:\", answer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B84-f7uVu3nM"
      },
      "source": [
        "### Fitz\n",
        "\n",
        "Reference libraries to install: pip install openai pymupdf faiss-cpu scikit-learn\n",
        "\n",
        "PyMuPDF is a Python library that provides tools for working with PDF files (as well as other document formats like XPS, OpenXPS, CBZ, EPUB, and FB2). It's built on the MuPDF library, a lightweight, high-performance PDF and XPS rendering engine. With PyMuPDF, you can perform various tasks like reading, creating, editing, and extracting content from PDFs, images, and annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuA84nvGu3nM"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "\n",
        "#open an example pdf\n",
        "doc = fitz.open(\"example.pdf\")\n",
        "\n",
        "# Extract text from the first page\n",
        "page = doc.load_page(0)\n",
        "text = page.get_text(\"text\")  # Use 'text' mode to get raw text\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA2amiJOu3nN"
      },
      "source": [
        "### Example: Text Summarization\n",
        "\n",
        "Let's ask LLAMA to perform a summarization of the example PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rv3PNFdu3nN"
      },
      "outputs": [],
      "source": [
        "#define the prompt to ask for text summarization.\n",
        "text_summarization_prompt = \"\"      #define your prompt here\n",
        "text = \"\"                           #load here the FULL text of the article\n",
        "p1 =  \"\"\"{PROMPT}. article: {BODY}\"\"\".format(PROMPT=text_summarization_prompt, BODY=text)\n",
        "\n",
        "#feed the prompt to llama\n",
        "#print the result of text summarization into bullets\n",
        "\n",
        "r1 = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3ANWBHIu3nO"
      },
      "source": [
        "### Adding a System Prompt\n",
        "\n",
        "Llama was trained with a system message that set the context and persona to assume when solving a task. One of the unsung advantages of open-access models is that you have full control over the system prompt in chat applications. This is essential to specify the behavior of your chat assistant –and even imbue it with some personality–, but it's unreachable in models served behind APIs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09KijUzju3nP"
      },
      "outputs": [],
      "source": [
        "#default standard system message from the Hugging Face blog to the prompt from above\n",
        "system_prompt = \"<<SYS>> You are a helpful, respectful and honest assistant. \\\n",
        "    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n",
        "    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n",
        "    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n",
        "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
        "    please don't share false information. <</SYS>>\"\n",
        "\n",
        "#concatenate the system prompt with your pront and get the response\n",
        "p2 = \"\"\n",
        "\n",
        "r2 = \"\"\n",
        "\n",
        "#what changes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSNFgFOau3nP"
      },
      "source": [
        "### Customizing the System prompt\n",
        "\n",
        "With Llama we have full control over the system prompt. The following experiment will instruct Llama to assume the persona of a researcher tasked with writing a concise brief.\n",
        "\n",
        "Apply the following changes the original system prompt:\n",
        "- Use the researcher persona and specify the tasks to summarize articles.\n",
        "- Remove safety instructions; they are unnecessary since we ask Llama to be truthful to the article.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNNSIGnBu3nP"
      },
      "outputs": [],
      "source": [
        "new_system_prompt = \"\"\n",
        "\n",
        "p3 = \"\"\n",
        "\n",
        "r3 = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6BEqoAUu3nP"
      },
      "source": [
        "### Chain-of-Thought prompting\n",
        "\n",
        "Chain-of-thought is when a prompt is being constructed using a previous prompt answer. For our use case to extract information from text, we will first ask Llama what the article is about and then use the response to ask a second question: what problem does [what the article is about] solve?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5rISc1Wu3nQ"
      },
      "outputs": [],
      "source": [
        "#define a prompt to ask what the article is about\n",
        "\n",
        "p4 = \"\"\n",
        "\n",
        "r4 = \"\"\n",
        "\n",
        "#now embed the result of the previous prompt in a new prompt to ask what that solves\n",
        "\n",
        "p5 = \"\"\n",
        "\n",
        "r5 = \"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5asYkDMu3nQ"
      },
      "source": [
        "### Generating JSONs with Llama\n",
        "\n",
        "Llama needs precise instructions when asking it to generate JSON. In essence, here is what works for me to get valid JSON consistently:\n",
        "\n",
        "- Explicitly state — “ All output must be in valid JSON. Don’t add explanation beyond the JSON” in the system prompt.\n",
        "- Add an “explanation” variable to the JSON example. Llama enjoys explaining its answers. Give it an outlet.\n",
        "- Use the JSON as part of the instruction. See the “in_less_than_ten_words” example below.\n",
        "Change “write the answer” to “output the answer.”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXozuRAMu3nQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#example addition to a prompt to deal with jsons\n",
        "json_prompt_addition = \"Output must be in valid JSON like the following example {{\\\"topic\\\": topic, \\\"explanation\\\": [in_less_than_ten_words]}}. Output must include only JSON.\"\n",
        "\n",
        "#now generate a prompt by correctly concatenating the system prompt, the json prompt instruction, and an article\n",
        "p6 = \"\"\n",
        "\n",
        "r6 = \"\"\n",
        "\n",
        "#compare the difference between the prompt with the formatting instruction and a regular prompt without formatting instructions. is there any difference?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inMoY3tBu3nQ"
      },
      "source": [
        "### One-to-Many Shot Learning Prompting\n",
        "\n",
        "One-to-Many Shot Learning is a term that refers to a type of machine learning problem where the goal is to learn to recognize many different classes of objects from only one or a few examples of each class. For example, if you have only one image of a cat and one image of a dog, can you train a model to distinguish between cats and dogs in new images? This is a challenging problem because the model has to generalize well from minimal data (source)\n",
        "\n",
        "Important points about the prompts:\n",
        "\n",
        "- The system prompt includes the instructions to output the answer in JSON.\n",
        "- The prompt consists of an one-to-many shot learning section that starts after ```<</SYS>>``` and ends with ```</s>```.  See the prompt template below will make it easier to understand.\n",
        "- The examples are given in JSON because the answers need to be JSON.\n",
        "- The JSON allows defining the response with name, type, and explanation.\n",
        "- The prompt question start with the second ```<s>[INST]``` and end with the last ```[/INST]```\n",
        "\n",
        "```\n",
        "<s>[INST] <<SYS>>\n",
        "SYSTEM MESSAGE\n",
        "<</SYS>>\n",
        "EXAMPLE QUESTION [/INST]\n",
        "EXAMPLE ANSWER(S)\n",
        "</s>\n",
        "<s>[INST]  \n",
        "QUESTION\n",
        "[/INST]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6opENedhu3nQ"
      },
      "outputs": [],
      "source": [
        "#describe all the main nouns in the example.pdf article\n",
        "\n",
        "#use the following addition for one-to-many prompting exampling\n",
        "nouns = \"\"\"[\\\n",
        "{{\"name\": \"semiconductor\", \"type\": \"industry\", \"explanation\": \"Companies engaged in the design and fabrication of semiconductors and semiconductor devices\"}},\\\n",
        "{{\"name\": \"NBA\", \"type\": \"sport league\", \"explanation\": \"NBA is the national basketball league\"}},\\\n",
        "{{\"name\": \"Ford F150\", \"type\": \"vehicle\", \"explanation\": \"Article talks about the Ford F150 truck\"}},\\\n",
        "{{\"name\": \"Ford\", \"type\": \"company\", \"explanation\": \"Ford is a company that built vehicles\"}},\\\n",
        "{{\"name\": \"John Smith\", \"type\": \"person\", \"explanation\": \"Mentioned in the article\"}},\\\n",
        "]\"\"\"\n",
        "\n",
        "#now build the prompt following the template described above\n",
        "p7 = \"\"\n",
        "\n",
        "r7 = \"\"\n",
        "\n",
        "#compare the response of the prompt described above and a zero-shot prompt. Are there any differences?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znV_hdEju3nQ"
      },
      "source": [
        "## Exercise 2: RAG (Retrieval-Augmented-Generation)\n",
        "\n",
        "RAG (Retrieval-Augmented Generation) is a powerful framework in Natural Language Processing (NLP) that enhances the performance of language models by combining traditional generative models with external knowledge retrieval. This hybrid approach allows models to retrieve relevant information from a large corpus (like a database or document collection) and incorporate this information into the generation process. It is particularly useful when a model needs to answer questions, generate content, or provide explanations based on real-time or domain-specific data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obe-hfOmu3nQ",
        "outputId": "7a1c0b49-301f-4f7b-aa23-3922fbecfbf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text from paper6.pdf:\n",
            "[b'Large Language Model (LLM)-enabled In-context\\nLearning for Wireless Network Optimization: A\\nCase Study of Power Control\\nHao Zhou, Chengming Hu, Dun Yuan, Ye Yuan, Di Wu,\\nXue Liu, Fellow, IEEE, and Charlie Zhang, Fellow, IEEE.\\nAbstract\\xe2\\x80\\x94Large language model (LLM) has recently been\\nconsidered a promising technique for many fields. This work\\nexplores LLM-based wireless network optimization via in-context\\nlearning. To showcase the potential of LLM technologies, we\\nconsider the base station (BS) power control as a case study,\\na fundamental but crucial technique that is widely investigated\\nin wireless networks. Different from existing machine learning\\n(ML) methods, our proposed in-context learning algorithm relies\\non LLM\\xe2\\x80\\x99s inference capabilities. It avoids the complexity of\\ntedious model training and hyper-parameter fine-tuning, which is\\na well-known bottleneck of many ML algorithms. Specifically, the\\nproposed algorithm first describes the target task via formatted\\nnatural language, and then designs the in-context learning\\nframework and demonstration examples. After that, it considers\\ntwo cases, namely discrete-state and continuous-state problems,\\nand proposes state-based and ranking-based methods to select\\nappropriate examples for these two cases, respectively. Finally, the\\nsimulations demonstrate that the proposed algorithm can achieve\\ncomparable performance as conventional deep reinforcement\\nlearning (DRL) techniques without dedicated model training or\\nfine-tuning. Such an efficient and low-complexity approach has\\ngreat potential for future wireless network optimization.\\nIndex Terms\\xe2\\x80\\x94Large language model, in-context learning, net-\\nwork optimization, transmission power control\\nI. INTRODUCTION\\nFrom LTE and 5G to envisioned 6G, wireless networks are\\nincreasingly complicated with diverse application scenarios\\nand novel signal processing and transmission techniques, e.g.,\\nunmanned aerial vehicle (UAV), vehicle-to-everything (V2X),\\nmmWave and THz networks, reconfigurable intelligent sur-\\nface, etc [1]. The constantly evolving network architecture re-\\nquires more efficient management schemes, and most existing\\nnetwork optimization methods can be summarized into two\\nmain approaches: convex optimization and machine learning\\n(ML) algorithms. Specifically, convex optimization usually\\nneeds dedicated problem formulation for each specific task,\\nthen transforms the objective function or constraints into con-\\nvex forms. By contrast, ML algorithms, such as reinforcement\\nlearning, have lower requirements for problem formulations,\\nbut the tedious model training and fine-tuning indicate a large\\nHao\\nZhou,\\nChengming\\nHu,\\nDun\\nYuan,\\nYe\\nYuan,\\nand\\nXue\\nLiu\\nare with the School of Computer Science, McGill University, Mon-\\ntreal, QC H3A 0E9, Canada. (mails:hao.zhou4, chengming.hu, dun.yuan,\\nye.yuan3@mail.mcgill.ca, xueliu@cs.mcgill.ca). Di Wu is with the School\\nof Electrical and Computer Engineering, McGill University, Montreal,\\nQC H3A 0E9, Canada. (email: di.wu5@mcgill.ca). Charlie Zhang is\\nwith Samsung Research America, Plano, Texas, TX 75023, USA. (email:\\njianzhong.z@samsung.com).\\nnumber of iterations [2]. Therefore, these potential issues, e.g.,\\nproblem-specific transformation and relaxation, hyperparame-\\nter tuning, and long training iterations, have become obstacles\\nto further improve the efficiency of next-generation networks.\\nLarge language models (LLMs) are recently considered\\nrevolutionary technologies that have been successfully ap-\\nplied to education, finance, healthcare, biology, etc [3]. The\\ngreat potential of LLM technologies also provides promis-\\ning opportunities for network management and optimization\\n[4]. For instance, a promising feature of LLMs is learning\\nfrom language-based descriptions and demonstrations, which\\nis known as in-context learning. Compared with convex opti-\\nmization or conventional ML approaches, in-context learning\\nhas multiple advantages [5]: 1) in-context learning relies on\\nLLM\\xe2\\x80\\x99s inference process, and it avoids the complexity of\\ndedicated model training and fine-tuning, which is a well-\\nknown bottleneck for many ML techniques; 2) in-context\\nlearning allows natural language-based task design and imple-\\nmentation, and the operator can easily formulate the target task\\nusing human language and instructions. Such a user-friendly\\napproach can also significantly lower the requirements for\\nprofessional knowledge when solving specific tasks.\\nLLM-enabled wireless networks have recently attracted\\nconsiderable interest, e.g., 6G edge intelligence [6], network\\nintrusion detection and LLM-enhanced reconfigurable intelli-\\ngent surface for internet of vehicles (IoV) [7], [8]. By contrast,\\nthis work focuses on optimization problems, and it considers\\nbase station (BS) power control as a case study, exploring\\nthe potential of using LLM to solve optimization problems.\\nThe power control of cellular networks has been extensively\\nstudied with diverse objectives and algorithms, i.e., convex\\noptimization, game theory, reinforcement learning [9], etc.\\nThese studies prove that power control is a fundamental\\nand critical technique to improve energy efficiency, reduce\\ninterference, and save power consumption [10]. Therefore,\\ngiven such crucial importance, we select BS power control\\nas a case study, and explore the potential of state-of-the-art\\nLLM technologies for wireless network optimization.\\nIn particular, our proposed LLM-enabled in-context learning\\nalgorithm first designs a natural language-based task descrip-\\ntion, i.e., task goal, definition, and rules. The formatted task\\ndescription, along with a set of selected examples, will become\\nthe prompt input for the LLM model. Then, the LLM model\\ncan utilize the task description and advisable examples to gen-\\narXiv:2408.00214v1  [eess.SY]  1 Aug 2024\\n', b'erate a decision based on the current environment state. After\\nthat, the output decision, environment state, and corresponding\\nsystem performance will become a new example and be stored\\nin an experience pool. Given the next environment state, we\\nwill select new advisable examples from the experience pool,\\nserving as references for the next LLM decision-making. In\\naddition, we further consider two scenarios, namely discrete-\\nstate and continuous-state problems, and propose state-based\\nand ranking-based example selection methods for these two\\ncases, respectively. Finally, we evaluate the proposed algorithm\\nwith various LLMs, e.g., Llama3-8b-instruct, Llama3-70b-\\ninstruct, and GPT-3.5 turbo, and the simulations prove that\\nthe proposed algorithm can achieve satisfactory performance.\\nThe main contribution of this work is that we explored\\nLLM-enabled in-context learning for wireless network op-\\ntimization problems. Specifically, it proposed an in-context\\noptimization technique that learns from the environment inter-\\nactions without model training and fine-tuning. The simulation\\nresults show that our proposed algorithm enables LLM to\\nlearn from previous explorations and examples, and constantly\\nimprove the performance on target tasks. Such an efficient and\\ntraining-free algorithm has great potential for wireless network\\noptimization and management.\\nII. SYSTEM MODEL\\nThis section introduces a BS power minimization problem,\\nserving as a case study of the proposed in-context learning\\nalgorithm1. Considering a BS with Ub users, the achievable\\ndata rate Cb,u between BS b and user u is defined by [11]\\nCb,u =\\nKb\\nP\\nk=1\\ndklog(1 +\\npb,khb,k,u\\xce\\xb3b,k,u\\nP\\nb\\xe2\\x80\\xb2\\xe2\\x88\\x88B\\xe2\\x88\\x92b\\npb\\xe2\\x80\\xb2,k\\xe2\\x80\\xb2hb\\xe2\\x80\\xb2,k\\xe2\\x80\\xb2,u\\xe2\\x80\\xb2\\xce\\xb3b\\xe2\\x80\\xb2,k\\xe2\\x80\\xb2,u\\xe2\\x80\\xb2+dkN0 ), (1)\\nwhere Kb is the total number of resource blocks (RBs) in BS\\nb, dk is the bandwidth of RB k, pb,k indicates the transmission\\npower of BS b on RB k, hb,k,u defines the channel gain\\nbetween BS b and user u on RB k, and N0 is the noise\\npower density. For the RB allocation, \\xce\\xb3b,k,u \\xe2\\x88\\x88{0, 1} indicates\\nwhether RB k is allocated to the transmission for user u.\\nFor the interference, B\\xe2\\x88\\x92b represent the set of adjacent BSs\\nexcept for BS b, pb\\xe2\\x80\\xb2,k\\xe2\\x80\\xb2hb\\xe2\\x80\\xb2,k\\xe2\\x80\\xb2,u\\xe2\\x80\\xb2\\xce\\xb3b\\xe2\\x80\\xb2,k\\xe2\\x80\\xb2,u\\xe2\\x80\\xb2 defines the inter-cell\\ninterference, and we assume orthogonal frequency-division\\nmultiplexing is applied to eliminate intra-cell interference.\\nThis work aims to minimize the BS transmission power and\\nmeanwhile satisfy the average data rate constraint [10]:\\nmin\\nPb\\nX\\nb\\xe2\\x88\\x88B Pb\\n(2)\\ns.t. 0 \\xe2\\x89\\xa4Pb \\xe2\\x89\\xa4Pmax,\\n(2a)\\nPb =\\nXKb\\nk=1 pb,k,\\n(2b)\\nXUb\\nu=1 Cb,u/Ub \\xe2\\x89\\xa5Cmin,\\n(2c)\\n1Various objectives and constraints can be defined for power control\\nproblems as summarized in [10], and here we select power minimization\\nas a specific problem formulation.\\nwhere Pb is the total transmission power of BS b and\\nPb = PKb\\nk=1 pb,k, pb,k has been defined in equation (1) as the\\ntransmission power of RB k, Pmax is the maximum power,\\nUb is the total number of users, and Cmin is the average\\nachievable data rate constraint. We assume Pb is equally\\nallocated to all RBs, and a proportional fairness method is used\\nfor RB allocation, which has been widely used as a classic\\napproach. Then we can better focus on LLM features.\\nThe control variable in equation (2) is the total BS transmis-\\nsion power Pb, which needs to be dynamically adjusted based\\non the wireless environment, e.g., current user numbers or\\nuser-BS distances, to save power consumption and meanwhile\\nmaintain the average data rate. Problem (2) has been exten-\\nsively investigated in existing studies, but this work differs\\nfrom previous works by presenting a unique view from the\\nperspective of natural language-based network optimization.\\nSpecifically, we propose a novel LLM-enabled in-context\\nlearning algorithm in the following Section III.\\nIII. IN-CONTEXT LEARNING-BASED OPTIMIZATION\\nALGORITHM\\nIn-context learning refers to the process that LLMs can learn\\nfrom formatted natural language such as task descriptions and\\ntask solution demonstrations, to improve the performance on\\ntarget tasks. In-context learning can be defined as [5]\\nDtask \\xc3\\x97 Et \\xc3\\x97 st \\xc3\\x97 LLM \\xe2\\x87\\x92at,\\n(3)\\nwhere Dtask is the task description, Et is the set of examples\\nat time t, st is the environment state at time t that is associated\\nwith the target task , LLM indicates the LLM model, and at\\nis the LLM output. For a sequential decision-making problem,\\nwe expect the LLM can utilize the initial task description\\nDtask, learn from the example set Et, and then make decision\\nat based on current environment state st of the target task. In\\nthe following, we will introduce the design of task description\\nDtask and the selection of example set Et.\\nA. Language-based Task Description\\nDtask is crucial to provide target task information to\\nthe LLM model. In particular, it involves \\xe2\\x80\\x9cTask goal\\xe2\\x80\\x9d,\\n\\xe2\\x80\\x9cTask definition\\xe2\\x80\\x9d, and extra \\xe2\\x80\\x9cRules\\xe2\\x80\\x9d. The following is a\\ndetailed task description we designed to prompt the LLM.\\nTask description for BS transmission power control\\nTask goal: You have a decision-making task for base\\nstation power control, and you need to select between\\n4 power levels from 1 to 4.\\nTask definition: You have to consider the specific user\\nnumber of each case, which is the \\xe2\\x80\\x9cbase station user\\nnumber\\xe2\\x80\\x9d.\\nFollowing are some examples {Example set}.\\nNow I will give you a new condition to solve, the\\ncurrent BS user number is {Num BS user}.\\nRules: Now please select from \\xe2\\x80\\x9clevel 1\\xe2\\x80\\x9d, \\xe2\\x80\\x9clevel 2\\xe2\\x80\\x9d,\\n\\xe2\\x80\\x9clevel 3\\xe2\\x80\\x9d, and \\xe2\\x80\\x9clevel 4\\xe2\\x80\\x9d based on the above examples.\\n2\\n', b'Fig. 1: Overall design of the proposed LLM-enabled in-context learning for transmission power control.\\nIn particular, the Task goal first specifies a \\xe2\\x80\\x9cdecision-\\nmaking task for base station power control\\xe2\\x80\\x9d, and the goal is to\\n\\xe2\\x80\\x9cselect between 4 power levels\\xe2\\x80\\x9d2. Then the Task definition\\nintroduces the environment states we need to consider. For\\nexample, this work assumes the total user numbers may change\\ndynamically, and then the LLM has to consider the \\xe2\\x80\\x9cbase\\nstation user number\\xe2\\x80\\x9d of each case. In addition, it means that\\nthe environment state st in equation (3) refers to the total user\\nnumber Ub in problem (2). After that, the example set Et is\\nincluded by \\xe2\\x80\\x9cFollowing are some examples....\\xe2\\x80\\x9d, and we provide\\na new condition for the LLM to solve, which is associated\\nwith the current user number Ub. Finally, we set extra reply\\nrules such as \\xe2\\x80\\x9cselect from ... based on the above examples\\xe2\\x80\\x9d,\\nindicating the LLM to focus on the decision-making process.\\nThe above task description provides a template to define\\noptimization tasks by formatted natural language, avoiding the\\ncomplexity of dedicated optimization model design. It is also\\nuser-friendly since the operator can easily add or remove task\\ndescriptions without requiring any professional knowledge of\\noptimization techniques.\\nB. In-context Learning Framework and Example Design\\nExamples are of great importance in in-context learning,\\nand they must be carefully selected because: 1) examples serve\\nas crucial references for LLM decision-making, which means\\nthe LLM relies on examples to justify its decision; 2) due\\nto the LLM context window size constraint3, it is impractical\\nto send a large number of examples to the LLM. Moreover,\\nthere are many optimization problems with continuous envi-\\nronment states, which are very common in wireless networks,\\ne.g., adjusting the BS transmission power based on user-BS\\ndistance. Such cases mean that there may be an infinite number\\nof examples, and therefore identifying the most relevant and\\n2Here we select 4 power levels as an example, which can be changed to\\nany number of levels\\n3The context window size indicates the largest number of tokens that can\\nbe sent to the LLM\\nuseful examples becomes challenging. This work defines an\\nexample E by\\nE = {s, a, r(s, a)}, E \\xe2\\x88\\x88E\\n(4)\\nwhere s and a are environment state and decision, respectively.\\nInspired by reinforcement learning, we further define a reward\\nvalue to evaluate the decision a by\\nr = Ptarget \\xe2\\x88\\x92Pb \\xe2\\x88\\x92\\xce\\xb2\\n(5)\\nwhere Ptarget is a target power consumption, and Pb has been\\ndefined in problem (2) as the total power consumption of BS\\nb. \\xce\\xb2 is a penalty term, which is only applied when constraint\\n(2c) is not satisfied. Then, r provides a comprehensive metric\\nto evaluate the selected decision a under environment state s.\\nFig.1 shows the overall design of the proposed in-context\\nlearning algorithm for transmission power control. Specifi-\\ncally, the above task description Dtask, current environment\\nstate st, and selected examples Et are integrated as input\\nprompt as defined in equation (3), and then the LLM model\\nwill generate a power control decision at based on st and\\nthe experiences in Et. Then, the decision at is implemented,\\nthe achieved data rate Cb,u is collected, and the reward rt is\\ncalculated as equation (5). Et = {st, at, rt(st, at)} becomes\\na new example in the accumulated experience pool Epool in\\nFig.1. After that, based on the next environment state st+1, a\\nnew example set Et+1 is selected, and the selected examples\\nare inserted into the task description with st+1, becoming a\\nnew prompt for the LLM model to generate at+1.\\nC. State-based Example Selection for Discrete State Problems\\nSelecting appropriate examples is critical for in-context\\nlearning since the LLM model learns from existing demon-\\nstrations to handle the target task. For problems with discrete\\nenvironment states, relevant demonstrations can be easily\\nidentified by finding existing examples with the same states in\\nthe accumulated experience pool Epool. Considering a target\\n3\\n', b'task with environment state value starget, the set of relevant\\nexamples can be identified by\\nErelevant =\\nn\\nE{s, a, r(s, a)}\\n\\x0c\\x0c\\x0cs = starget, E \\xe2\\x88\\x88Epool\\no\\n(6)\\nwhere Epool is the accumulated experience pool in Fig. 2.\\nGiven Erelevant, we can easily select recommended exam-\\nples with high reward, i.e., top-K examples, and inadvisable\\nexamples, e.g., examples with lower reward or violating the\\nminimum data rate constraint.\\nIn addition, we include a well-known epsilon-greedy policy\\nto balance exploration and exploitation.\\na =\\n\\x1a Random action selection,\\nif rand < \\xcf\\xb5;\\nLLM-based decision-making,\\nelse,\\n(7)\\nwhere \\xcf\\xb5 is a predefined value, and rand is a random number\\nbetween 0 and 1. Therefore, the random exploration in equa-\\ntion (7) can constantly explore new examples, and then the\\nLLM model can learn from better relevant examples Erelevant\\nto improve the performance.\\nD. Ranking-based Example Selection for Continuous State\\nProblems\\nCompared with discrete-state problems, environments with\\ncontinuous states can be much more complicated. For instance,\\nwhen using average user-BS distance as an environment state\\nfor BS transmission power control with a target task starget, it\\nis unlikely to find a specific existing example E{s, a, r(s, a)}\\nwith s = starget, since starget is a random number within\\nthe BS maximum coverage distance. This problem may be\\nsolved by discretizing the continuous states into some discrete\\nvalues, but this may still lead to a large number of states or\\nextra errors. To this end, we define a new metric L for example\\nselection with continuous states:\\nL(E, starget) = r(s, a) \\xe2\\x88\\x92\\xcf\\x84||s \\xe2\\x88\\x92starget||,\\n(8)\\nwhere L(E, starget) is a comprehensive metric to evaluate the\\nusefulness of E = {s, a, r(s, a)} to the decision-making of\\nstarget, and ||s\\xe2\\x88\\x92starget|| is the L2 norm to define the distance\\nbetween s and starget. Equation (8) aims to jointly consider\\nthe reward and states of example E, and \\xcf\\x84 is a weighting\\nfactor to balance the importance of higher reward r(s, a)\\nand more similar states between s and starget. Specifically,\\na higher reward r(s, a) indicates that E includes a good\\naction selection a under environment state s, and meanwhile\\nlower ||s \\xe2\\x88\\x92starget|| value means the environment state s in\\nE is more similar to starget. Therefore, we use L(E, starget)\\nas a comprehensive metric, and then the recommended and\\ninadvisable examples can be selected similarly as in Section\\nIII-C by using top-K methods.\\nE. Baseline Algorithm\\nWe consider deep reinforcement learning (DRL) as a base-\\nline since it is one of the most widely used ML algorithms to\\nsolve network optimization problems [12]. DRL can usually\\nproduce satisfactory optimization results, but it requires proper\\nparameter fine-tuning and dedicated model training. This work\\ninvestigates two scenarios, and the Markov decision processes\\n(MDPs) are defined by: 1) for discrete-state problems, the state\\nis defined by user numbers associated with the BS; 2) for\\ncontinuous-state problems, we use average user-BS distance\\nas a continuous-changing state. The action is defined by the\\nBS power level, and the reward is shown as equation (5).\\nIV. PERFORMANCE EVALUATION\\nA. Simulation Settings\\nWe consider three adjacent small base stations (SBSs); the\\nuser number of each SBS randomly changes from 5 to 15,\\nand the SBS\\xe2\\x80\\x99s coverage is 20 meters. The channel gain applies\\n3GPP urban network models, and 2 cases are evaluated:\\nCase I: Discrete states defined by user numbers of each SBS;\\nCase II: Continuous states defined by average user-SBS dis-\\ntance, which represents 2 kinds of network optimization prob-\\nlems. Then, the simulation considers two main approaches:\\n1) LLM-based method includes 3 models: Llama3-8b-\\ninstruct, Llama3-70b-instruct, and GPT-3.5 turbo. Llama3-8b\\nis a small-scale LLM, while Llama3-70b and GPT-3.5 turbo\\nare large models. Using LLM models with various sizes can\\nbetter evaluate the capabilities of our proposed algorithms.\\n2) DRL-based method as introduced in Section III-E. With\\ndedicated model training, here we consider DRL as an optimal\\nbaseline since its capability has been demonstrated in many\\nexisting studies [11], [12].\\nB. Simulation Results\\nFig. 2 shows the simulation results and comparisons, and\\nthe metrics include average reward, power consumption, and\\nservice quality (indicating the probability of satisfying the\\nminimum average data rate constraint defined in equation (2)).\\nFirstly, Fig. 2(a) presents the system reward and service\\nquality of different LLMs under discrete state space. One can\\nobserve that both Llama3 LLMs achieve a comparable reward\\nand service quality as the DRL baseline, while GPT-3.5 shows\\na lower reward and service quality. Fig. 2(a) demonstrates that\\nthe proposed in-context learning algorithm and state-based ex-\\nample selection method can provide satisfactory performance\\nfor problems with a limited number of environment states.\\nThen, we consider more complicated scenarios with contin-\\nuous states defined by the average user-BS distance. Fig. 2(b)\\nand 2(c) present the reward and average power consumption,\\nrespectively. All LLM models achieve higher rewards and\\nlower power consumption as the number of episodes increases\\nand finally converge to stable values. The results demonstrate\\nthat LLMs can learn from previous examples and explorations\\nand then improve the performance on target tasks.\\nIn addition, we observe the algorithm performance under\\ndifferent minimum data rate constraints. Fig. 2(d), 2(e), and\\n2(f) present the average reward, power consumption, and\\nservice quality, respectively. Here, every value in the results\\nis obtained by taking the average performance of converged\\nepisodes of corresponding LLMs as in Fig. 2(b) and 2(c).\\nAs expected, the simulation results show that increasing the\\nminimum data rate constraint leads to lower reward, lower\\n4\\n', b'(a) Discrete state space: System reward and\\nservice quality comparison of various LLMs.\\n(b) Continuous state space: System reward\\ncomparison of various LLMs.\\n(c) Continuous state space: Power consumption\\ncomparison of various LLMs.\\n(d) Continuous state space: Average reward com-\\nparison under different data rate constraints.\\n(e) Continuous state space: Average power\\nconsumption comparison under different\\ndata rate constraints.\\n(f) Continuous state space: Average service quality\\ncomparison under different data rate constraints.\\nFig. 2: Simulation results and comparisons\\nservice quality, and higher power consumption. Therefore, Fig.\\n2(d), 2(e), and 2(f) demonstrate that the proposed algorithms\\ncan adapt to different optimization settings and then adjust\\ntheir policies to improve the performance on target tasks.\\nNote that the algorithm performance is also related to\\nspecific LLMs. Llama3 represents state-of-the-art LLM de-\\nsigns, while GPT-3.5 is an early LLM model. Therefore,\\nit is reasonable that Llama3-8b and Llama3-70b maintain\\ncomparable performance as the DRL baseline, while GPT-\\n3.5 turbo presents a worse performance in different tasks. For\\ninstance, when the minimum data rate constraint is 2 Mbps per\\nuser, GPT-3.5 has a 25% lower reward and 20% lower service\\nquality than other LLMs. In summary, the simulations in Fig.\\n2 demonstrate that our proposed in-context learning algorithm\\ncan achieve comparable performance as conventional DRL\\nalgorithms without dedicated model training and fine-tuning.\\nV. CONCLUSION\\nLLM is a promising technique for future wireless networks,\\nand this work proposes an LLM-enabled in-context learning\\nalgorithm for BS transmission power control. The proposed\\nalgorithm can handle both discrete and continuous state prob-\\nlems, and the simulations show that it achieves comparable\\nperformance as conventional DRL algorithms. This work\\ndemonstrates the great potential of in-context learning for\\nhandling network management and optimization problems.\\nREFERENCES\\n[1] Z. Zhang, Y. Xiao, Z. Ma, M. Xiao, Z. Ding, X. Lei, G. K. Karagiannidis,\\nand P. Fan, \\xe2\\x80\\x9c6g wireless networks: Vision, requirements, architecture,\\nand key technologies,\\xe2\\x80\\x9d IEEE vehicular technology magazine, vol. 14,\\nno. 3, pp. 28\\xe2\\x80\\x9341.\\n[2] H. Zhou, M. Erol-Kantarci, Y. Liu, and H. V. Poor, \\xe2\\x80\\x9cA survey on model-\\nbased, heuristic, and machine learning optimization approaches in ris-\\naided wireless networks,\\xe2\\x80\\x9d IEEE Communications Surveys & Tutorials,\\n2023.\\n[3] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang,\\nJ. Zhang, Z. Dong et al., \\xe2\\x80\\x9cA survey of large language models,\\xe2\\x80\\x9d arXiv\\npreprint arXiv:2303.18223, 2023.\\n[4] H. Zhou, C. Hu, Y. Yuan, Y. Cui, Y. Jin, C. Chen, H. Wu, D. Yuan,\\nL. Jiang, D. Wu et al., \\xe2\\x80\\x9cLarge language model (LLM) for telecommu-\\nnications: A comprehensive survey on principles, key techniques, and\\nopportunities,\\xe2\\x80\\x9d arXiv preprint arXiv:2405.10825, 2024.\\n[5] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, and et al., \\xe2\\x80\\x9cA survey on\\nin-context learning,\\xe2\\x80\\x9d arXiv preprint arXiv:2301.00234, 2022.\\n[6] Z. Lin, G. Qu, Q. Chen, X. Chen, Z. Chen, and K. Huang, \\xe2\\x80\\x9cPushing large\\nlanguage models to the 6G edge: Vision, challenges, and opportunities,\\xe2\\x80\\x9d\\narXiv preprint arXiv:2309.16739, 2023.\\n[7] M. Fu, P. Wang, M. Liu, Z. Zhang, and X. Zhou, \\xe2\\x80\\x9cIov-bert-ids: Hybrid\\nnetwork intrusion detection system in iov using large language models,\\xe2\\x80\\x9d\\nIEEE Transactions on Vehicular Technology, 2024.\\n[8] Q. Liu, J. Mu, D. ChenZhang, Y. Liu, and T. Hong, \\xe2\\x80\\x9cLlm enhanced\\nreconfigurable intelligent surface for energy-efficient and reliable 6g\\niov,\\xe2\\x80\\x9d IEEE Transactions on Vehicular Technology, 2024.\\n[9] F. H. C. Neto, D. C. Ara\\xc2\\xb4ujo, M. P. Mota, T. F. Maciel, and A. L.\\nde Almeida, \\xe2\\x80\\x9cUplink power control framework based on reinforcement\\nlearning for 5g networks,\\xe2\\x80\\x9d IEEE Transactions on Vehicular Technology,\\nvol. 70, no. 6, pp. 5734\\xe2\\x80\\x935748, 2021.\\n[10] M. Chiang, P. Hande, T. Lan, C. W. Tan et al., \\xe2\\x80\\x9cPower control in\\nwireless cellular networks,\\xe2\\x80\\x9d Foundations and Trends\\xc2\\xae in Networking,\\nvol. 2, no. 4, pp. 381\\xe2\\x80\\x93533, 2008.\\n[11] H. Zhou, M. Erol-Kantarci, and H. V. Poor, \\xe2\\x80\\x9cLearning from peers:\\nDeep transfer reinforcement learning for joint radio and cache resource\\n5\\n', b'allocation in 5G RAN slicing,\\xe2\\x80\\x9d IEEE Transactions on Cognitive Com-\\nmunications and Networking, vol. 8, no. 4, pp. 1925\\xe2\\x80\\x931941, 2022.\\n[12] L. Zhang and Y.-C. Liang, \\xe2\\x80\\x9cDeep reinforcement learning for multi-\\nagent power control in heterogeneous networks,\\xe2\\x80\\x9d IEEE Transactions on\\nWireless Communications, vol. 20, no. 4, pp. 2551\\xe2\\x80\\x932564, 2020.\\n6\\n']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Text from paper9.pdf:\n",
            "[b'Democratizing Energy Management with LLM-Assisted Optimization\\nAutoformalism\\nMing Jin, Bilgehan Sel, Fnu Hardeep, Wotao Yin\\nAbstract\\xe2\\x80\\x94 This paper introduces a method for personaliz-\\ning energy optimization using large language models (LLMs)\\ncombined with an optimization solver. This approach, termed\\nhuman-guided optimization autoformalism, translates natural\\nlanguage speci\\xef\\xac\\x81cations into optimization problems, enabling\\nLLMs to handle various user-speci\\xef\\xac\\x81c energy-related tasks. It\\nallows for nuanced understanding and nonlinear reasoning\\ntailored to individual preferences. The research covers common\\nenergy sector tasks like electric vehicle charging, HVAC control,\\nand long-term planning for renewable energy installations. This\\nnovel strategy represents a signi\\xef\\xac\\x81cant advancement in context-\\nbased optimization using LLMs, facilitating sustainable energy\\npractices customized to individual needs.\\nI. INTRODUCTION\\nDespite computational advances, the complex challenges\\nof meeting energy demands and reducing carbon emissions\\npersist. Optimization techniques, as seen in EV charging [1],\\nenergy storage [2], renewable investments [3], smart building\\noperations [4], and demand side management [5], hold great\\npromise. However, the broader democratization of these\\ntools remains a signi\\xef\\xac\\x81cant hurdle. This paper tackles this\\nissue, advocating for the accessibility and practicality of\\nadvanced computational methods for all, especially the un-\\nderserved [6]. The emergence of LLMs offers a breakthrough\\nin overcoming these barriers. We demonstrate that LLMs can\\nbridge the gap between the high costs of traditional optimiza-\\ntion and the need for personalized, accessible solutions. Our\\ngoal is not industry-grade optimization, but rather to provide\\nusers with tools for informed decision-making, potentially\\ninvolving optimization problem formulation. This approach\\nleverages LLMs to streamline modeling and optimization,\\nenabling interaction through natural language without exten-\\nsive programming or mathematical optimization knowledge.\\nTechnical challenges and solutions. While LLMs such as\\nOpenAI\\xe2\\x80\\x99s ChatGPT offer a natural substrate for conversa-\\ntional AI (a technology that enables machines to understand,\\ninterpret, and engage in natural conversations), it is essential\\nto recognize that current LLMs have not excelled in tasks like\\narithmetic and logical reasoning [7]. Recent works have in-\\ntroduced methods such as chain of thoughts [8] (and variants\\nsuch as algorithm of thoughts [9]), which prompts a series\\nof intermediate reasoning steps, and autoformalism [10],\\nwhich automatically translates natural language mathematics\\nto formal speci\\xef\\xac\\x81cations and proofs. However, many energy\\nproblems, like energy storage control and long-term planning\\nfor PV panel installation, demand complex decision-making.\\nThese problems differ from arithmetic reasoning, common-\\nsense reasoning, and symbolic reasoning in the following\\naspects: Complexity, as they often involve numerous vari-\\n0\\n20\\n40\\n60\\n80\\n100\\nSolar panel\\nHVAC control\\nHeat pump\\nEV charging\\nBattery\\ncharging\\nBattery\\ncapacity\\nSuccess rate (%)\\nProblem category\\nBaseline\\nEC (CVXPY)\\nEC (SCIPY)\\nFig. 1: Comparison of the baseline method (simple prompt-\\ning with GPT-4) and EC using either CVXPY or SCIPY as\\nthe code projection layer across various problem categories.\\nSuccess rate denotes the percentage of instances in which the\\nprovided answer aligns with user expectations, satis\\xef\\xac\\x81es all\\ngiven constraints, and is either close to or exactly optimal.\\nables, constraints, and objectives with potentially nonlinear\\nrelationships between variables; and incomplete information,\\nas energy systems are in\\xef\\xac\\x82uenced by various factors, such as\\nuser preferences, which may not be provided in the initial\\nproblem description.\\nA. Contributions\\nIn this research, we address the above challenges as\\nfollows. To manage complexity, we develop a procedure\\nto map a problem description to code using the grammar\\nof packages that support optimization formulation, and to\\niteratively formulate, debug, and execute the program. We\\nalso introduce external tooling to execute the written code\\nand solve the formulated optimization using dedicated al-\\ngorithms. Furthermore, we implement prompt engineering\\ntechniques to enable LLMs to accurately understand and\\nrespond to user-speci\\xef\\xac\\x81c preferences. To address incomplete\\ninformation, we leverage the reasoning capabilities of LLMs\\nto identify key parameters and use a question-and-answer for-\\nmat in natural conversations to solicit this information from\\nusers. Additionally, we employ LLMs for auto-informalism\\nto explain the solution to users. An overview of the EC\\nframework is depicted in Fig. 2. This approach bears dual\\nbene\\xef\\xac\\x81ts: it not only guides the user through complex nonlin-\\near reasoning tasks of energy saving planning but also offers\\nan insightful explanation of the solution.\\nWe further demonstrate the possibility of our proposed ap-\\nproach in solving a variety of tasks within the energy domain,\\nranging from EV charging, HVAC and battery control, to\\nlong-term planning problems such as cost-bene\\xef\\xac\\x81t evaluation\\nof installing rooftop solar PVs, heat pumps, and battery\\nsizing. In comparison to the simple prompting method,\\nwhere the task description is directly presented to the LLM,\\n2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\\n979-8-3503-1855-5/24/$31.00 \\xc2\\xa92024 IEEE\\n258\\n2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm) | 979-8-3503-1855-5/24/$31.00 \\xc2\\xa92024 IEEE | DOI: 10.1109/SmartGridComm60555.2024.10738100\\nAuthorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \\n', b'our proposed EC framework improves the success rate, as\\nillustrated in Fig. 1. Although the optimizations formulated\\nare not exceedingly sophisticated, we \\xef\\xac\\x81nd that the LLM\\neffectively addresses the problem to a considerable degree.\\nThroughout the remainder of the paper, unless speci\\xef\\xac\\x81ed\\notherwise, we refer to OpenAI\\xe2\\x80\\x99s ChatGPT (GPT-4) when\\nmentioning LLM.\\nB. Related Work\\nRecent work has explored using LLMs for mathematical\\nautoformalization, converting natural language into formal\\nlanguages like Isabelle/HOL [11]. Analogously, we use in-\\ntermediaries like SCIPY and CVXPY to transform intuitive\\nformulations of optimization problems into standard form\\n[12]. Despite few CVXPY examples online, LLMs exhibit\\nsurprising adeptness, albeit with occasional syntax errors that\\nwe correct via Python debugging. While LLMs struggle with\\nmulti-step problems [13], techniques like in-context learning\\n[14] and algorithmic search [9] have shown promise. We\\npresent a novel viewpoint using optimization itself as a rea-\\nsoning tool to enrich existing LLM augmentation techniques\\n[15]. LLMs can also revise responses given feedback, e.g. in\\nquestion-answering and code debugging [16]. We incorporate\\nrevisions based on programming language feedback and user\\nrequests, enabling rapid error correction.\\nOptimizing energy systems requires commonsense reason-\\ning and domain knowledge [17]. Prior work has used ML\\nand expert systems [18], but we harness LLM expertise for\\noptimization through natural language interaction. To our\\nknowledge, we are the \\xef\\xac\\x81rst to tackle non-trivial, energy-\\nspeci\\xef\\xac\\x81c problems with incomplete information this way.\\nThe remaining sections of this paper are structured as\\nfollows. Section II describes our proposed framework in\\ndetail, including the design principles and the optimization\\nautoformalism approach. Section III presents experimental\\nresults that demonstrate the effectiveness of our approach\\nin solving various energy-related tasks. We discuss the po-\\ntential and future directions of conversational AI for energy\\nsustainability and conclude in Section IV.\\nII. ENERGY CONCIERGE FRAMEWORK\\nUsing an EV charging query as an example, our Energy\\nConcierge operates as follows:\\n1) User submits natural language request (e.g. optimize\\nmy charging schedule). LLM determines if it is an\\noptimization problem.\\n2) If so, LLM requests input parameters needed to solve\\nit. It may ask clarifying questions (e.g. charging ca-\\npacity? preferred hours?).\\n3) LLM formulates Python code with user information,\\ntranslating the query into an optimization problem via\\nautoformalism.\\n4) Interface executes code to solve problem, with debug-\\nging iterations if needed.\\n5) LLM explains the optimal solution clearly (charging\\nschedule, cost savings). Enabling informed user deci-\\nsions.\\nFig. 2: Energy Concierge framework. The user engages with\\nan LLM through natural language queries and responses.\\nThe LLM identi\\xef\\xac\\x81es the necessary input parameters for opti-\\nmization and generates Python code to address the problem.\\nThe program interface then executes the code and relays the\\nsolution back to the LLM, which subsequently provides a\\nclear explanation to the user.\\nThrough interactivity and personalization grounded in opti-\\nmization and autoformalism, this conversational framework\\nempowers users to improve energy ef\\xef\\xac\\x81ciency.\\nA. Optimization autoformalism\\nA general optimization problem can be written as:\\nminimizex2Rp\\nf(x; \\xe2\\x9c\\x93)\\nsubject to\\ngi(x; \\xe2\\x9c\\x93) \\xef\\xa3\\xbf0,\\ni = 1, 2, . . . , m\\nhj(x; \\xe2\\x9c\\x93) = 0,\\nj = 1, 2, . . . , n,\\n(P(\\xe2\\x9c\\x93))\\nwhere we seek to minimize the objective function f(x; \\xe2\\x9c\\x93)\\nsubject to a set of inequality constraints gi(x; \\xe2\\x9c\\x93) \\xef\\xa3\\xbf0, and\\nequality constraints hj(x; \\xe2\\x9c\\x93) = 0. Here, x 2 Rp is the\\ndecision variable and \\xe2\\x9c\\x93is the collection of hyperparameters\\nthat de\\xef\\xac\\x81nes the optimization instance, including objective\\nand constraint functions; in other words, the solution of\\nthe optimization P(\\xe2\\x9c\\x93) can be regarded as a function of the\\nhyperparameters \\xe2\\x9c\\x93[19].\\nOptimization techniques can be used to solve energy-\\nrelated problems [1]\\xe2\\x80\\x93[5]. Automating the formulation and\\nsolution of optimization problems is essential due to the tech-\\nnical skills gap, challenges in manually incorporating user\\npreferences and constraints, and the inef\\xef\\xac\\x81ciencies in manual\\nmodi\\xef\\xac\\x81cations based on user feedback. The human-guided\\nautoformalism proposed in this study automatically translates\\nnatural language task speci\\xef\\xac\\x81cations to optimization instances.\\nHowever, directlying implementing this approach may face\\nissues such as ambiguity, incompleteness, and incorporating\\nuser-speci\\xef\\xac\\x81c preferences. The subsequent subsections will\\ndelineate strategies to address them.\\n1) Optimization formulation: The optimization process\\nbegins with identifying the objective function, decision vari-\\nables, and constraints from the user\\xe2\\x80\\x99s task description [T].\\nWe tested two approaches:\\n2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\\n259\\nAuthorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \\n', b'\\xe2\\x80\\xa2 Approach 1: Directly ask the LLM to identify the\\noptimization components in [T].\\n\\xe2\\x80\\xa2 Approach 2: First prompt the LLM to identify the\\n5 most important parameters in [T], then use these\\nparameters to formulate the optimization instance.\\nApproach 2 outperforms Approach 1 because LLMs like\\nGPT-4 are trained on diverse data, not just optimization tasks.\\nAsking for the top 5 parameters simpli\\xef\\xac\\x81es the task and pro-\\nvides context, helping LLMs generate more precise optimiza-\\ntion formulations [8], [9]. Directly requesting optimization\\nformulations (Approach 1) can introduce ambiguityDirectly\\nrequesting optimization formulations can introduce ambigu-\\nity; initial parameter identi\\xef\\xac\\x81cation (Approach 2) reduces this\\nuncertainty.\\nAfter identifying the essential parameters, the natural\\nlanguage query is transformed into a computational instance\\nusing the prompt: \\xe2\\x80\\x9cWrite a Python code using [lib] to solve\\nthis optimization problem,\\xe2\\x80\\x9d where [lib] is either CVXPY or\\nSCIPY. We found that SCIPY yields a higher success rate\\n(71% to 100%) compared to CVXPY (51% to 80%) (see\\nFig. 1). This difference is likely due to SCIPY\\xe2\\x80\\x99s broader\\nacceptance and longer existence in the Python community,\\nresulting in more SCIPY-related content available online for\\nthe model to leverage during training.1\\n2) Solving an optimization and debugging: To address\\nLLMs\\xe2\\x80\\x99 limitations in nonlinear reasoning [7], we enable\\nthe model to interact with an external Python program to\\nsolve optimization tasks. We extract the code block from\\nthe LLM\\xe2\\x80\\x99s output using regular expressions, searching for\\nunique delimiters enclosing the code. This method reliably\\nextracts code without requiring a large labeled dataset, unlike\\ntraining a machine learning model [20]. During development,\\nwe encountered two types of errors in the LLM-generated\\ncode: erroneous translation into an optimization problem and\\nsyntactic bugs in an otherwise correctly translated problem.2\\nTo rectify translation errors, we rely on user interaction\\nand clari\\xef\\xac\\x81cation of the formulated optimization. For syntactic\\nerrors, we use an automated process: identify the error\\nmessage, feed it to the LLM to isolate relevant code snippets,\\ngenerate potential remedies, and assess the proposed solu-\\ntions in a new iteration (Fig. 7 in Appendix). Taking multiple\\ncode samples before debugging often results in fewer LLM\\nqueries (Fig. 5). By following this procedure, we successfully\\nresolved most errors encountered during our experiments.\\n3) Optimization auto-informalism: After discovering the\\noptimal solution, the LLM articulates the results in a com-\\nprehensible, natural language format. This system provides\\ndetailed explanations of the optimal solution and any con-\\nstraints or preferences factored in during the optimization\\nprocess (Steps 7a and 7b in Fig. 2). This auto-informalism\\napproach complements autoformalism by offering intuitive\\n1A search on GitHub (as of 4/25/24) returns 8K repositories for \\xe2\\x80\\x9dSCIPY\\xe2\\x80\\x9d\\nand only 272 for \\xe2\\x80\\x9dCVXPY,\\xe2\\x80\\x9d supporting our preference for SCIPY.\\n2Errors can also arise due to the limitations of the optimization solvers,\\nbut since most energy problems involve linear or quadratic convex opti-\\nmization, we presume such errors are comparatively infrequent. We do not\\nclassify infeasibility as an error.\\n0\\n0.03\\n0.06\\n0.09\\n0.12\\n0.15\\n0.18\\n0.21\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n6pm\\n7pm\\n8pm\\n9pm\\n10pm\\n11pm\\n12am\\n1am\\n2am\\n3am\\n4am\\n5am\\nElectricity Price ($)\\nCharging Power (kW)\\nCommon Charging\\nBaseline\\nEC\\nPrice\\nFig. 3: A comparison of EV charging plans to improve\\nthe cost and load on the grid. Observations reveal that EC\\neffectively capitalizes on hours with lower prices in contrast\\nto the baseline.\\ninsight into the optimization outcomes.\\nIII. EXPERIMENTS AND CASE STUDIES\\nThis section examines the potential of LLM-based auto-\\nformalism in real-time decision-making (Sec. III-A) and sus-\\ntainable long-term planning (Sec. III-B). A general analysis\\nof the proposed methods is provided in Sec. III-C. The [21,\\nappendix] offers example interactions.\\nA. Real-time decision making\\n1) Smart EV charging: The core issue in smart EV\\ncharging is to optimize EV charging patterns to balance\\npower grid loads, mitigate energy costs, and ful\\xef\\xac\\x81ll user pref-\\nerences [1]. EC can correspond user-provided information to\\noptimization parameters, such as the maximum charging rate.\\nIt grasps the user\\xe2\\x80\\x99s charging availability and translates this\\ninformation into decision variables and constraints, leading\\nto an optimized schedule (see Fig. 3).\\n2) HVAC control: HVAC control is a pivotal issue that\\nhas been extensively studied [4]. However, the algorithms\\nthat have been developed are often more advanced than\\nthe current control panels in most buildings, leading to a\\ndisconnect between sophisticated methods and user compre-\\nhension. The heart of the issue is to discover a setpoint\\nthat ensures a comfortable indoor climate while minimizing\\nenergy consumption. EC provides clear and actionable advice\\nlike \\xe2\\x80\\x9cSet your thermostat to the optimal temperature (75\\xe2\\x97\\xa6F in\\nthis case) during hot, humid days...\\xe2\\x80\\x9d with a clear rationale. It\\nalso offers customized suggestions based on user conditions,\\nalong with valuable energy ef\\xef\\xac\\x81ciency tips such as \\xe2\\x80\\x9cmonitor\\nyour energy consumption\\xe2\\x80\\x9d, and \\xe2\\x80\\x9cadapt to changing condi-\\ntions\\xe2\\x80\\x9d. Nevertheless, the cost of simplicity is the inability\\nto perform sophisticated controls like pre-heating/cooling,\\nwhich entails heating the room in anticipation of occupancy\\nand can only be accounted for through a multiperiod formu-\\nlation involving intricate room thermal dynamics [4].\\n3) Battery charging control: This problem involves opti-\\nmizing the charging and discharging cycles of a home battery\\nsystem, considering factors such as electricity pricing, solar\\ngeneration, and household demand, as seen in competitions\\nlike the CityLearn Challenge [22]. As discussed in [23],\\nselecting the appropriate optimization problem for a given\\ncontext is crucial. Unlike [23], where the context is derived\\n2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\\n260\\nAuthorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \\n', b'from a reward signal, our context is provided by the user in\\nnatural language. The EC interprets this context accurately\\nto establish the right optimization problem.\\nEC successfully formulates an accurate objective that com-\\nputes the weighted sum of total electricity cost, considering\\nthe electricity price. It also correctly formulates the lower\\nand upper limits for the charging and discharging variables,\\nthe temporal interdependence of the state of charge and\\nthe charging rate, and the maximum capacity constraint.\\nThe energy balance constraint identi\\xef\\xac\\x81ed demonstrates LLM\\xe2\\x80\\x99s\\nunderstanding of world modeling, including this constraint\\nwithout speci\\xef\\xac\\x81c user instruction. EC\\xe2\\x80\\x99s explanation further\\nshows an understanding of physical constraints and its over-\\nall objective. However, EC implicitly assumes that excess\\nenergy produced each hour can be sold back to the grid at\\nthe same rate as it was purchased, which is not typically\\nthe case. Future work can explore methods to encourage\\nEC to be more explicit about assumptions when constructing\\noptimization instances.\\nB. Long-term planning for sustainability\\n1) Cost-bene\\xef\\xac\\x81t analysis of installing rooftop solar PVs:\\nTo perform a cost-bene\\xef\\xac\\x81t analysis of solar PV, one must\\nestimate the costs and bene\\xef\\xac\\x81ts over the system\\xe2\\x80\\x99s lifespan\\nand compare them to a relevant alternative system [24].\\nEC shows pro\\xef\\xac\\x81ciency in understanding physical con-\\nstraints, such as the stipulation that the panel area shouldn\\xe2\\x80\\x99t\\nsurpass the roof area, and proposes that the installed area\\nbe adequate to supply the electricity demand. However, it\\noperates under a few assumptions, such as the user planning\\nto source all electricity demand from PV to fully leverage\\nthe budget, and that the PV\\xe2\\x80\\x99s ef\\xef\\xac\\x81ciency stands at 0.12. While\\nthese assumptions are generally sensible, stating them in\\nthe explanations would make the model\\xe2\\x80\\x99s workings more\\ntransparent. Additionally, the model seems to underutilize the\\nuser\\xe2\\x80\\x99s provided information, such as the building\\xe2\\x80\\x99s location,\\nwhich could potentially inform the required area-to-power\\nconversion ef\\xef\\xac\\x81ciency more accurately. This is understandable\\ngiven the model\\xe2\\x80\\x99s lack of access to an external database,\\nand future work that enhances this capability could be a\\nworthwhile pursuit.\\n2) Cost-bene\\xef\\xac\\x81t analysis of installing a heat pump: Key\\nconsiderations in the cost-bene\\xef\\xac\\x81t analysis of a heat pump\\ninclude the initial cost of installation, which can vary based\\non the type, size, and complexity of the system, as well as\\nthe availability of incentives and rebates [25].\\nSimilar to the PV installation scenario in Sec. III-B.1,\\nEC pro\\xef\\xac\\x81ciently identi\\xef\\xac\\x81es pivotal relationships, including the\\nannual operating cost of a central AC and a heat pump.\\nEC\\xe2\\x80\\x99s approach focuses on optimizing annual savings, leaving\\nthe actual estimation of the payback period to the user.\\nFor instance, EC calculates that the annual savings with a\\nheat pump equals $550, then elaborates on how to use this\\ndata for investment decisions, suggesting \\xe2\\x80\\x9cIf the purchase\\nand installation of the heat pump cost $5,000, the payback\\nperiod would equate to around 9.1 years ($5,000 / $550).\\xe2\\x80\\x9d\\nEV Charging\\nBattery Charging Battery Capacity\\nSolar Panel\\nHVAC\\nHeat Pump\\n0\\n20\\n40\\n60\\n80\\n100\\nCompilation\\nCorrectness\\nExplanation\\nProblem category\\nSuccess rate (%)\\nFig. 4: The ef\\xef\\xac\\x81ciency of EC when integrated with SCIPY\\nis gauged by the compilation success of the Python code it\\ngenerates, the solution\\xe2\\x80\\x99s precision, and the lucidity of the\\nexplanation presented to the user. The black lines highlight\\nthe performance shifts observed when employing CVXPY.\\nFig. 5: (A) A comparative analysis of the number of itera-\\ntions needed to achieve executable code across all problem\\ncategories (note that none of the experimental instances use\\n4 iterations). (B) A comparison of the average number of\\ncode generations needed for EC to produce executable code,\\nlimited to a maximum of 5. The black bars represent the\\nvariation in iterations across problem categories over 20\\nrandom runs.\\nEC also suggests users compare alternatives, plan long-\\nterm strategies, evaluate environmental impact, and seek\\nincentives or rebates. This shows that EC can provide not\\nonly a decision but also the context and information nec-\\nessary to empower users to make more cost-effective and\\nenvironmentally conscious choices.\\nC. General \\xef\\xac\\x81ndings and analysis\\nTo elucidate the types of errors EC commits, we display\\nthe success rates concerning error-free code production,\\nlogical correctness of the code, and the ability to articulate\\nthe solution derived from the optimization code to the end\\nuser in Fig. 4. Our framework enables EC to achieve a 100%\\ncompilation rate. When EC generates an error-free optimiza-\\n2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\\n261\\nAuthorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \\n', b'tion code, it consistently explains the results to the end\\nuser. SCIPY outperforms CVXPY in overall performance,\\naligning with our observation that SCIPY has broader online\\nrecognition and use, where most LLMs receive training.\\nFig. 5 presents the average number of code regenerations\\nneeded and the average debugging iterations required to\\nobtain syntax error-free optimization code. Although each\\nsubproblem within the optimization problem category ex-\\nhibits different success rates, a relatively low number of code\\nregenerations are typically needed. When the LLM consis-\\ntently commits errors, incorporating debugging information\\nin our framework is essential for achieving optimization solu-\\ntions. Reducing the number of generated code samples from\\ns = 5 to s = 1 slightly increases the required debugging\\niterations across all examples, suggesting that debugging is\\na more ef\\xef\\xac\\x81cient error-handling strategy compared to simple\\nregeneration.\\nEstimating the Probability of One-Round Autoformal-\\nism Success. The aforementioned results allow us to infer\\nthe probability of generating valid code successfully, denoted\\nas p. We postulate that the event of successfully generating\\nvalid code is independent and identically distributed for the\\nsame category of problems, following a truncated geometric\\ndistribution with a cap at 5 trials. Based on this supposition,\\nwe deduce that P5\\nk=1 k(1\\xe2\\x88\\x92p)k\\xe2\\x88\\x921p+6 P\\nk\\xe2\\x89\\xa56(1\\xe2\\x88\\x92p)k\\xe2\\x88\\x921p =\\nz, where z signi\\xef\\xac\\x81es the number of generations needed to\\nachieve success (with generations capped at 5, as shown in\\nFig. 5), and p is the probability of success to be estimated.\\nSubstituting the values for z from Fig. 5 into the equation and\\nsolving for p, we obtain the one-round autoformalism success\\nrates p = 0.8, 0.25, 0.38, 0.53, 0.83, 0.38 for the problems\\nof EV Charging, Battery Charging, Battery Capacity, Solar\\nPanel, HVAC Control, and Heat Pump Investment, respec-\\ntively.\\nEstimation of the Probability of Debugging Success.\\nWe posit that the event of successful debugging, given the\\npresence of an erroneous code, is independent and identically\\ndistributed across all problem classes. Using Fig. 5, we \\xef\\xac\\x81rst\\nnormalize the frequency of the required number of debugging\\niterations by the frequency of generating an erroneous code\\nin the \\xef\\xac\\x81rst run (i.e., 1 \\xe2\\x88\\x920.7 = 0.3). This reveals that the\\nfrequencies of observing debugging iterations 1, 2, 3, 4, and\\n\\xe2\\x89\\xa55 are represented by yk, where k 2 {1, 2, ..., 5}. Let q\\ndenote the probability of successfully debugging the code.\\nFrom this, we can construct a system of polynomial equa-\\ntions in the form of (1 \\xe2\\x88\\x92q)k\\xe2\\x88\\x921q = yk for k 2 {1, 2, 3, 4},\\nand 1 \\xe2\\x88\\x92P4\\nk=1(1 \\xe2\\x88\\x92q)k\\xe2\\x88\\x921q = y5 given that debugging is\\ncapped at 5 iterations. Using a line search between 0 and 1\\nin increments of 0.01, we calculate the values of \\xcb\\x86y1 to \\xcb\\x86y5\\nfor each q using the speci\\xef\\xac\\x81ed equations, and determine the\\nmean squared error (MSE) between the vectors \\xcb\\x86y and y. Our\\noptimal estimate for the probability is q = 0.26.\\nOptimality Gap and Improvement Over Baseline. The\\noptimality gap for a feasible solution is calculated as the ratio\\nv/v\\xe2\\x87\\xa4\\xe2\\x88\\x921, where v denotes the objective value of the candidate\\nsolution and v\\xe2\\x87\\xa4signi\\xef\\xac\\x81es the optimal value of the corre-\\nsponding minimization problem. In Fig. 4, we classify test\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n0\\n5\\n10\\n15\\n20\\nProblem Category\\nEV Charging\\nBattery Charging\\nBattery Capacity\\nSolar Panel\\nHVAC\\nHeat Pump\\nImprovement over Baseline (%)\\nOptimality Gap (%)\\nFig. 6: We assess the average optimality gap by compar-\\ning the exact solution of the optimization problem with\\nthe solution determined by EC, alongside evaluating the\\nenhancement over the baseline in terms of optimality.\\ncases as correct only if they equate to the globally optimal\\nsolution value, i.e., a 0% optimality gap. Additional results\\npresented in Fig. 6 demonstrate the average optimality gap\\nwhen incorrect logic within the code results in the omission\\nof key parameters (such as the ef\\xef\\xac\\x81ciency of a battery). Our\\nframework was evaluated using 20 examples for each energy\\noptimization problem, including EV charging. As depicted,\\nthe instances display an optimality gap generally within the\\n20% range, with the majority under 10%.\\nFor the baseline method, the model was simply prompted\\nwith our question along with necessary parameters to solve\\nthe optimization problem. We noticed that even when the\\nproblem is clearly an optimization issue, LLMs may opt\\nto generate responses by attempting to apply logic towards\\nreaching an answer. As seen in Fig. 6, this approach does not\\nyield favorable results in comparison to the EC framework.\\nThis can be attributed to the fact that many energy-related\\noptimization problems of importance do not yield closed-\\nform solutions, making the correct utilization of convex\\nprogram solvers critical.\\nWe further report on the improvement over the baseline,\\nmeasured by vb/v \\xe2\\x88\\x921, where vb represents the objective\\nvalue of the baseline solution. As is evident, the improvement\\ncan amount to as much as 60%, with most instances falling\\nwithin the 30% range. Some of the problems yielding the\\ngreatest improvements include Solar Panel, Heat Pump,\\nBattery Charging, and EV Charging, which encompass both\\nreal-time decisions and long-term investments.\\nIV. DISCUSSIONS AND CONCLUSION\\nConversational AI for sustainability has long seemed\\naspirational, but LLMs offer new potential. By providing\\nan intuitive interface, our framework helps individuals en-\\ngage more consciously about energy use. Greater awareness\\ndrives sustainable behaviors like responsible consumption,\\nef\\xef\\xac\\x81ciency, and solar adoption [26], [27].\\nHouseholds average 3.5 kWh peak electricity use, while\\nEV chargers approach 10 kWh. EV charging at peak triples\\nload, straining infrastructure. Utilities offer off-peak rates,\\nbut \\xef\\xac\\x81nancial incentives alone don\\xe2\\x80\\x99t fully shift behaviors\\xe2\\x80\\x94\\nan \\xe2\\x80\\x9cef\\xef\\xac\\x81ciency gap\\xe2\\x80\\x9d phenomenon [26], [27]. Our framework\\nsimpli\\xef\\xac\\x81es this via conversation. With 2022\\xe2\\x80\\x99s 3 million EVs,\\nadopting off-peak charging could save $876 million annually\\n(at $0.06 vs $0.14 per kWh rates) [28], [29]. With 2035\\xe2\\x80\\x99s\\n2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\\n262\\nAuthorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \\n', b'projected 73 million EVs [30], savings could reach $21.3\\nbillion. Beyond consumer savings, off-peak charging lessens\\ngrid stress, stabilizes prices, and reduces needs for new\\nplants. By simplifying optimization, our system enables\\nimpactful sustainability actions.\\nA key limitation of EC is its reliance on the LLM\\xe2\\x80\\x99s\\nability to accurately formulate optimization problems, ensur-\\ning proper alignment of objectives and constraints. Incorrect\\nformulations can lead to ineffective or harmful solutions,\\ndespite the perceived reliability of these systems. Auto-\\ninformalism (Sec. II-A.3) helps mitigate this risk by scru-\\ntinizing problem formulations more carefully and alerting\\nusers to potential issues or assumptions. Enhancing auto-\\ninformalism\\xe2\\x80\\x99s effectiveness is thus critical for reliable solu-\\ntions. Additional reliability methods like validation routines,\\nrobustness testing, and expanded user feedback loops can\\nfurther strengthen the framework.\\nWhile we merely sketch out the potential, the pro-\\nposed shift towards human-guided optimization autoformal-\\nism could democratize access to sophisticated technologies\\nand set the stage for a more equitable and sustainable future.\\nREFERENCES\\n[1] S. M. Arif, T. T. Lie, B. C. Seet, S. Ayyadi, and K. Jensen, \\xe2\\x80\\x9cReview\\nof electric vehicle technologies, charging methods, standards and\\noptimization techniques,\\xe2\\x80\\x9d Electronics, vol. 10, no. 16, p. 1910, 2021.\\n[2] R. Khezri, A. Mahmoudi, and H. Aki, \\xe2\\x80\\x9cOptimal planning of solar\\nphotovoltaic and battery storage systems for grid-connected residential\\nsector: Review, challenges and new perspectives,\\xe2\\x80\\x9d Renewable and\\nSustainable Energy Reviews, vol. 153, p. 111763, 2022.\\n[3] Y. Yang, S. Bremner, C. Menictas, and M. Kay, \\xe2\\x80\\x9cBattery energy\\nstorage system size determination in renewable energy systems: A\\nreview,\\xe2\\x80\\x9d Renewable and Sustainable Energy Reviews, vol. 91, pp. 109\\xe2\\x80\\x93\\n125, 2018.\\n[4] J. Drgo\\xcb\\x87na, J. Arroyo, I. C. Figueroa, D. Blum, K. Arendt, D. Kim,\\nE. P. Oll\\xc2\\xb4e, J. Oravec, M. Wetter, D. L. Vrabie et al., \\xe2\\x80\\x9cAll you need to\\nknow about model predictive control for buildings,\\xe2\\x80\\x9d Annual Reviews\\nin Control, vol. 50, pp. 190\\xe2\\x80\\x93232, 2020.\\n[5] B. P. Esther and K. S. Kumar, \\xe2\\x80\\x9cA survey on residential demand\\nside management architecture, approaches, optimization models and\\nmethods,\\xe2\\x80\\x9d Renewable and Sustainable Energy Reviews, vol. 59, pp.\\n342\\xe2\\x80\\x93351, 2016.\\n[6] J. Currie, D. I. Wilson, N. Sahinidis, and J. Pinto, \\xe2\\x80\\x9cOpti: Lowering the\\nbarrier between open source optimizers and the industrial matlab user,\\xe2\\x80\\x9d\\nFoundations of computer-aided process operations, vol. 24, p. 32,\\n2012.\\n[7] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\\nJ. Aslanides, S. Henderson, R. Ring, S. Young et al., \\xe2\\x80\\x9cScaling language\\nmodels: Methods, analysis & insights from training gopher,\\xe2\\x80\\x9d arXiv\\npreprint arXiv:2112.11446, 2021.\\n[8] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V.\\nLe, D. Zhou et al., \\xe2\\x80\\x9cChain-of-thought prompting elicits reasoning in\\nlarge language models,\\xe2\\x80\\x9d in Advances in Neural Information Processing\\nSystems, 2022.\\n[9] B. Sel, A. Al-Tawaha, v Vanshaj, R. Jia, and M. Jin, \\xe2\\x80\\x9cAlgorithm of\\nthoughts: Enhancing exploration of ideas in large language models,\\xe2\\x80\\x9d\\nICML, 2024.\\n[10] Y. Wu, A. Q. Jiang, W. Li, M. N. Rabe, C. E. Staats, M. Jamnik,\\nand C. Szegedy, \\xe2\\x80\\x9cAutoformalization with large language models,\\xe2\\x80\\x9d in\\nAdvances in Neural Information Processing Systems, 2022.\\n[11] Y. Wu, A. Q. Jiang, W. Li, M. Rabe, C. Staats, M. Jamnik, and\\nC. Szegedy, \\xe2\\x80\\x9cAutoformalization with large language models,\\xe2\\x80\\x9d Ad-\\nvances in Neural Information Processing Systems, vol. 35, pp. 32 353\\xe2\\x80\\x93\\n32 368, 2022.\\n[12] A. Agrawal, R. Verschueren, S. Diamond, and S. Boyd, \\xe2\\x80\\x9cA rewriting\\nsystem for convex optimization problems,\\xe2\\x80\\x9d Journal of Control and\\nDecision, vol. 5, no. 1, pp. 42\\xe2\\x80\\x9360, 2018.\\n[13] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. Cai, M. Terry, Q. Le et al., \\xe2\\x80\\x9cProgram synthesis with large\\nlanguage models,\\xe2\\x80\\x9d arXiv preprint arXiv:2108.07732, 2021.\\n[14] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V.\\nLe, D. Zhou et al., \\xe2\\x80\\x9cChain-of-thought prompting elicits reasoning in\\nlarge language models,\\xe2\\x80\\x9d Advances in Neural Information Processing\\nSystems, vol. 35, pp. 24 824\\xe2\\x80\\x9324 837, 2022.\\n[15] G. Mialon, R. Dess`\\xc4\\xb1, M. Lomeli, C. Nalmpantis, R. Pasunuru,\\nR. Raileanu, B. Rozi`ere, T. Schick, J. Dwivedi-Yu, A. Celikyil-\\nmaz et al., \\xe2\\x80\\x9cAugmented language models: a survey,\\xe2\\x80\\x9d arXiv preprint\\narXiv:2302.07842, 2023.\\n[16] T. X. Olausson, J. P. Inala, C. Wang, J. Gao, and A. Solar-Lezama,\\n\\xe2\\x80\\x9cDemystifying gpt self-repair for code generation,\\xe2\\x80\\x9d arXiv preprint\\narXiv:2306.09896, 2023.\\n[17] A. B. Saka, L. O. Oyedele, L. A. Akanbi, S. A. Ganiyu, D. W.\\nChan, and S. A. Bello, \\xe2\\x80\\x9cConversational arti\\xef\\xac\\x81cial intelligence in the\\naec industry: A review of present status, challenges and opportunities,\\xe2\\x80\\x9d\\nAdvanced Engineering Informatics, vol. 55, p. 101869, 2023.\\n[18] R. Panchalingam and K. C. Chan, \\xe2\\x80\\x9cA state-of-the-art review on\\narti\\xef\\xac\\x81cial intelligence for smart buildings,\\xe2\\x80\\x9d Intelligent Buildings Inter-\\nnational, vol. 13, no. 4, pp. 203\\xe2\\x80\\x93226, 2021.\\n[19] M. Jin, V. Khattar, H. Kaushik, B. Sel, and R. Jia, \\xe2\\x80\\x9cOn solution func-\\ntions of optimization: Universal approximation and covering number\\nbounds,\\xe2\\x80\\x9d AAAI, 2023.\\n[20] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, \\xe2\\x80\\x9cSummarizing\\nsource code using a neural attention model,\\xe2\\x80\\x9d in Proceedings of the\\n54th Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers), 2016, pp. 2073\\xe2\\x80\\x932083.\\n[21] M. Jin, B. Sel, H. FNU, and W. Yin, \\xe2\\x80\\x9cDemocratizing energy manage-\\nment with llm-assisted optimization autoformalism,\\xe2\\x80\\x9d 2024, full Ver-\\nsion:\\nhttp://www.jinming.tech/papers/ChatEnergy23-smartgridcomm.\\npdf.\\n[22] J. R. V\\xc2\\xb4azquez-Canteli, J. K\\xc2\\xa8ampf, G. Henze, and Z. Nagy, \\xe2\\x80\\x9cCitylearn\\nv1.0: An openai gym environment for demand response with deep\\nreinforcement learning,\\xe2\\x80\\x9d in Proceedings of the 6th ACM International\\nConference on Systems for Energy-Ef\\xef\\xac\\x81cient Buildings, Cities, and\\nTransportation, ser. BuildSys \\xe2\\x80\\x9919.\\nNew York, NY, USA: Association\\nfor Computing Machinery, 2019, p. 356\\xe2\\x80\\x93357. [Online]. Available:\\nhttps://doi.org/10.1145/3360322.3360998\\n[23] V. Khattar and M. Jin, \\xe2\\x80\\x9cWinning the citylearn challenge: Adaptive opti-\\nmization with evolutionary search under trajectory-based guidance,\\xe2\\x80\\x9d in\\nProceedings of the AAAI Conference on Arti\\xef\\xac\\x81cial Intelligence, vol. 37,\\nno. 12, 2023, pp. 14 286\\xe2\\x80\\x9314 294.\\n[24] M. Thebault and L. Gaillard, \\xe2\\x80\\x9cOptimization of the integration of\\nphotovoltaic systems on buildings for self-consumption\\xe2\\x80\\x93case study in\\nfrance,\\xe2\\x80\\x9d City and Environment Interactions, vol. 10, p. 100057, 2021.\\n[25] F. Bela\\xc2\\xa8\\xc4\\xb1d, Z. Ranjbar, and C. Massi\\xc2\\xb4e, \\xe2\\x80\\x9cExploring the cost-effectiveness\\nof energy ef\\xef\\xac\\x81ciency implementation measures in the residential sector,\\xe2\\x80\\x9d\\nEnergy Policy, vol. 150, p. 112122, 2021.\\n[26] O. I. Asensio and M. A. Delmas, \\xe2\\x80\\x9cNonprice incentives and energy\\nconservation,\\xe2\\x80\\x9d Proceedings of the National Academy of Sciences, vol.\\n112, no. 6, pp. E510\\xe2\\x80\\x93E515, 2015.\\n[27] T. D. Gerarden, R. G. Newell, and R. N. Stavins, \\xe2\\x80\\x9cAssessing the\\nenergy-ef\\xef\\xac\\x81ciency gap,\\xe2\\x80\\x9d Journal of economic literature, vol. 55, no. 4,\\npp. 1486\\xe2\\x80\\x931525, 2017.\\n[28] APPALACHIAN\\nPOWER,\\n\\xe2\\x80\\x9cVirginia\\ns.c.c.\\ntariff\\nno.\\n26\\nappalachian\\npower\\ncompany,\\xe2\\x80\\x9d\\n2021.\\n[Online].\\nAvailable: https://www.appalachianpower.com/lib/docs/ratesandtariffs/\\nVirginia/Tariff26-MASTER-Standard-June1-2023RPS-RAC.pdf\\n[29] IEA,\\n\\xe2\\x80\\x9cTrends\\nin\\nelectric\\nlight-duty\\nvehicles,\\xe2\\x80\\x9d\\n2023.\\n[On-\\nline]. Available: https://www.iea.org/reports/global-ev-outlook-2023/\\ntrends-in-electric-light-duty-vehicles\\n[30] NREL, \\xe2\\x80\\x9cNational economic value assessment of plug-in electric\\nvehicles,\\xe2\\x80\\x9d\\n2016.\\n[Online].\\nAvailable:\\nhttps://www.nrel.gov/docs/\\nfy17osti/66980.pdf\\n[31] H. C. Hesse, R. Martins, P. Musilek, M. Naumann, C. N. Truong, and\\nA. Jossen, \\xe2\\x80\\x9cEconomic optimization of component sizing for residential\\nbattery storage systems,\\xe2\\x80\\x9d Energies, vol. 10, no. 7, p. 835, 2017.\\n2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\\n263\\nAuthorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \\n']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Text from example2.pdf:\n",
            "[b\"Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \\nprofessional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \\nA, the top tier of Italian football. In its early history, Milan played its home games in different grounds \\naround the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \\nbuilt by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \\nlargest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \\nwith whom they contest the Derby della Madonnina, one of the most followed derbies in football. \\n \\nMilan has spent its entire history in Serie A with the exception of the 1980\\xe2\\x80\\x9381 and 1982\\xe2\\x80\\x9383 seasons. \\nSilvio Berlusconi\\xe2\\x80\\x99s 31-year tenure as Milan president was a standout period in the club's history, as \\nthey established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \\ntrophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \\n1991\\xe2\\x80\\x9392 season, the club notably achieved the feat of being the first team to win the Serie A title \\nwithout losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \\nplayers, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \\npodium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \\n \\nDomestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \\ninternational competitions, Milan is Italy's most successful club. The club has won seven European \\nCup/Champions League titles, making them the competition's second-most successful team behind \\nReal Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \\nrecord two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \\n \\nMilan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \\nnow-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \\nClub Association. \\n \\n\"]\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Text from paper0.pdf:\n",
            "[b'XIWU: A BASIS FLEXIBLE AND LEARNABLE LLM FOR HIGH\\nENERGY PHYSICS\\nZhengde Zhang1, Yiyu Zhang1, Haodong Yao1, Jianwen Luo2, Rui Zhao1, Bo Huang1, Jiameng Zhao3,1,\\nYipu Liao1, Ke Li1, Lina Zhao1, Jun Cao1, Fazhi Qi1,\\xe2\\x88\\x97, and Changzheng Yuan1,\\xe2\\x80\\xa0\\n1Institute of High Energy Physics, Chinese Academy of Sciences, Beijing 100049, China\\n2School of Physical Sciences, University of Chinese Academy of Sciences, Beijing 100049, China\\n3School of Computer and Artificial Intelligence, ZhengZhou University, Henan 450066, China\\nABSTRACT\\nLarge Language Models (LLMs) are undergoing a period of rapid updates and changes, with state-\\nof-the-art (SOTA) model frequently being replaced. When applying LLMs to a specific scientific\\nfield, it\\xe2\\x80\\x99s challenging to acquire unique domain knowledge while keeping the model itself advanced.\\nTo address this challenge, a sophisticated large language model system named as Xiwu has been\\ndeveloped, allowing you switch between the most advanced foundation models and quickly teach the\\nmodel domain knowledge. In this work, we will report on the best practices for applying LLMs in the\\nfield of high-energy physics (HEP), including: a seed fission technology is proposed and some data\\ncollection and cleaning tools are developed to quickly obtain domain AI-Ready dataset; a just-in-time\\nlearning system is implemented based on the vector store technology; an on-the-fly fine-tuning system\\nhas been developed to facilitate rapid training under a specified foundation model.\\nThe results show that Xiwu can smoothly switch between foundation models such as LLaMA, Vicuna,\\nChatGLM and Grok-1. The trained Xiwu model is significantly outperformed the benchmark model\\non the HEP knowledge Q&A and code generation. This strategy significantly enhances the potential\\nfor growth of our model\\xe2\\x80\\x99s performance, with the hope of surpassing GPT-4 as it evolves with the\\ndevelopment of open-source models. This work provides a customized LLM for the field of HEP,\\nwhile also offering references for applying LLM to other fields, the corresponding codes are available\\non Github https://github.comzhang/zhengde0225/Xiwu.\\nKeywords LLM \\xc2\\xb7 Deep Learning \\xc2\\xb7 Artificial Intelligence \\xc2\\xb7 Particle Physics \\xc2\\xb7 HEP\\n\\xe2\\x88\\x97Corresponding Author: qfz@ihep.ac.cn\\n\\xe2\\x80\\xa0Corresponding Author: yuancz@ihep.ac.cn\\narXiv:2404.08001v1  [hep-ph]  8 Apr 2024\\n', b'Running Title for Header\\nFigure 1: The hallucination of GPT-4 when answering domain questions.\\n1\\nIntroduction\\nLarge Language Models\\nThe Large Language Models (LLMs) such as GPT-4 [1] and LLaMA[2, 3] have exhibited\\ncapabilities beyond expectations in terms of general intent understanding, robust continuous dialogue, intelligent\\ninteraction correction, and adequate logical reasoning, actively propelling the shifts towards an intelligent paradigm in\\nscientific research.\\n2\\n', b'Running Title for Header\\nHigh Energy Physics\\nHigh Energy Physics (HEP) is a crucial branch of physics that delves into the characteristics\\nand interactions of the most basic elements of matter. It serves as an essential field for unraveling the universe\\xe2\\x80\\x99s\\nunderlying principles and laws. Creating Large Language Models (LLMs) for High Energy Physics (HEP) can play\\na pivotal role in streamlining research activities, including literature review, coding, data management, analysis of\\nphysics, interpretation of results, and manuscript composition. This advancement can free researchers from mundane\\ntasks that demand less creativity, thereby amplifying the efficiency and productivity of scientific investigations.\\nHallucinations\\nHowever, the LLMs can suffer from hallucinations, i.e. \\xe2\\x80\\x9cproduce content that is nonsensical or\\nuntruthful in relation to certain sources\\xe2\\x80\\x9d [4, 5], which is particularly harmful in scientific fields. As shown in Figure\\n1, even the most powerful LLM (GPT-4) can struggle with domain-specific questions or generating scientific code of\\nHEP. The capability of general LLMs to precisely tackle specialized topics or develop code for focused areas may be\\nconstrained.\\nWay to Reduce Hallucinations\\nThere are two methods can enhance the model\\xe2\\x80\\x99s ability to handle domain-specific\\ntasks: one is further training the model, including both secondary pre-training and fine-tuning, and the other is Retrieval-\\nAugmented Generation (RAG). The foundation models are often referred to as L0 (Level 0) models, such as LLaMA and\\nGPT-3. Models fine-tuned for specific tasks based on the foundation model are called L1 models, such as LLaMA-Chat,\\nVicuna [6] and GPT-3.5 [7], which is further adjusted for conversational tasks. Models that are further fine-tuned with\\ndomain-specific knowledge can be termed as L2 models.\\nHEP\\xc2\\xb7Xiwu LLM\\nHere, we introduce the Xiwu (/SeI\"wu:/), a L2 large language model that exhibits fewer hallu-\\ncinations in HEP field. The most significant features of this model are its basis flexibility and learnability. Basis\\nflexible means that the foundational model can be changed, becoming increasingly powerful with the upgrade of\\nopen-source models. Learnable implies that once the foundational model is updated, it can quickly be taught domain\\nknowledge. Specifically, a just-in-time learning system based on a vector store has been implemented, allowing for\\ninstant knowledge updates to the Xiwu at a low cost.\\nWhy Consider Flexible?\\nTraining a foundation LLM is extremely costly. For a specific scientific field, on one hand,\\ntraining LLM from scratch is unaffordable; on the other hand, even if trained LLM from scratch, it might not keep up\\nwith the pace of development of open-source models. \"Using the strengths of others to improve oneself,\" we believe\\nthat having a flexible foundational model is the best strategy to maintain the most advanced performance of the model\\nat the lowest cost.\\nContributions\\nThe contributions are as follows:\\n1. The seed fission technology is proposed and realized, which has proven its effectiveness in acquiring training\\nQ&A data related to the field with only one seed. It can be widely applied in various fields.\\n2. A basis flexible LLM system has been strategized and implemented, evolving from the initial LLaMA to now\\nsupport upgrades to LLaMA2 and Vicuna, and beyond.\\n3. An just-in-time learning system based on RAG is realized, capable of acquiring knowledge instantly.\\n4. Xiwu, the first LLM specilized for high energy physics outperforms the foundation model in accuracy for\\ndomain-specific knowledge question answering, and exceeds GPT-4 in BOSS (BESIII Offline Software\\nSystem) code generation.\\n5. The WebUI application has been deployed into HepAI platform, the related codes is open-sourced on GitHub.\\n2\\nRelated Work\\nLanguage Models\\nThe introduction of deep learning has greatly propelled the development of language models. Early\\napplications of neural networks to language models, such as Feedforward Neural Networks[8], laid the groundwork.\\nThe emergence of RNN[9, 10, 11] and LSTM[12] models advanced language model by effectively capturing temporal\\ndependencies. However, their inability to process sequences in parallel limited their efficiency. In 2017, the introduction\\nof the Transformer model[13], with its self-attention mechanism, overcame these limitations, significantly boosting the\\nhandling of long-distance dependencies and computational efficiency, propelling the development of language models\\ninto a new era. Building on this architecture, BERT[14] and OpenAI\\xe2\\x80\\x99s GPT series[5, 4] were developed for substantial\\nadvancements in NLP tasks through pre-training and fine-tuning. Subsequent works [15, 16, 17, 18] further refined and\\nenhanced these methodologies, leading to continuous performance improvements. In 2020, The release of GPT-3[19],\\na significantly larger laguage model, underscored the effectiveness of few-shot and zero-shot learning, prompting a\\n3\\n', b'Running Title for Header\\nsignificant trend toward model scaling. By 2022, the launch of ChatGPT[7] marked milestones in NLP, showcasing\\nthe potential of Large Language Models (LLMs) as efficient AI communicators. Moreover, the open-source model\\nVicuna[6], which is based on fine-tuning LLAMA[2], made a substantial contribution to the open-source community by\\nachieving up to 90% of GPT-4\\xe2\\x80\\x99s[1] performance, sparking widespread research interest.[20, 21, 22, 23].\\nLLMs in Specific-Domain\\nRecent research has shown that BERT models, when fine-tuned for specific domains, can\\nachieve significant performance improvements. This trend is exemplified by models such as BioBERT[24](targeted at\\nthe biomedical domain), SciBERT[25] (for scientific literature), and ClinicalBERT[26](for clinical data). However, the\\nhigh costs associated with pre-training large-scale, domain-specific models like Galactica[27] and BloombergGPT[28]\\nunderscore the value of domain-specific fine-tuning as a more practical and cost-effective strategy. For instance,\\nFlan-PaLM[29] constructs a specialized large model for the clinical medicine domain by combining few-shot learning,\\nchain-of-thought (CoT) prompting strategies, self-consistency prompting, and instruction fine-tuning. WizardMath[30]\\nfine-tunes LLAMA2[3] using the Reinforcement Learning from Evol-Instruct Feedback method to create a large model\\nfor mathematical reasoning. Meanwhile, SciGLM[31] develops a specialized large model for the scientific domain by\\nfine-tuning with high-quality datasets built using a self-reflexive annotation framework. These studies showcase the\\npotential and value of fine-tuning large language models for specific domains.\\nLLMs in HEP In the field of high-energy physics, ChATLAS has trained a domain-customized model using documents\\nsuch as Twiki, ATLAS docs, and the E-group Archive, providing AI services to the collaboration group. AccGPT,\\ndeveloped based on LLAMA, is used for accelerator auxiliary control at CERN. Based on ChatGPT, the ATLAS Open\\nData Higgs Analysis Guide can assist individuals interested in physics analysis who are not specialized in high-energy\\nphysics. LLMs are being experimented with in various types of task of HEP.\\n3\\nMethodology\\n3.1\\nOverall Architecture\\nFigure 2: The architecture of Xiwu large language model system.\\nThe architecture of Xiwu LLM is shown in FIG. 2. It consists of four parts: data engine, large language model, memory\\nbased on external knowledge lib and intelligent agent with its interface.\\nData Engine\\nThe data engine is primarily used to collect text data from eight fields related to HEP. The specific\\nmethods of data collection include seed fission technology, real concerns from ChatBot, knowledge from highly cited\\nfull-text literature, and a large number of literature abstracts. The data is cleaned and organized using LLM, and\\nreviewed by professional staffs.\\nXiwu LLM\\nBased on the Level 0 Model LLaMA open-sourced by Meta, the Lmsys team has implemented a Level\\n1 Vicuna through secondary training. Currently, Xiwu is a Level 2 model trained on high-energy physics data based\\non Vicuna. With the upgrades of LLaMA and Vicucna, Xiwu\\xe2\\x80\\x99s current base model is Vicuna-1.5 (corresponding to\\nLLaMA2), with parameter sizes of 7B and 13B. Plans are in place to adapt to even more advanced models.\\n4\\n', b'Running Title for Header\\nMemory Module\\nRAG has been widely used to mitigate hallucinations in LLMs, essentially enhancing the model\\xe2\\x80\\x99s\\ncapabilities through external knowledge library. Similarly, we vectorize HEP data using an Embedding model and store\\nit in a vector database as the long-term memory for Xiwu. Additionally, the process of adding or modifying information\\nin the vector database represents a low-cost, rapid, and just-in-time learning process.\\nIntelligent Agent\\nThe intelligent agent is structured around the Xiwu model, supplemented by an onboard knowl-\\nedge base. Utilizing our proposed HepAI Distributed Deployment Framework (HepAI-DDP), we\\xe2\\x80\\x99ve successfully\\nimplemented both an Application Programming Interface (API) and a ChatBot Web User Interface (WebUI). This\\nadvancement significantly simplifies the interaction process between the intelligent agent and its human counterparts,\\nbe they scientists or developers.\\nLearning Loops\\nThere are two learning loops in the system. The first loop is a just-in-time learning system, wherein\\nusers can directly embed accurate information into the knowledge base via the WebUI or API. Xiwu then retrieves and\\nsynthesizes this information to provide answers to queries. The second loop involves stable, unchanging information\\nbeing integrated into the HEP text datasets, which are subsequently used to fine-tune the weights of Xiwu model. The\\nformer loop facilitates the rapid assimilation of evolving information, while the latter progressively enables the model\\nto \"understand\" knowledge.\\n3.2\\nDatasets\\nFigure 3: The data resources and acquisition methods. (a) Eight domains related to High Energy Physics that are of our\\nconcern; (b) Four methods employed to gather the dataset\\nThe FIG. 3 shows HEP text data from eight sub-domains and the data collection methods. As shown in FIG. 3 (a),\\nin order to ensure the diversity and breadth of the model, we extensively gathered Q&A pairs and literatures from\\neight HEP related domains, including particle physics, particle astrophysics, experimental physics, theoretical physics,\\naccelerator physics, synchrotron radiation, neutron science and computer science. Data of each field is collected using\\nthe methods shown in FIG. 3 (b) on the right. There are four methods we used to collect data, including seed fission\\ntechnology, chat robot backend, full-text of highly-cited papers and abundant abstracts of the papers. Through those\\nmethods, totally 26k Q&A pairs for fine-tuning and 750M tokens for pre-training are collected and clearned up.\\nThe details will be described below.\\n3.2.1\\nData from Seed Fission Technology\\nSeed Fission\\nThe seed fission technology was proposed by us to quickly obtain related Q&A pairs in a specific\\ndomain. This technology allows a single seed such as Particle Physics, can fission into a multitude of diverse and\\nin-depth Q&A pairs. By employing this technique, we successfully generated more than 1K Q&A pairs for each\\nHEP-related domain as shown in FIG. 3 (a). After a thorough human review, we obtained about 8K high-quality data\\nentries.\\nFission Process\\nAs shown in FIG. 4, our approach involved designing three chat robots through prompt engineering:\\nNewbie, Expert and Checker. The Newbie asking questions with curiosity based on information. The Expert providing\\nanswers based on information and reliable sources as much as possible. The Checker is a topic supervisor that\\nresponsible for selecting topics and determining when to transition away from the current topic. The fission process is\\nas follows:\\n5\\n', b'Running Title for Header\\nFigure 4: The seed fission technology for getting diverse and in-depth data\\n1. The initial topic is considered a seed, such as \"Particle Physics,\" is given to the Newbie.\\n2. The Newbie generates 2 to 10 random questions based on the input. A single input leading to multiple outputs\\nis thus referred to as fission.\\n3. These questions are then reduplicated and filtered by the Checker, selecting the most interesting ones, or\\ndeciding to exit the current topic to choose another candidate topic. The selected question is directed to the\\nnext robot, while the remaining questions go into a candidate question list.\\n4. The Expert answers questions based on the knowledge base and search engine it is equipped with, and provides\\nthe source of the answers.\\n5. The answer serves as input information for the Newbie, who then poses more questions.\\nData Quality\\nBy using \"Particle Physics\" as the \"seed,\" 50 question-answer pairs and 2,822 candidate questions can\\nbe generated. Some samples are shown in FIG. 4, from just this seed alone, concepts related to high-energy physics\\nsuch as fermions, spin, gravitational waves, and redshift can be derived, thus the data generated is diverse and in-depth.\\nAdvantage\\nThe significant advantage of this technique is that it allows us to generate a large volume of relevant and\\ndiverse question-answer datasets with depth using just one topic as a guide.\\nLimitations\\nInitially, a notable limitation is the tendency for both Newbie and Expert robots to become ensnared\\nwithin similar topics. Subsequently, we introduced the Checker to oversee this process, effectively addressing the issue.\\nAnother limitation is that the quality of the generated answers depends on the robot itself, and there is a possibility of\\ngenerating incorrect or even harmful information, equipping the Expert with a trustworthy knowledge base, knowledge\\nretrieval capabilities, and incorporating human verification can to some extent solve this problem.\\n3.2.2\\nData from Chat Robot\\nHaiChat Sever\\nWe developed HaiChat [32] to provide generative model services for HEP researchers. The backend\\nincludes API for close models, local deployment of open-source models, and Xiwu model. High Energy Physics (HEP)\\nresearchers utilize the HaiChat service in their daily work, generating real questions of interest and corresponding\\nanswers. This serves as an important data source for further improving the performance of the Xiwu model.\\nData and Quality\\nAs of now, HaiChat has generated about 600,000 Q&A pairs, of which 8%, or 48,000, are related\\nto high-energy physics. To ensure the quality and accuracy of the data, we employed a multi-step filtering and cleaning\\nprocess. First, we utilized the ChatGPT model to automatically filter out irrelevant or inaccurate dialogues from the\\ncollected question-answer pairs. Then, we performed manual review and editing to further enhance the quality and\\nusability of the data.\\n3.2.3\\nData from Highly Cited Papers\\nData Sources\\nFor the eight sub-domains mentioned above, We analyzed the citation data of papers over the past five\\nyears and used tools to download a carefully selected set of 20,000 papers.\\n6\\n', b'Running Title for Header\\nHaiNougat PDF Parser\\nBased on the Nougat [33] model, we trained a PDF parsing algorithm, HaiNougat [34],\\nusing a domain-specific dataset, which achieves more accurate parsing of formulas and tables of HEP.\\nQ&A Pairs Generation and Human Verification\\nBased on the parsed data, we extracted the core contributions,\\nviewpoints, and results of the papers through prompt engineering, obtaining relevant question-and-answer pairs. To\\nensure quality, these were reviewed by humans.\\nData Collected\\nFrom those highly cited papers, about 8,000 Q&A pairs and 150M tokens were collected.\\n3.2.4\\nAbstracts Data from arXiv\\nKaggle offers comprehensive coverage of research papers across various domains, but it does not provide the full text.\\nWe downloaded approximately 2 million paper abstracts from the aforementioned eight related fields on Kaggle. The\\nquality of these abstract data is very high, they can be directly used for the secondary pre-training of the Xiwu model.\\n3.2.5\\nFinal Dataset\\nAfter data collection and cleaning, a AI-Ready dataset of HEP texts was ultimately formed that can be used for LLM\\ntraining, which includes 750M tokens for secondary pre-training and 26k Q&A pairs for fine-tuning.\\n3.3\\nNeural Networks\\n3.3.1\\nModel\\nBasis Flexible Design\\nXiwu is positioned as a Level 2 LLM. With the goal of minimizing costs while maintaining\\nadvanced capabilities, it has been designed to be adaptable to foundational models of Level 0 or Level 1, including\\nLLaMA, Vicuna, Chat-GLM, Grok-1 and more in the future. This adaptability is implemented in the code repository\\nthrough an \\xe2\\x80\\x99apis\\xe2\\x80\\x99 folder, where interfaces for any model have appropriate API adaptations. For more details, you can\\nrefer to the Github [35] repository.\\nCurrent Foundation Model\\nThe Vicuna [36] model was chosen as the foundational model after a comparative\\nanalysis, as its overall performance surpassed that of others like LLaMA-13B, LLaMA2-13B, Koala-13B, Oasst-\\npythia-13B, Alpaca-13B, and ChatGLM-6B. Vicuna is a Level 1 model based on LLAMA, and currently, we are using\\nVicuna-1.5, which corresponds to LLAMA-2. In this model, the transformer architecture is employed. Compared to\\nRecurrent Neural Networks (RNNs), the transformer model has proven to be superior in quality for many sequence-to-\\nsequence tasks while being more parallelizable.\\nModeling of NLP Tasks\\nThe language modeling task is to assign a probability for the likelihood of a given word\\n(or a sequence of words) to follow a sequence of words. A sequence of tokens are passed to the embedding layer first,\\nfollowed by a positional encoding layer to account for the order of the word. Positional Encoding module injects some\\ninformation about the relative or absolute position of the tokens in the sequence. The BPE (Byte-Pair Encoding) is used,\\nthe size of vocabulary is 32000. The positional encoding, RoRE and Pre-LN (layer normalization) [37] is used..\\nThe Pre RMS Norm is as follows:\\nRMS(x) =\\nv\\nu\\nu\\nt 1\\nN\\nN\\nX\\ni=1\\nx2\\ni\\nThe SwishGLU is used as activation function, there are tree trainable weight matrix in SwishGLU, the formular of\\nSwishGLU is\\nSwishGLU = Swish\\xce\\xb2(xW) \\xe2\\x8a\\x97xV\\nThe nheads is 32, the headdim is 128, the FFNdimension is 11008, the hdimension is 4096.\\n3.3.2\\nTraining Techniques\\nDuring the training process, several advanced techniques are employed to enhance the performance and efficiency of\\nthe Xiwu model. These techniques, namely BF16 and TF32 mixed-precision training, FlashAttention, FSDP (Fully\\nSharded Data Parallelism), CPU-Offload, and Torchrun, played a crucial role in achieving improved training outcomes.\\nIn this section, we provide a detailed explanation of these techniques and their effects.\\n7\\n', b'Running Title for Header\\nFigure 5: The illustration algorithm components and training technologies\\nMixed-precision Training\\nMixed-precision training[38] involves performing computations using a combination\\nof lower (BF16) and higher (TF32) precision formats. By utilizing BF16 for most of the computations and TF32 for\\ncertain critical operations, we were able to reduce memory usage, improve computational throughput, and accelerate\\nthe training process without sacrificing model accuracy.\\nFlashAttention\\nFlashAttention [39] is a technique designed to enhance Flops (floating-point operations per second)\\nutilization during self-attention computations, a critical component in transformer-based models like Xiwu. It optimizes\\nthe memory access patterns and computation flow, resulting in improved computational efficiency and reduced training\\ntime.\\nFSDP\\nFully Sharded Data Parallelism (FSDP)[40] is a data parallelism technique that enables efficient parallel\\ntraining across multiple devices by dividing the model\\xe2\\x80\\x99s parameters into shards and assigning each shard to a different\\ndevice. By distributing the model\\xe2\\x80\\x99s parameters, FSDP reduces memory consumption per device, enables larger models\\nto fit within device memory constraints, and facilitates seamless parallelization for faster and more scalable training.\\nCPU-Offload\\nCPU-Offload[41, 42] is a strategy used to alleviate excessive GPU memory usage during training. By\\noffloading certain computations to the CPU, we were able to free up GPU memory for more efficient memory utilization.\\nThis approach allowed us to train Xiwu on GPUs without encountering memory limitations and improved the overall\\ntraining performance.\\nTorchrun for Single-Node Multi-GPU Parallelism\\nTorchrun is a utility that facilitates single-node multi-GPU\\nparallelism in PyTorch[43], the framework used for developing Xiwu. By leveraging Torchrun, we achieved an effective\\ndistribution of computations across multiple GPUs within a single node, allowing for parallel training and accelerated\\nmodel convergence.\\nBitsandbytes\\nBitsandbytes[44] is an innovative software designed to address the substantial GPU memory require-\\nments encountered by LLMs during inference. Employing vector quantization and mixed-precision decomposition,\\nBitsandbytes substantially reduces memory consumption without compromising on performance. It enables the deploy-\\nment of large-scale models, such as those with 175B parameters like OPT-175B/BLOOM[45], on consumer-grade GPU\\nservers, presenting a highly efficient solution for managing LLMs.\\n8\\n', b'Running Title for Header\\nDeepspeed\\nThe Deepspeed framework[46] is designed to accelerate deep learning training by utilizing mixed\\nprecision, gradient checkpointing, and offloading memory to the host CPU. It allows for larger models and batch sizes,\\nreducing memory requirements and improving training speed.\\nZeRO Series\\nZero Redundancy Optimizer series techniques aim to reduce memory consumption during training\\nby partitioning model weights and optimizer states across multiple devices or nodes. These techniques include\\nZeRO-2[47], ZeRO-3[47], and ZeRO-offload[48]. By leveraging memory optimization strategies such as activation\\ncheckpointing and optimizer state partitioning, ZeRO techniques enable training larger models that would otherwise\\nexceed available memory limits. Adopting ZeRO techniques can potentially allow us to scale up Xiwu and achieve even\\nbetter performance.\\nLoRA\\nLow-Rank Adaptive technique[49] is a low-rank approximation technique that reduces the computational cost\\nof large-scale language models. It achieves this by approximating the weight matrices with low-rank factors while\\npreserving the model\\xe2\\x80\\x99s representational capacity. By leveraging the inherent redundancy in model parameters, LoRA\\nsignificantly reduces both the memory and compute requirements, making training and inference more efficient.\\nThese techniques collectively contributed to the enhanced performance and efficiency of Xiwu during the training\\nprocess. By leveraging mixed-precision training, FlashAttention, FSDP, CPU-Offload, and Torchrun, we achieved\\nimproved memory utilization, accelerated training speed, and scalable parallelization. These advancements enable us to\\ntrain Xiwu more effectively, ultimately leading to improved performance in high-energy physics question-answering\\ntasks.\\n3.3.3\\nComputing Power and Training Settings\\nComputing Power\\nWe conducted full-scale training of the Xiwu-13B model using eight A100 GPUs, each with\\na memory capacity of 40 GB. Additionally, the Xiwu-7B model can be trained on two A100 GPUs with a memory\\ncapacity of 80 GB each.\\nTraining Parameters\\nThe following parameters were carefully selected and tuned to optimize the training process\\nand improve the performance of Xiwu:\\n\\xe2\\x80\\xa2 Learning rate: 2e-5\\n\\xe2\\x80\\xa2 Learning rate scheduler: cosine\\n\\xe2\\x80\\xa2 Weight decay: 0.0\\n\\xe2\\x80\\xa2 Warmup ratio: 0.03\\n\\xe2\\x80\\xa2 Per-device training batch size: 2\\n\\xe2\\x80\\xa2 Gradient accumulation steps: 16\\nLearning Rate\\nThe learning rate determines the step size at which the model adjusts its weights during training.\\nA value of 2e-5 was chosen to strike a balance between ensuring stable convergence and preventing overshooting or\\ngetting stuck in sub-optimal local minima.\\nLearning Rate Scheduler\\nThe cosine scheduler gradually reduces the learning rate as training progresses. It helps\\nthe model to converge smoothly by reducing the learning rate towards the end of training. The cosine scheduler was\\nselected due to its proven effectiveness in stabilizing training and improving the model\\xe2\\x80\\x99s generalization ability.\\nWeight Decay\\nWeight decay is a regularization technique that adds a penalty to the loss function based on the\\nmagnitude of the model\\xe2\\x80\\x99s weights. A weight decay value of 0.0 indicates that no additional regularization is applied\\nduring training, allowing the model to fully utilize the available information in the training data.\\nWarmup Ratio\\nThe warmup ratio determines the proportion of training steps dedicated to gradually increasing the\\nlearning rate from zero to its initial value. A warm-up ratio of 0.03 ensures that the model\\xe2\\x80\\x99s learning rate ramps up\\nslowly at the beginning of training, allowing the model to stabilize and avoid sudden, large weight updates.\\nPer-device Training Batch Size\\nThis parameter determines the number of samples processed in parallel on each\\ndevice during training. A batch size of 2 per device strikes a balance between utilizing the available memory efficiently\\nand maintaining reasonable training speed.\\n9\\n', b'Running Title for Header\\nGradient Accumulation Steps\\nGradient accumulation is a technique that helps simulate larger batch sizes by\\naccumulating gradients over multiple smaller batches before performing weight updates. With 16 gradient accumulation\\nsteps, the model benefits from an effective utilization of computational resources while still obtaining accurate gradient\\nestimates.\\nIt is important to note that the presented configuration and training setup can be adjusted according to the available\\nresources and specific requirements of different applications.\\n3.4\\nJust-In-Time Learning System\\nLimitations of Secondary Training\\nAlthough secondary training of LLMs is effective, it is costly, inefficient, and\\ntraining can lead to a degradation in model performance. For example, when dealing with unseen information, the\\nmodel cannot effectively learn new information under the usual learning intensity (lower learning rates and fewer\\nepochs). If the learning intensity is increased, the model can learn new information, but then issues such as performance\\ndegradation on other Q&A tasks and repetitive answer fragments may occur.\\nJust-In-Time Learning\\nJust-In-Time Learning (JITL) is a learning system that utilizes a vector database for long-term\\nmemory enhancement to augment model capabilities, operating in a basic process equivalent to RAG. We call it JITL to\\nemphasize its learning attributes. JITL is capable of rapid knowledge learning in seconds, specific knowledge updating,\\nand unloading. These features are not present in the traditional model training approach.\\nFigure 6: The just-in-time learning system\\nImplementation of JITL\\nThe architecture of the Just-in-time Learning system is illustrated in FIG. 6, where the core\\nconcept is integrating an external knowledge base as the memory part of the model. New information is saved in the\\nknowledge base, and after retrieval, comprehensive information is outputted. The complete information flow of the\\nJITL system is as follows:\\n1. When an expert user asks the model a specialized question related to HEP and receives a nonsensical answer,\\nthe expert can suggest modifications or directly edit the answer, then teach the model via WebUI or API.\\n2. The system categorizes and organizes the new information, storing it in a text database.\\n3. The text data is processed by a pre-trained embedding model to generate feature vectors, which are then stored\\nin a vector database. These feature vectors capture the semantic information of the text, and semantically\\nsimilar expressions are close to each other in the feature space.\\n10\\n', b'Running Title for Header\\n4. When ordinary users ask the model questions, these go through a question classifier and select an appropriate\\nmodel, while text data is converted into feature vectors by the embedding model. As different texts are\\nseparable in high-dimensional vector space, a simple similarity search can retrieve stored related texts.\\n5. The related texts and query are processed through automatic prompt engineering and then fed into a suitable\\nLLM such as Xiwu. Due to the intent understanding and contextual learning abilities of Xiwu, the final output\\nis an accurate answer that also includes information sources.\\nAdvantages\\nCompared to training models, Just-In-Time learning systems only store vectors after embedding into the\\ndatabase, making the learning process extremely fast, achieving learning in seconds. Knowledge can be dynamically\\nupdated, which is particularly effective for rapidly changing information. Knowledge can also be specifically unloaded, a\\nfeature not achievable in traditional model training processes. Compared to conventional retrieval systems, Just-In-Time\\nlearning systems accomplish semantic-level rather than keyword-level retrieval, resulting in substantial performance\\nimprovements.\\nLimitations\\nJust-In-Time learning systems have limitations. First, the additional vector database search process\\ncan be time-consuming, leading to higher latency compared to pure LLMs. One mitigation strategy is to use GPU\\nacceleration to speed up the search process, another approach is to train the model with information that is always\\ncorrect. The second limitation is that the model doesn\\xe2\\x80\\x99t truly \"understand\" the knowledge in the external database,\\nmeaning there\\xe2\\x80\\x99s no improvement in reasoning ability. This mode is akin to open-book exams where students don\\xe2\\x80\\x99t have\\nto understand the material but can look up information when needed.\\n4\\nEvaluation\\n4.1\\nEvaluation between Models\\nEvaluation Data\\nTo assess the performance of Xiwu in comparison to other models, we conducted a human evaluation\\nusing a set of 5,100 prompts. These prompts consisted of questions related to particle physics, astrophysics, synchrotron\\nphysics, and neutron science. Examples of these questions include: \"What quarks make up protons and neutrons?\",\\n\"What is general relativity?\", \"Why can\\xe2\\x80\\x99t we directly observe dark matter?\", and \"Why is particle therapy more\\nexpensive than X-ray treatment?\". The evaluation dimensions included accuracy, clarity, and fluency. Accuracy assessed\\nwhether the answers correctly addressed the questions and contained the necessary information. Clarity evaluated\\nthe clarity and comprehensibility of the answers, including the use of accessible language and terminology. Fluency\\nassessed the overall fluency and coherence of the answers. Human scientists participated in the evaluation process\\nand compared the answers generated by Xiwu with those from other models, namely Vicuna-13B and ChatGPT. The\\nevaluation results were categorized into three outcomes: Xiwu wins, Xiwu fails, and Draw.\\nComparison Results\\nAs shown in the FIG. 7, Xiwu-13B achieved a win or draw rate of 95% when compared to the\\nbaseline model Vicuna-13B. The performance of Xiwu-13B reached 65% of ChatGPT-175B, while Vicuna achieved\\nless than 10%. These findings indicate that fine-tuning a large-scale model with domain-specific data, even on a model\\nwith 13 billion parameters, can significantly improve the accuracy of domain-specific question answering. Fine-tuning\\nsmaller models based on general-domain knowledge has the potential to achieve or even surpass the performance of\\nlarger models in specialized domains.\\nFigure 7: Human preference evaluation, Compare Xiwu-13B with Vicuna-13B and ChatGPT-175B\\nFine-tuning Cannot Learn New Knowledge\\nFurthermore, we also tested the ability of Xiwu to incorporate internal\\nknowledge that was not pre-trained. Examples of such knowledge included \"What is HepAI?\" (a high-energy physics\\nartificial intelligence platform we are developing) and \"What is HaiGF?\" (an AI application interface framework we are\\n11\\n', b'Running Title for Header\\ndeveloping). The results showed that a limited number of epochs were insufficient to guide Xiwu in answering these\\ntypes of questions. Increasing the number of epochs not only failed to yield correct answers but also led to a decrease in\\nthe model\\xe2\\x80\\x99s performance on other questions.\\n4.2\\nAbsolute Evaluation\\nResults\\nIn the absolute evaluation, human scientists assessed the content of each Xiwu answer, categorizing them\\nas Excellent (Xiwu provided an outstanding response), Pass (Xiwu\\xe2\\x80\\x99s answer was acceptable), or Fail (Xiwu\\xe2\\x80\\x99s answer\\nwas incorrect). During the absolute evaluation, it was observed that occasionally the 13-billion-parameter Xiwu model\\nproduced answers that did not understand the human intent, resulting in responses that were unrelated to the questions.\\nAdditionally, there were instances where the model repeated the same text. This behavior may be attributed to the\\nlack of reinforcement learning from human feedback. Further investigation and incorporating reinforcement learning\\ntechniques are necessary to address these issues.\\n5\\nConclusion\\nFirst LLM for HEP\\nIn this work, we developed Xiwu, the first large language model customized for high energy\\nphysics. Xiwu features a flexible foundational model and two distinct learning systems. The flexibility of the\\nfoundational model allows Xiwu to evolve alongside the development of open-source models. The learning system\\nbased on model training is traditional, enabling the model to effectively learn and \"understand\" HEP knowledge. The\\nvector database-based just-in-time learning system enables rapid learning of new knowledge, dynamic knowledge\\nupdates, and unloading, all at a low cost.\\nResults\\nCurrently, Xiwu is an Level 2 LLM based on Vicuna. Xiwu-13B significantly outperforms the Vicuna-13B\\non the HEP domain Q&A test set, achieving about 65% of the performance of ChatGPT-175B. Furthermore, our\\njust-in-time learning system enables multiple individuals to collectively teach Xiwu, demonstrating the potential for\\ncollaborative AI training. The seed fission technology we have developed has potential for wide application, capable of\\ngenerating Q&A pair data in diverse fields.\\nMeaning\\nThis paper not only presents our findings but also provides the corresponding implementation code, which is\\nreadily available on GitHub [35]. We hope that our work will inspire further research and development in the application\\nof large language models in specialized scientific fields.\\nAcknowledgments\\nThis work is Supported by the Informatization Plan of Chinese Academy of Science, Grant No. CAS-WX2022SF-0104\\nand \"From 0 to 1\" Original Innovation Project of IHEP, Grant No. E3545PU2. We would like to express our gratitude\\nto Beijiang Liu, Yaquan Fang, Gang Li, Wuming Luo, Ye Yuan, Shengsen Sun, Yi Jiao and others who are not listed\\nhere for engaging in beneficial discussions or providing computing resources.\\nReferences\\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint\\narXiv:2303.08774, 2023.\\n[2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\\xc3\\xa9e Lacroix,\\nBaptiste Rozi\\xc3\\xa8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language\\nmodels. arXiv preprint arXiv:2302.13971, 2023.\\n[3] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.\\narXiv preprint arXiv:2307.09288, 2023.\\n[4] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n[5] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by\\ngenerative pre-training. OpenAI blog, 2018.\\n12\\n', b'Running Title for Header\\n[6] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\\nLi, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with\\nmt-bench and chatbot arena, 2023.\\n[7] OpenAI. Introducing chatgpt, 2022.\\n[8] Yoshua Bengio, R\\xc3\\xa9jean Ducharme, and Pascal Vincent. A neural probabilistic language model. Advances in\\nneural information processing systems, 13, 2000.\\n[9] Tom\\xc3\\xa1\\xc5\\xa1 Mikolov et al. Statistical language models based on neural networks. Presentation at Google, Mountain\\nView, 2nd April, 80(26), 2012.\\n[10] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural\\nnetworks. In 2013 IEEE international conference on acoustics, speech and signal processing, pages 6645\\xe2\\x80\\x936649.\\nIeee, 2013.\\n[11] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In Proceedings of the 2013\\nconference on empirical methods in natural language processing, pages 1700\\xe2\\x80\\x931709, 2013.\\n[12] Alex Graves and Alex Graves. Long short-term memory. Supervised sequence labelling with recurrent neural\\nnetworks, pages 37\\xe2\\x80\\x9345, 2012.\\n[13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \\xc5\\x81ukasz Kaiser, and\\nIllia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n[15] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet:\\nGeneralized autoregressive pretraining for language understanding. Advances in neural information processing\\nsystems, 32, 2019.\\n[16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\\narXiv:1907.11692, 2019.\\n[17] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\\nLi, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of\\nmachine learning research, 21(140):1\\xe2\\x80\\x9367, 2020.\\n[18] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as\\ndiscriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\\n[19] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances\\nin neural information processing systems, 33:1877\\xe2\\x80\\x931901, 2020.\\n[20] Deepanway Ghosal, Yew Ken Chia, Navonil Majumder, and Soujanya Poria. Flacuna: Unleashing the problem\\nsolving power of vicuna using flan fine-tuning. arXiv preprint arXiv:2307.02053, 2023.\\n[21] Pritam Mukherjee, Benjamin Hou, Ricardo B Lanfredi, and Ronald M Summers. Feasibility of using the\\nprivacy-preserving large language model vicuna for labeling radiology reports. Radiology, 309(1):e231147, 2023.\\n[22] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language\\nunderstanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\\n[23] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolalpaca: Generalized tool\\nlearning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301, 2023.\\n[24] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\\nBiobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics,\\n36(4):1234\\xe2\\x80\\x931240, 2020.\\n[25] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. arXiv preprint\\narXiv:1903.10676, 2019.\\n[26] Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and Matthew\\nMcDermott. Publicly available clinical bert embeddings. arXiv preprint arXiv:1904.03323, 2019.\\n[27] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew\\nPoulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint\\narXiv:2211.09085, 2022.\\n13\\n', b'Running Title for Header\\n[28] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur,\\nDavid Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. arXiv preprint\\narXiv:2303.17564, 2023.\\n[29] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay\\nTanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. arXiv\\npreprint arXiv:2212.13138, 2022.\\n[30] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin,\\nShifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models\\nvia reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.\\n[31] Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, and\\nJie Tang. Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. arXiv\\npreprint arXiv:2401.07950, 2024.\\n[32] Zhengde Zhang and Yiyu Zhang. Haichat: High energy physics generative artificial intelligence chat robot service.\\nhttps://chat.ihep.ac.cn, 2023. Accessed: 2024-04-07.\\n[33] Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical understanding for\\nacademic documents. arXiv preprint arXiv:2308.13418, 2023.\\n[34] Jianwen Luo and Zhengde Zhang. Hainougat: A academic document parser that preserves formulas and tables for\\nhigh energy physics. https://ai.ihep.ac.cn/m/hai-nougat, 2024. Accessed: 2024-04-07.\\n[35] Zhengde Zhang, Yiyu Zhang, Haodong Yao, Jianwen Luo, Rui Zhao, Bo Huang, Jiaomeng Zhao, Yipu Liao,\\nKe Li, Lina Zhao, Jun Cao, Fazhi Qi, and Changzheng Yuan. Xiwu: A basis flexible and learnable llm for high\\nenergy physics. https://github.com/zhangzhengde0225/Xiwu, 2024.\\n[36] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing\\ngpt-4 with 90%* chatgpt quality, March 2023.\\n[37] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\\nHongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in Neural\\nInformation Processing Systems, 34:19822\\xe2\\x80\\x9319835, 2021.\\n[38] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Gins-\\nburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint\\narXiv:1710.03740, 2017.\\n[39] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\\xc3\\xa9. Flashattention: Fast and memory-efficient\\nexact attention with io-awareness, 2022.\\n[40] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri,\\nMyle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint\\narXiv:2304.11277, 2023.\\n[41] Bharadwaj Pudipeddi, Maral Mesmakhosroshahi, Jinwen Xi, and Sujeeth Bharadwaj. Training large neural\\nnetworks with constant memory using a new execution algorithm. arXiv preprint arXiv:2002.05645, 2020.\\n[42] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W Keckler. vdnn: Virtualized\\ndeep neural networks for scalable, memory-efficient neural network design. In 2016 49th Annual IEEE/ACM\\nInternational Symposium on Microarchitecture (MICRO), pages 1\\xe2\\x80\\x9313. IEEE, 2016.\\n[43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\\nZeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep\\nlearning library. In Advances in Neural Information Processing Systems, volume 32, pages 8026\\xe2\\x80\\x938037. Curran\\nAssociates, Inc., 2019.\\n[44] Dettmers Tim, Belkada Mike, Lewis adn Younes, and Zettlemoyer Luke. bitsandbytes: Highly optimized bit and\\nbyte level operations for deep learning. https://github.com/TimDettmers/bitsandbytes, 2023.\\n[45] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\\xc2\\xb4c, Daniel Hesslow, Roman Castagn\\xc3\\xa9,\\nAlexandra Sasha Luccioni, Fran\\xc3\\xa7ois Yvon, Matthias Gall\\xc3\\xa9, et al. Bloom: A 176b-parameter open-access\\nmultilingual language model. arXiv preprint arXiv:2211.05100, 2022.\\n[46] Microsoft. Deepspeed: A deep learning optimization library. https://github.com/microsoft/DeepSpeed,\\n2023. Accessed: 2023-10-01.\\n[47] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward\\ntraining trillion parameter models. arXiv preprint arXiv:1910.02054, 2020.\\n14\\n', b'Running Title for Header\\n[48] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong\\nLi, and Yuxiong He. {Zero-offload}: Democratizing {billion-scale} model training. In 2021 USENIX Annual\\nTechnical Conference (USENIX ATC 21), pages 551\\xe2\\x80\\x93564, 2021.\\n[49] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\\nChen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\\n15\\n']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Text from paper3.pdf:\n",
            "[b'Posted on 12 Aug 2024 \\xe2\\x80\\x94 CC-BY-NC-SA 4 \\xe2\\x80\\x94 https://doi.org/10.36227/techrxiv.172348951.12175366/v1 \\xe2\\x80\\x94 e-Prints posted on TechRxiv are preliminary reports that are not peer reviewed. They should not b...\\nOptimizing LLM Inference Clusters for Enhanced Performance and\\nEnergy E\\xef\\xac\\x83ciency\\nSoka Hisaharo1, Yuki Nishimura1, and Aoi Takahashi1\\n1A\\xef\\xac\\x83liation not available\\nAugust 12, 2024\\nAbstract\\nThe growing demand for e\\xef\\xac\\x83cient and scalable AI solutions has driven research into optimizing the performance and energy\\ne\\xef\\xac\\x83ciency of computational infrastructures. The novel concept of redesigning inference clusters and modifying the GPT-Neo\\nmodel o\\xef\\xac\\x80ers a signi\\xef\\xac\\x81cant advancement in addressing the computational and environmental challenges associated with AI\\ndeployment.\\nBy developing a novel cluster architecture and implementing strategic architectural and algorithmic changes,\\nthe research achieved substantial improvements in throughput, latency, and energy consumption. The integration of advanced\\ninterconnect technologies, high-bandwidth memory modules, and energy-e\\xef\\xac\\x83cient power management techniques, alongside\\nsoftware optimizations, enabled the redesigned clusters to outperform baseline models signi\\xef\\xac\\x81cantly.\\nEmpirical evaluations\\ndemonstrated superior scalability, robustness, and environmental sustainability, emphasizing the potential for more sustainable\\nAI technologies. The \\xef\\xac\\x81ndings underscore the importance of balancing performance with energy e\\xef\\xac\\x83ciency and provide a robust\\nframework for future research and development in AI optimization. The research contributes valuable insights into the design\\nand deployment of more e\\xef\\xac\\x83cient and environmentally responsible AI systems.\\n1\\n', b'1\\nOptimizing LLM Inference Clusters for Enhanced\\nPerformance and Energy Efficiency\\nSoka Hisaharo*, Yuki Nishimura, and Aoi Takahashi\\nAbstract\\xe2\\x80\\x94The growing demand for efficient and scalable AI\\nsolutions has driven research into optimizing the performance\\nand energy efficiency of computational infrastructures. The novel\\nconcept of redesigning inference clusters and modifying the\\nGPT-Neo model offers a significant advancement in addressing\\nthe computational and environmental challenges associated with\\nAI deployment. By developing a novel cluster architecture and\\nimplementing strategic architectural and algorithmic changes,\\nthe research achieved substantial improvements in throughput,\\nlatency, and energy consumption. The integration of advanced\\ninterconnect technologies, high-bandwidth memory modules, and\\nenergy-efficient power management techniques, alongside soft-\\nware optimizations, enabled the redesigned clusters to out-\\nperform baseline models significantly. Empirical evaluations\\ndemonstrated superior scalability, robustness, and environmental\\nsustainability, emphasizing the potential for more sustainable\\nAI technologies. The findings underscore the importance of\\nbalancing performance with energy efficiency and provide a\\nrobust framework for future research and development in AI\\noptimization. The research contributes valuable insights into the\\ndesign and deployment of more efficient and environmentally\\nresponsible AI systems.\\nIndex Terms\\xe2\\x80\\x94Inference clusters, Performance optimization,\\nEnergy efficiency, AI scalability, Sustainable AI.\\nI. INTRODUCTION\\nT\\nHE optimization of inference clusters for large language\\nmodels (LLMs) holds significant importance due to the\\never-increasing demand for efficient and scalable natural lan-\\nguage processing solutions. As LLMs become more sophisti-\\ncated and widely used, the need to improve their performance\\nwhile minimizing energy consumption becomes paramount.\\nThe open-source LLM GPT-Neo, in particular, offers a valu-\\nable platform for exploring innovative approaches to achieving\\nthese goals. By focusing on redesigning inference clusters,\\nthis research aims to enhance the operational efficiency of\\nGPT-Neo, thereby contributing to the broader field of artificial\\nintelligence.\\nA. Background\\nLLMs have revolutionized the field of natural language\\nprocessing by enabling machines to understand and generate\\nhuman language with remarkable accuracy. These models,\\ntrained on vast datasets, have demonstrated proficiency in a\\nwide range of tasks, including language translation, summa-\\nrization, and question answering. However, the deployment of\\nLLMs in real-world applications poses significant challenges,\\nparticularly in terms of computational requirements and energy\\nconsumption. Inference clusters, which facilitate the execution\\nof these models, play a crucial role in addressing these\\nchallenges. Effective optimization of inference clusters can\\nlead to substantial improvements in model performance and\\nenergy efficiency, making the widespread adoption of LLMs\\nmore feasible and sustainable.\\nThe relevance of GPT-Neo within this context cannot\\nbe overstated. As an open-source alternative to proprietary\\nLLMs, GPT-Neo provides researchers and developers with\\nthe flexibility to modify and enhance the model\\xe2\\x80\\x99s architecture\\nand performance. The ability to experiment with different\\nconfigurations and optimizations makes GPT-Neo an ideal\\ncandidate for this research. By leveraging the unique features\\nof GPT-Neo, this study aims to demonstrate how targeted mod-\\nifications to inference clusters can yield significant benefits in\\nterms of speed, accuracy, and energy consumption.\\nB. Motivation\\nThe motivation behind optimizing inference clusters for\\nLLMs is multifaceted. Firstly, there is a growing demand for\\nreal-time natural language processing applications, such as\\nvirtual assistants, chatbots, and automated content generation\\nsystems. These applications require LLMs to process large\\nvolumes of data quickly and accurately, necessitating the\\ndevelopment of more efficient inference clusters. Secondly,\\nthe environmental impact of deploying LLMs on a large scale\\ncannot be ignored. Data centers, which host inference clusters,\\nconsume substantial amounts of energy, contributing to carbon\\nemissions and environmental degradation. Enhancing the en-\\nergy efficiency of inference clusters is therefore essential for\\nreducing the carbon footprint of AI technologies.\\nFurthermore, the financial cost associated with running\\nLLMs in production environments is a significant considera-\\ntion. Organizations that rely on LLMs for their operations must\\nbalance the need for high performance with the imperative to\\ncontrol operational expenses. By optimizing inference clusters,\\nit is possible to achieve better performance at a lower cost,\\nthereby making LLMs more accessible to a wider range\\nof users and applications. The potential for cost savings,\\ncombined with the environmental benefits, underscores the\\nimportance of this research.\\nC. Objectives\\nThe primary objectives of this research are to redesign\\nthe inference clusters used by GPT-Neo to enhance both\\nperformance and energy efficiency, and to empirically evaluate\\nthe impact of these modifications. Specific objectives include:\\ndeveloping a novel cluster architecture that optimally balances\\ncomputational load and energy consumption; implementing\\nsoftware optimizations to improve the efficiency of inference\\nprocesses; and systematically assessing the performance and\\n', b'2\\nenergy efficiency of the redesigned clusters compared to\\nbaseline configurations.\\nBy achieving these objectives, the research aims to con-\\ntribute valuable insights into the design and optimization of\\ninference clusters for LLMs. The findings are expected to\\ninform best practices for deploying LLMs in various real-\\nworld applications, ultimately leading to more efficient and\\nsustainable AI technologies. The research will also provide a\\nframework for further exploration and experimentation with\\nother open-source LLMs, fostering innovation and collabora-\\ntion within the AI research community.\\nII. RELATED STUDIES\\nA comprehensive examination of existing literature on the\\noptimization of large language models (LLMs) and the de-\\nvelopment of energy-efficient inference clusters is essential\\nfor understanding the current state of the art and identifying\\nareas for further research. This section reviews significant\\ncontributions in the areas of performance optimization and\\nenergy efficiency as they pertain to LLMs, focusing on the\\ntechnical advancements and outcomes achieved through vari-\\nous methodologies.\\nA. Performance Optimization\\nPerformance optimization of LLMs has been a focal point\\nof research, with numerous approaches developed to enhance\\nthe computational efficiency and processing speed of these\\nmodels. Advanced parallelization techniques significantly im-\\nproved the throughput of LLMs, enabling more efficient\\nhandling of large-scale data during inference processes [1],\\n[2]. Leveraging model pruning and quantization, considerable\\nreductions in computational overhead were achieved without\\ncompromising the accuracy of language understanding tasks\\n[3]. The implementation of distributed training frameworks\\nfacilitated the scaling of LLMs across multiple processing\\nunits, thereby accelerating model training and inference [4],\\n[5]. Techniques such as gradient checkpointing and mixed\\nprecision training were employed to reduce memory consump-\\ntion, thus allowing larger models to be deployed on limited\\nhardware resources [6], [7]. Novel architectures, including\\ntransformer variants with optimized attention mechanisms,\\ndemonstrated significant improvements in processing speed\\nand model efficiency [8]. The adoption of adaptive computa-\\ntion strategies enabled dynamic adjustment of computational\\nresources based on the complexity of input data, enhanc-\\ning overall performance [9]. Incorporating efficient data pre-\\nprocessing pipelines streamlined the handling of text data,\\nfurther boosting the performance of LLMs during inference\\n[10], [11]. Custom hardware accelerators, specifically designed\\nfor LLM workloads, provided substantial gains in inference\\nspeed and energy efficiency [12], [13]. Advanced software\\noptimizations, such as optimized matrix multiplication li-\\nbraries, contributed to faster execution of model operations\\n[14]. Utilizing knowledge distillation techniques, smaller and\\nfaster models were derived from larger LLMs, maintaining\\nperformance while reducing computational requirements [15],\\n[16]. The integration of intelligent caching mechanisms re-\\nduced redundant computations, thereby improving inference\\nlatency and throughput [17].\\nB. Energy Efficiency\\nResearch focused on enhancing the energy efficiency of\\ninference clusters for LLMs has yielded various strategies to\\nminimize power consumption while maintaining high perfor-\\nmance levels. Dynamic voltage and frequency scaling (DVFS)\\ntechniques were utilized to adjust the power usage of process-\\ning units based on workload demands, resulting in significant\\nenergy savings [9], [18]. The deployment of energy-aware\\nscheduling algorithms ensured optimal utilization of computa-\\ntional resources, thereby reducing overall energy consumption\\nduring inference [19]. Implementing efficient cooling systems\\nand heat dissipation techniques in data centers contributed to\\nlower energy usage and improved system reliability [20]. The\\nuse of low-power processing units and accelerators specifically\\ndesigned for LLM tasks reduced the energy footprint of\\ninference operations [21], [22]. Techniques such as power\\ngating and clock gating were employed to disable inactive\\ncomponents of the hardware, thereby conserving energy [23].\\nThe development of energy-efficient model architectures, in-\\ncluding those with reduced parameter counts, maintained\\nperformance while operating at lower power levels [24], [25].\\nUtilizing energy-efficient memory hierarchies and data storage\\nsolutions minimized the power required for data access and\\nretrieval during inference [26]. The adoption of renewable\\nenergy sources for powering data centers helped to offset the\\nenvironmental impact of running large-scale LLM inference\\nclusters [27]. Advanced monitoring and management systems\\nenabled real-time tracking of energy usage, facilitating more\\nefficient energy consumption strategies [17]. Implementing\\nvirtualization and containerization technologies allowed for\\nmore flexible and efficient use of computational resources,\\nthereby enhancing energy efficiency [28], [29]. Techniques\\nsuch as workload consolidation and intelligent resource al-\\nlocation optimized the distribution of tasks across available\\nhardware, reducing idle power consumption [30].\\nIII. METHODOLOGY\\nThis section outlines the comprehensive methodology em-\\nployed to redesign the inference clusters and modify GPT-Neo\\nto enhance performance and energy efficiency. The methodol-\\nogy encompasses the redesign of cluster architecture, specific\\nmodifications to the GPT-Neo model, and the evaluation\\nmetrics used to assess the improvements achieved through\\nthese modifications.\\nA. Cluster Redesign\\nThe redesign of the inference clusters involved several\\nstrategic changes aimed at improving both performance and\\nenergy efficiency. A novel cluster architecture was developed,\\nincorporating advanced interconnect technologies to reduce\\nlatency and enhance data transfer speeds across the nodes. By\\noptimizing the spatial arrangement of processing units, signif-\\nicant reductions in communication overhead were achieved,\\n', b'3\\nthereby accelerating inference processes. The integration of\\nhigh-bandwidth memory (HBM) modules facilitated faster\\ndata access and reduced bottlenecks, contributing to overall\\nperformance improvements. Energy-efficient power manage-\\nment techniques, such as dynamic voltage and frequency\\nscaling (DVFS), were implemented to adjust the power usage\\nof the clusters in response to workload demands. Additionally,\\ncustom cooling solutions were deployed to maintain optimal\\noperating temperatures, thereby enhancing the reliability and\\nefficiency of the clusters. The adoption of containerization\\ntechnologies enabled efficient resource allocation and work-\\nload management, further optimizing cluster performance.\\nAdvanced monitoring systems were integrated to provide real-\\ntime insights into cluster performance and energy usage,\\nenabling more effective management and optimization. The\\nuse of machine learning algorithms for predictive maintenance\\nhelped to minimize downtime and ensure consistent perfor-\\nmance levels. Finally, the redesigned clusters incorporated re-\\nnewable energy sources, reducing the overall carbon footprint\\nof the operations.\\nAs illustrated in Figure 1, the integration of high-bandwidth\\nmemory (HBM) modules facilitated faster data access and\\nreduced bottlenecks, contributing to overall performance im-\\nprovements. Energy-efficient power management techniques,\\nsuch as dynamic voltage and frequency scaling (DVFS), were\\nimplemented to adjust the power usage of the clusters in\\nresponse to workload demands. Additionally, custom cooling\\nsolutions were deployed to maintain optimal operating tem-\\nperatures, thereby enhancing the reliability and efficiency of\\nthe clusters. The adoption of containerization technologies en-\\nabled efficient resource allocation and workload management,\\nfurther optimizing cluster performance. Advanced monitoring\\nsystems were integrated to provide real-time insights into\\ncluster performance and energy usage, enabling more effective\\nmanagement and optimization. The use of machine learning\\nalgorithms for predictive maintenance helped to minimize\\ndowntime and ensure consistent performance levels. Finally,\\nthe redesigned clusters incorporated renewable energy sources,\\nreducing the overall carbon footprint of the operations.\\n1) Hardware Configurations: The hardware configurations\\nused in the redesigned clusters were meticulously selected to\\nbalance performance and energy efficiency. High-performance\\nGPUs with tensor cores were employed to accelerate the\\nmatrix computations integral to LLM inference. Each node\\nwas equipped with multiple GPUs, interconnected through\\nhigh-speed NVLink to enhance data transfer rates and reduce\\nlatency. The inclusion of HBM modules provided rapid access\\nto large datasets, minimizing data retrieval times and improv-\\ning overall throughput. Energy-efficient CPUs with advanced\\npower management features were utilized to handle ancillary\\ntasks, ensuring that the primary computational power was\\ndedicated to LLM inference. Custom-designed cooling solu-\\ntions, including liquid cooling systems, were implemented to\\nmaintain optimal temperatures and prevent thermal throttling.\\nThe use of solid-state drives (SSDs) with high read/write\\nspeeds further accelerated data access and storage operations.\\nRedundant power supplies and failover mechanisms were inte-\\ngrated to ensure continuous operation and minimize downtime.\\nFinally, the clusters were configured to operate in a hybrid\\ncloud environment, leveraging both on-premises and cloud-\\nbased resources to optimize performance and scalability.\\n2) Software Optimizations: Several software optimizations\\nwere implemented to enhance the performance and energy\\nefficiency of the redesigned clusters. Customized deep learning\\nframeworks were developed to take full advantage of the\\nhardware configurations, enabling efficient execution of LLM\\ninference tasks. Optimized matrix multiplication libraries, such\\nas cuBLAS and cuDNN, were employed to accelerate com-\\nputational operations. Advanced scheduling algorithms were\\nused to distribute workloads evenly across the clusters, reduc-\\ning idle times and maximizing resource utilization. Techniques\\nsuch as gradient checkpointing and mixed precision training\\nwere applied to reduce memory usage and computational\\noverhead. The integration of intelligent caching mechanisms\\nminimized redundant computations and improved inference\\nlatency. Container orchestration tools, such as Kubernetes,\\nwere used to manage and scale the deployment of LLM models\\nacross the clusters. Real-time performance monitoring and\\nmanagement tools provided insights into system performance,\\nenabling continuous optimization and adjustment of resource\\nallocation. The use of automated deployment pipelines en-\\nsured that updates and patches could be applied seamlessly,\\nmaintaining system stability and performance. Finally, energy-\\nefficient algorithms were implemented to adjust computational\\nloads dynamically, optimizing energy usage without compro-\\nmising performance.\\nB. Modifications to GPT-Neo\\nModifications to the GPT-Neo model were aimed at en-\\nhancing its efficiency and performance during inference. These\\nmodifications included changes to the model architecture and\\nimprovements to the inference algorithms used by GPT-Neo.\\n1) Model Architecture Changes: The architecture of GPT-\\nNeo was modified to optimize its performance for the re-\\ndesigned inference clusters. A more efficient attention mecha-\\nnism was implemented to reduce the computational complexity\\nof processing long sequences. This mechanism enabled the\\nmodel to focus on relevant portions of the input data, thereby\\nimproving processing speed and accuracy. The model\\xe2\\x80\\x99s depth\\nand width were adjusted to balance the trade-off between per-\\nformance and computational requirements. Techniques such as\\nlayer normalization and residual connections were applied to\\nenhance the stability and efficiency of the model. Additionally,\\nthe parameter initialization process was optimized to acceler-\\nate convergence during training and reduce the computational\\noverhead during inference. The integration of a dynamic\\nrouting mechanism enabled the model to adaptively allocate\\ncomputational resources based on the complexity of the input\\ndata, further enhancing performance and efficiency.\\n2) Inference Algorithm Improvements: Several improve-\\nments were made to the inference algorithms used by GPT-Neo\\nto enhance its efficiency and performance. As illustrated in\\nAlgorithm 1, a more efficient beam search algorithm was im-\\nplemented to optimize the generation of output sequences, re-\\nducing computational overhead and improving response times.\\n', b'4\\nGPU 1\\nGPU 2\\nGPU 3\\nGPU 4\\nHigh-Bandwidth Memory\\nDVFS Module\\nCooling System\\nContainerization\\nMonitoring System\\nPredictive Maintenance\\nRenewable Energy\\nFig. 1. Illustration of the redesigned inference cluster architecture incorporating advanced interconnect technologies, high-bandwidth memory, DVFS modules,\\ncustom cooling solutions, containerization, advanced monitoring systems, predictive maintenance, and renewable energy sources.\\nTechniques such as early stopping and dynamic batch sizing\\nwere employed to minimize unnecessary computations, further\\nenhancing inference efficiency. The integration of a caching\\nmechanism allowed for the reuse of previously computed\\nresults, reducing redundant computations and speeding up the\\ninference process. Additionally, the inference pipeline was\\noptimized to take full advantage of the hardware configurations\\nof the redesigned clusters, enabling faster and more efficient\\nprocessing of input data. Advanced load balancing algorithms\\nwere used to distribute inference tasks evenly across the\\navailable computational resources, maximizing throughput and\\nminimizing latency. Finally, the incorporation of model paral-\\nlelism techniques allowed for the distribution of model param-\\neters across multiple GPUs, enabling the efficient handling of\\nlarger models and datasets.\\nAlgorithm 1 Enhanced Inference Algorithm for GPT-Neo\\n1: Input: X = {x1, x2, . . . , xn}\\n\\xe2\\x96\\xb7Input sequence\\n2: H \\xe2\\x86\\x90Initialize hidden states\\n3: C \\xe2\\x86\\x90Initialize cache\\n4: Q \\xe2\\x86\\x90Initialize beam search queue\\n5: for t = 1 to T do\\n6:\\nA \\xe2\\x86\\x90Attention(X, H)\\n7:\\nU \\xe2\\x86\\x90LayerNorm(A)\\n8:\\nR \\xe2\\x86\\x90ResidualConnect(U, H)\\n9:\\nP \\xe2\\x86\\x90DynamicRouting(R, X)\\n10:\\nH \\xe2\\x86\\x90UpdateHiddenStates(P)\\n11:\\nif t \\xe2\\x88\\x88Checkpoints then\\n12:\\nC \\xe2\\x86\\x90Cache(H)\\n13:\\nend if\\n14:\\nO \\xe2\\x86\\x90GenerateOutput(H)\\n15:\\nif O \\xe2\\x88\\x88StopCriteria then\\n16:\\nBreak\\n17:\\nend if\\n18:\\nQ \\xe2\\x86\\x90BeamSearch(O, Q)\\n19: end for\\n20: Output: Y = RetrieveBestSequence(Q)\\nC. Evaluation Metrics\\nThe evaluation of the redesigned inference clusters and\\nmodified GPT-Neo model was conducted using a comprehen-\\nsive set of performance and energy efficiency metrics. Table I\\nsummarizes the key metrics used for this assessment.\\n1) Performance Metrics: Performance metrics were used\\nto assess the efficiency and speed of the redesigned inference\\nclusters and the modified GPT-Neo model. As summarized\\nin Table I, these metrics included throughput, measured as\\nthe number of input sequences processed per second, and\\nlatency, defined as the time taken to generate an output\\nsequence for a given input. Additionally, the accuracy of the\\nmodel\\xe2\\x80\\x99s predictions was evaluated using standard benchmarks\\nand datasets. The scalability of the clusters was assessed\\nby measuring the performance gains achieved through the\\naddition of computational resources. The robustness of the\\nmodel was evaluated by testing its performance under various\\nload conditions and input complexities.\\n2) Energy Efficiency Metrics: Energy efficiency metrics\\nwere employed to evaluate the power consumption and envi-\\nronmental impact of the redesigned inference clusters and the\\nmodified GPT-Neo model. As detailed in Table I, these metrics\\nincluded the power usage effectiveness (PUE), defined as the\\nratio of total energy consumed by the data center to the energy\\nconsumed by the computational resources. Additionally, the\\nenergy per inference, measured as the amount of energy\\nrequired to process a single input sequence, was used to\\nassess the efficiency of the inference process. The carbon\\nfootprint of the operations was evaluated by calculating the\\ntotal greenhouse gas emissions associated with the energy\\nconsumption of the clusters. The effectiveness of the energy-\\nsaving techniques, such as DVFS and power gating, was\\nassessed by comparing the energy usage before and after their\\nimplementation. Finally, the overall energy efficiency of the\\nclusters was evaluated by measuring the performance-to-power\\nratio, defined as the throughput achieved per unit of energy\\nconsumed.\\n', b'5\\nTABLE I\\nEVALUATION METRICS FOR REDESIGNED INFERENCE CLUSTERS AND MODIFIED GPT-NEO\\nCategory\\nMetric\\nDescription\\nPerformance\\nThroughput\\nNumber of input sequences processed per second\\nLatency\\nTime taken to generate an output sequence for a given input\\nAccuracy\\nEvaluation of the model\\xe2\\x80\\x99s predictions using standard bench-\\nmarks and datasets\\nScalability\\nPerformance gains achieved through the addition of compu-\\ntational resources\\nRobustness\\nPerformance under various load conditions and input com-\\nplexities\\nEnergy Efficiency\\nPower\\nUsage\\nEffectiveness\\n(PUE)\\nRatio of total energy consumed by the data center to the\\nenergy consumed by the computational resources\\nEnergy per Inference\\nAmount of energy required to process a single input sequence\\nCarbon Footprint\\nTotal greenhouse gas emissions associated with the energy\\nconsumption of the clusters\\nEnergy-Saving Techniques\\nEffectiveness of techniques such as DVFS and power gating\\nbefore and after implementation\\nPerformance-to-Power Ratio\\nThroughput achieved per unit of energy consumed\\nIV. RESULTS\\nThe results of the experiments conducted to evaluate the re-\\ndesigned inference clusters and the modified GPT-Neo model\\nare presented in this section. The focus is on performance\\nand energy efficiency improvements achieved through various\\nmodifications and optimizations. Detailed analysis of perfor-\\nmance metrics, energy efficiency metrics, and comparisons\\nwith baseline models and clusters are provided.\\nA. Performance Analysis\\nThe performance analysis involved measuring throughput,\\nlatency, accuracy, scalability, and robustness of the redesigned\\ninference clusters and the modified GPT-Neo model. Through-\\nput was measured as the number of input sequences processed\\nper second, while latency was defined as the time taken to gen-\\nerate an output sequence for a given input. The accuracy of the\\nmodel\\xe2\\x80\\x99s predictions was evaluated using standard benchmarks\\nand datasets.\\nAs shown in Figure 2, the redesigned clusters achieved a\\nthroughput of 2500 sequences per second when processing five\\ninput sequences simultaneously, significantly outperforming\\nthe baseline clusters, which achieved a maximum throughput\\nof 1500 sequences per second under the same conditions.\\nLatency measurements, illustrated in Figure 3, indicate that\\nthe redesigned clusters generated output sequences with an\\naverage latency of 50 milliseconds, compared to 80 millisec-\\nonds for the baseline clusters.\\nThe accuracy of the modified GPT-Neo model, evaluated\\nusing the standard GLUE benchmark, showed an improvement\\nof approximately 5% over the baseline model, as depicted\\nin Table II. Scalability was assessed through performance\\ngains achieved via the addition of computational resources,\\nwith the redesigned clusters demonstrating a linear increase\\nin throughput and a proportional decrease in latency, thereby\\nconfirming their superior scalability. Robustness was evalu-\\nated by testing the model\\xe2\\x80\\x99s performance under various load\\n1\\n2\\n3\\n4\\n5\\n0\\n500\\n1,000\\n1,500\\n2,000\\n2,500\\nNumber of Input Sequences\\nThroughput (sequences/sec)\\nThroughput Analysis\\nRedesigned Clusters\\nBaseline Clusters\\nFig. 2.\\nThroughput analysis comparing redesigned clusters and baseline\\nclusters.\\nconditions and input complexities, revealing consistent high\\nperformance across different scenarios.\\nB. Energy Efficiency Analysis\\nThe energy efficiency analysis focused on metrics such\\nas power usage effectiveness (PUE), energy per inference,\\ncarbon footprint, and the performance-to-power ratio. PUE\\nwas defined as the ratio of total energy consumed by the data\\ncenter to the energy consumed by the computational resources.\\n', b'6\\n1\\n2\\n3\\n4\\n5\\n0\\n20\\n40\\n60\\n80\\nNumber of Input Sequences\\nLatency (milliseconds)\\nLatency Analysis\\nRedesigned Clusters\\nBaseline Clusters\\nFig. 3. Latency analysis comparing redesigned clusters and baseline clusters.\\nTABLE II\\nACCURACY ANALYSIS USING GLUE BENCHMARK\\nTask\\nBaseline Accuracy (%)\\nModified GPT-Neo Accuracy (%)\\nCoLA\\n54.8\\n59.7\\nSST-2\\n92.5\\n95.0\\nMRPC\\n88.0\\n91.0\\nSTS-B\\n89.5\\n93.2\\nQQP\\n88.5\\n91.5\\nMNLI\\n84.6\\n89.0\\nQNLI\\n90.4\\n94.2\\nRTE\\n71.8\\n77.6\\nEnergy per inference measured the amount of energy required\\nto process a single input sequence.\\nAs shown in Figure 4, the redesigned clusters consumed\\n22 Joules per inference when processing five input sequences\\nsimultaneously, compared to 42 Joules for the baseline clus-\\nters. The PUE of the redesigned clusters was measured at\\n1.2, indicating a highly efficient energy usage compared to\\nthe baseline clusters with a PUE of 1.5. The carbon footprint\\nanalysis, depicted in Table III, demonstrated a significant\\nreduction in greenhouse gas emissions for the redesigned\\nclusters.\\nThe effectiveness of energy-saving techniques, such as\\ndynamic voltage and frequency scaling (DVFS) and power\\ngating, was assessed by comparing energy usage before and\\nafter their implementation. The redesigned clusters demon-\\nstrated a 30% reduction in energy consumption through these\\ntechniques. The performance-to-power ratio, defined as the\\nthroughput achieved per unit of energy consumed, was mea-\\nsured at 113 sequences per Joule for the redesigned clusters,\\ncompared to 75 sequences per Joule for the baseline clusters,\\nindicating a substantial improvement in energy efficiency.\\n1\\n2\\n3\\n4\\n5\\n0\\n10\\n20\\n30\\n40\\n50\\nNumber of Input Sequences\\nEnergy per Inference (Joules)\\nEnergy per Inference Analysis\\nRedesigned Clusters\\nBaseline Clusters\\nFig. 4.\\nEnergy per inference analysis comparing redesigned clusters and\\nbaseline clusters.\\nC. Comparison with Baseline\\nComparisons with baseline models and clusters were con-\\nducted to highlight the improvements achieved through the\\nredesign and modifications. The redesigned clusters and mod-\\nified GPT-Neo model consistently outperformed the baseline in\\nall evaluated metrics, including throughput, latency, accuracy,\\nscalability, robustness, PUE, energy per inference, carbon\\nfootprint, and performance-to-power ratio.\\nFigure 5 illustrates the performance improvement percent-\\nages over the baseline clusters and models. The redesigned\\nclusters exhibited a 66% improvement in throughput and a\\n37.5% reduction in latency. Accuracy improvements, though\\nmodest at 5%, were significant in the context of language\\nmodel tasks. Scalability and robustness improvements were\\n20% and 15%, respectively, indicating the effectiveness of the\\nredesigned cluster architecture and modified model in handling\\nvarious load conditions. Energy efficiency metrics showed a\\n20% improvement in PUE, a 47.6% reduction in energy per\\ninference, and a 50.6% increase in the performance-to-power\\nratio. The results of the experiments conclusively demonstrate\\nthe benefits of redesigning inference clusters and modifying\\nthe GPT-Neo model in terms of both performance and energy\\nefficiency. These improvements underscore the potential of\\nadvanced cluster architectures and optimized model configu-\\nrations in enhancing the capabilities of large language models\\nfor real-world applications.\\nV. DISCUSSION\\nThe discussion section provides an in-depth analysis of\\nthe implications of the results obtained, acknowledges the\\npotential limitations of the study, and suggests directions\\n', b'7\\nTABLE III\\nCARBON FOOTPRINT ANALYSIS\\nMetric\\nBaseline Clusters\\nRedesigned Clusters\\nTotal Energy Consumption (MWh)\\n1200\\n800\\nGreenhouse Gas Emissions (Metric Tons CO2)\\n600\\n400\\nThroughput\\nLatency\\nAccuracy\\nScalability\\nRobustness\\nPUE\\nEnergy per Inference\\nPerformance-to-Power Ratio\\n0\\n20\\n40\\n60\\nMetric\\nPerformance Improvement (%)\\nPerformance Improvement over Baseline\\nFig. 5.\\nPerformance improvement percentages over baseline clusters and\\nmodels.\\nfor future research. Through a detailed examination of the\\nfindings, this section aims to contextualize the significance of\\nthe research within the broader field of large language model\\noptimization.\\nA. Broader Impact of Findings\\nThe findings from the redesign of inference clusters and\\nmodifications to the GPT-Neo model have far-reaching im-\\nplications for the field of natural language processing. The\\nsignificant improvements in throughput and latency highlight\\nthe potential for more efficient real-time processing of large\\nvolumes of data, which is critical for applications such as\\nautomated customer support, real-time translation services,\\nand large-scale text analysis. The enhancements in accuracy,\\nalthough modest, suggest that even small architectural and\\nalgorithmic modifications can yield meaningful improvements\\nin performance, thereby enhancing the overall reliability and\\neffectiveness of language models. Furthermore, the demon-\\nstrated scalability and robustness indicate that the redesigned\\nclusters and modified model can handle varying loads and\\ncomplex input scenarios, making them suitable for deploy-\\nment in diverse operational environments. The reduction in\\nenergy consumption and carbon footprint underscores the\\nenvironmental benefits of optimizing computational resources,\\naligning with global efforts to develop more sustainable AI\\ntechnologies. These findings collectively contribute to the\\nongoing discourse on balancing performance with energy\\nefficiency in the development and deployment of large-scale\\nAI systems.\\nB. Constraints and Limitations\\nDespite the promising results, the study is not without its\\nlimitations. One primary limitation pertains to the scope of\\nthe evaluation, which focused on specific performance and\\nenergy efficiency metrics. While comprehensive, the study\\ndid not account for other potential factors such as the long-\\nterm operational stability of the redesigned clusters and the\\npotential impact of hardware failures or network disruptions.\\nAdditionally, the modifications made to the GPT-Neo model,\\nwhile effective, were limited to architectural and algorithmic\\nchanges; further improvements could potentially be achieved\\nthrough more extensive hyperparameter tuning and exploration\\nof alternative optimization techniques. The experimental setup,\\nalthough rigorous, was constrained by the availability of\\ncomputational resources, which may limit the generalizability\\nof the results to different hardware configurations and oper-\\national environments. Furthermore, the reliance on standard\\nbenchmarks and datasets, while necessary for consistency, may\\nnot fully capture the diverse range of real-world scenarios in\\nwhich the modified model and clusters would be deployed.\\nThese limitations should be considered when interpreting the\\nresults and their broader applicability.\\nC. Prospects for Future Exploration\\nFuture research should build on the findings of this study\\nto explore additional avenues for optimizing large language\\nmodels and their underlying computational infrastructure. One\\npotential direction involves investigating the impact of ad-\\nvanced hyperparameter tuning techniques on the performance\\nand efficiency of the modified GPT-Neo model. Exploring\\nthe integration of novel hardware accelerators, such as tensor\\nprocessing units (TPUs) and application-specific integrated\\ncircuits (ASICs), could further enhance the performance and\\nenergy efficiency of inference clusters. Additionally, research\\ncould examine the potential benefits of incorporating more\\nsophisticated cooling solutions and advanced power manage-\\nment strategies to further reduce energy consumption. Another\\npromising area of exploration involves the application of fed-\\nerated learning and distributed training techniques to optimize\\nthe training and inference processes across geographically\\n', b'8\\ndispersed data centers. Finally, future studies should consider\\na broader range of evaluation metrics, including operational\\nstability, fault tolerance, and adaptability to dynamic network\\nconditions, to provide a more holistic assessment of the\\noptimized clusters and model.\\nVI. CONCLUSION\\nThe research conducted on the redesign of inference clusters\\nand the modifications made to the GPT-Neo model has yielded\\nsignificant insights and advancements in the field of natural\\nlanguage processing. The comprehensive analysis presented\\nin this article highlights the substantial improvements in per-\\nformance and energy efficiency achieved through strategic ar-\\nchitectural and algorithmic changes. This section summarizes\\nthe key findings and contributions of the research, providing\\nfinal thoughts and reflections on the broader implications of\\nthe work.\\nA. Summary of Contributions\\nThe primary contributions of this research include the\\ndevelopment of a novel cluster architecture that incorporates\\nadvanced interconnect technologies, high-bandwidth memory\\nmodules, and energy-efficient power management techniques.\\nThese innovations have led to significant reductions in latency\\nand communication overhead, thereby enhancing the overall\\nperformance of the inference clusters. The modifications to\\nthe GPT-Neo model, including the implementation of a more\\nefficient attention mechanism, dynamic routing, and optimized\\nparameter initialization, have further improved the model\\xe2\\x80\\x99s\\nprocessing speed and accuracy. The integration of software\\noptimizations, such as advanced scheduling algorithms, in-\\ntelligent caching, and real-time performance monitoring, has\\nenabled more efficient resource allocation and workload man-\\nagement. The empirical evaluation conducted using compre-\\nhensive performance and energy efficiency metrics has demon-\\nstrated the superior scalability, robustness, and environmental\\nsustainability of the redesigned clusters and modified model.\\nThese contributions collectively advance the state of the art\\nin large language model optimization, providing a robust\\nframework for future research and development in this domain.\\nB. Final Remarks\\nThe findings of this research underscore the critical impor-\\ntance of balancing performance with energy efficiency in the\\ndevelopment and deployment of large-scale AI systems. The\\ndemonstrated improvements in throughput, latency, and energy\\nconsumption highlight the potential for more sustainable and\\nefficient natural language processing solutions. The broader\\nimplications of this work extend beyond technical advance-\\nments, emphasizing the necessity of integrating environmental\\nconsiderations into the design of computational infrastructure.\\nBy continuing to refine and expand upon the optimizations\\npresented in this article, researchers and practitioners can con-\\ntribute to the development of AI technologies that are not only\\nmore powerful but also more environmentally responsible. The\\nresearch provides a solid foundation for future exploration,\\nencouraging further innovation and collaboration within the\\nAI research community. Through ongoing efforts to optimize\\ninference clusters and language models, the field can achieve\\nsignificant strides toward more sustainable and effective AI\\nsolutions, ultimately benefiting a wide range of applications\\nand industries.\\nREFERENCES\\n[1] G. Choquet, A. Aizier, and G. Bernollin, \\xe2\\x80\\x9cExploiting privacy vulnera-\\nbilities in open source llms using maliciously crafted prompts,\\xe2\\x80\\x9d 2024.\\n[2] D. Fares, \\xe2\\x80\\x9cThe role of large language models (llms) driven chatbots\\nin shaping the future of government services and communication with\\ncitizens in uae,\\xe2\\x80\\x9d 2023.\\n[3] L. Danas, \\xe2\\x80\\x9cSecurity and interpretability in large language models,\\xe2\\x80\\x9d 2024.\\n[4] J. Kundu, W. Guo, A. BanaGozar, U. De Alwis, S. Sengupta, P. Gupta,\\nand A. Mallik, \\xe2\\x80\\x9cPerformance modeling and workload analysis of dis-\\ntributed large language model training and inference,\\xe2\\x80\\x9d arXiv preprint\\narXiv:2407.14645, 2024.\\n[5] R. Fredheim, \\xe2\\x80\\x9cVirtual manipulation brief 2023/1: Generative ai and its\\nimplications for social media analysis,\\xe2\\x80\\x9d 2023.\\n[6] J. Owens and S. Matthews, \\xe2\\x80\\x9cEfficient large language model inference\\nwith vectorized floating point calculations,\\xe2\\x80\\x9d 2024.\\n[7] A. Laverghetta Jr, \\xe2\\x80\\x9cA psychometric analysis of natural language infer-\\nence using transformer language models,\\xe2\\x80\\x9d 2023.\\n[8] D. De Bari, \\xe2\\x80\\x9cEvaluating large language models in software design: A\\ncomparative analysis of uml class diagram generation,\\xe2\\x80\\x9d 2024.\\n[9] S. R. Cunningham, D. Archambault, and A. Kung, \\xe2\\x80\\x9cEfficient training\\nand inference: Techniques for large language models using llama,\\xe2\\x80\\x9d 2024.\\n[10] K. V. Day, \\xe2\\x80\\x9cTraining a massively multimodal transformer on youtube\\ndata: pre-training and parameter efficient fine-tuning on hpc infrastruc-\\nture,\\xe2\\x80\\x9d 2023.\\n[11] F. Liang, Z. Zhang, H. Lu, C. Li, V. Leung, Y. Guo, and X. Hu,\\n\\xe2\\x80\\x9cResource allocation and workload scheduling for large-scale distributed\\ndeep learning: A survey,\\xe2\\x80\\x9d arXiv preprint arXiv:2406.08115, 2024.\\n[12] F. Dall\\xe2\\x80\\x99Agata, \\xe2\\x80\\x9cInstructing network devices via large language models,\\xe2\\x80\\x9d\\n2024.\\n[13] M. Jakesch, Assessing the Effects and Risks of Large Language Models\\nin AI-Mediated Communication.\\nCornell University, 2022.\\n[14] C. Donner, \\xe2\\x80\\x9cMisinformation detection methods using large language\\nmodels and evaluation of application programming interfaces,\\xe2\\x80\\x9d 2024.\\n[15] T. R. McIntosh, T. Susnjak, T. Liu, P. Watters, and M. N. Halgamuge,\\n\\xe2\\x80\\x9cFrom google gemini to openai q*(q-star): A survey of reshaping the\\ngenerative artificial intelligence (ai) research landscape,\\xe2\\x80\\x9d arXiv preprint\\narXiv:2312.10868, 2023.\\n[16] S.-h. Huang and C.-y. Chen, \\xe2\\x80\\x9cCombining lora to gpt-neo to reduce large\\nlanguage model hallucination,\\xe2\\x80\\x9d 2024.\\n[17] Z. Du and K. Hashimoto, \\xe2\\x80\\x9cExploring sentence-level revision capabilities\\nof llms in english for academic purposes writing assistance,\\xe2\\x80\\x9d 2024.\\n[18] E. Vaillancourt and C. Thompson, \\xe2\\x80\\x9cInstruction tuning on large language\\nmodels to improve reasoning performance,\\xe2\\x80\\x9d 2024.\\n[19] A. Gundogmusler, F. Bayindiroglu, and M. Karakucukoglu, \\xe2\\x80\\x9cMathemat-\\nical foundations of hallucination in transformer-based large language\\nmodels for improvisation,\\xe2\\x80\\x9d 2024.\\n[20] T. Goto, K. Ono, and A. Morita, \\xe2\\x80\\x9cA comparative analysis of large\\nlanguage models to evaluate robustness and reliability in adversarial\\nconditions,\\xe2\\x80\\x9d 2024.\\n[21] M. Huang, A. Shen, K. Li, H. Peng, B. Li, and H. Yu, \\xe2\\x80\\x9cEdgellm:\\nA highly efficient cpu-fpga heterogeneous edge accelerator for large\\nlanguage models,\\xe2\\x80\\x9d arXiv preprint arXiv:2407.21325, 2024.\\n[22] H. Gupta, \\xe2\\x80\\x9cInstruction tuned models are quick learners with instruction\\nequipped data on downstream tasks,\\xe2\\x80\\x9d 2023.\\n[23] T. Hata and R. Aono, \\xe2\\x80\\x9cDynamic attention seeking to address the\\nchallenge of named entity recognition of large language models,\\xe2\\x80\\x9d 2024.\\n[24] J. Stojkovic, C. Zhang, \\xc2\\xb4I. Goiri, J. Torrellas, and E. Choukse, \\xe2\\x80\\x9cDy-\\nnamollm: Designing llm inference clusters for performance and energy\\nefficiency,\\xe2\\x80\\x9d arXiv preprint arXiv:2408.00741, 2024.\\n[25] H. Fujiwara, R. Kimura, and T. Nakano, \\xe2\\x80\\x9cModify mistral large perfor-\\nmance with low-rank adaptation (lora) on the big-bench dataset,\\xe2\\x80\\x9d 2024.\\n[26] K. Fujiwara, M. Sasaki, A. Nakamura, and N. Watanabe, \\xe2\\x80\\x9cMeasuring\\nthe interpretability and explainability of model decisions of five large\\nlanguage models,\\xe2\\x80\\x9d 2024.\\n[27] K. Dave, \\xe2\\x80\\x9cAdversarial privacy auditing of synthetically generated data\\nproduced by large language models using the tapas toolbox,\\xe2\\x80\\x9d 2024.\\n', b'9\\n[28] G. Fazlija, \\xe2\\x80\\x9cToward optimising a retrieval augmented generation pipeline\\nusing large language model,\\xe2\\x80\\x9d 2024.\\n[29] A. Katal, S. Dahiya, and T. Choudhury, \\xe2\\x80\\x9cEnergy efficiency in cloud\\ncomputing data centers: a survey on software technologies,\\xe2\\x80\\x9d Cluster\\nComputing, vol. 26, no. 3, pp. 1845\\xe2\\x80\\x931875, 2023.\\n[30] T. Dyde, \\xe2\\x80\\x9cDocumentation on the emergence, current iterations, and\\npossible future of artificial intelligence with a focus on large language\\nmodels,\\xe2\\x80\\x9d 2023.\\n']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Text from paper8.pdf:\n",
            "[b'Can Private LLM Agents Synthesize Household Energy\\nConsumption Data?\\nMahathir Almashor\\xe2\\x88\\x97\\nmahathir.almashor@csiro.au\\nCSIRO Energy\\nSydney, Australia\\nYusuke Miyashita\\xe2\\x88\\x97\\nyusuke.miyashita@csiro.au\\nCSIRO Energy\\nMelbourne, Australia\\nSam West\\xe2\\x88\\x97\\nsam.west@csiro.au\\nCSIRO Energy\\nNewcastle, Australia\\nThi Van Dai Dong\\xe2\\x88\\x97\\nthivandai.dong@csiro.au\\nCSIRO Energy\\nWollonggong, Australia\\nABSTRACT\\nReproducible science requires easy access to data, especially with\\nthe rise of data-driven and increasingly complex models used within\\nenergy research. Too often however, the data to reconstruct and\\nverify purported solutions in publications is hidden due to some\\ncombination of commercial, legal, and sensitivity issues. This early\\nwork presents our initial efforts to leverage the recent advance-\\nments in Large Language Models (LLMs) to create usable and share-\\nable energy datasets. In particular, we\\xe2\\x80\\x99re utilising their mimicry of\\nhuman behaviors, with the goal of extracting and exploring syn-\\nthetic energy data through the simulation of LLM agents capable of\\ninteracting with and executing actions in controlled environments.\\nWe also analyse and visualise publicly available data in an attempt\\nto create realistic but not quite exact copies of the originals. Our\\nearly results show some promise, with outputs that resemble the\\ntwin peak curves for household energy consumption. The hope is\\nthat our generalised approach can be used to easily replicate usable\\nand realistic copies of otherwise secret or sensitive data.\\nCCS CONCEPTS\\n\\xe2\\x80\\xa2 Computing methodologies \\xe2\\x86\\x92Multi-agent systems; Natural\\nlanguage generation; \\xe2\\x80\\xa2 Security and privacy \\xe2\\x86\\x92Social aspects\\nof security and privacy; \\xe2\\x80\\xa2 Information systems \\xe2\\x86\\x92Data ana-\\nlytics.\\nKEYWORDS\\nSynthetic Data, Generative AI, Large Language Models, Household\\nElectricity Consumption\\nACM Reference Format:\\nMahathir Almashor, Yusuke Miyashita, Sam West, and Thi Van Dai Dong.\\n2024. Can Private LLM Agents Synthesize Household Energy Consumption\\nData?. In The 15th ACM International Conference on Future and Sustainable\\n\\xe2\\x88\\x97Commonwealth Science and Industrial Research Organisation (CSIRO) Energy\\nThis work is licensed under a Creative Commons\\nAttribution-NonCommercial-ShareAlike International 4.0 License.\\nE-Energy \\xe2\\x80\\x9924, June 04\\xe2\\x80\\x9307, 2024, Singapore, Singapore\\n\\xc2\\xa9 2024 Copyright held by the owner/author(s).\\nACM ISBN 979-8-4007-0480-2/24/06\\nhttps://doi.org/10.1145/3632775.3661993\\nEnergy Systems (E-Energy \\xe2\\x80\\x9924), June 04\\xe2\\x80\\x9307, 2024, Singapore, Singapore. ACM,\\nNew York, NY, USA, 5 pages. https://doi.org/10.1145/3632775.3661993\\n1\\nINTRODUCTION\\nThe energy community is working tirelessly on the transition to\\nrenewables and there is a pressing need to share data across research\\norganisations. However, the sharing of useful and contemporary\\ndata in many domains is often an exercise fraught with hurdles\\nand competing motives. The barriers to effective sharing range\\nfrom privacy and cyber-security concerns, through to competing\\ncommercial interests. Given the urgency and enormity of the task\\nat hand, we need a way for partners and collaborators across the\\nglobe to effectively evaluate their solutions against agreed datasets.\\nIt is in this spirit that we present this very early work exploring\\nthe generation of synthetic yet realistic data within the energy\\ndomain. Our aim is arrive at method to generate raw household\\nenergy consumption data that (i) reflects the demographics and\\nbehaviours of a particular geographic region; (ii) be realistic and\\nemergent in a way that accounts for the seemingly stochastic nature\\nof human actions; and (iii) be free of the encumbrances that typically\\nprevent the wider dissemination of data between institutions.\\nTo this end, we propose the use of private Large Language Mod-\\nels (LLMs) [3] in the synthesis of such data. We see a range of\\npossibilities in the abilities of current models to drive interactions\\nwithin multi-agent simulations, and want to leverage their unique\\nso-called hallucinations to arrive at realistic behavioural patterns.\\nPut simply, we want our LLM-powered agents to \\xe2\\x80\\x9cdream\\xe2\\x80\\x9d about\\ntheir day-to-day actions and organically arrive at energy consump-\\ntion patterns that mirror real-life. The reasons are as follows:\\n\\xe2\\x80\\xa2 As was seen in [11], emergent behaviours were seen when the\\nauthors simulated a small town of 25 agents powered by ChatGPT\\n[10]. This included an unscripted event where the agents created\\na mayoral election and then proceeded to realistically interact\\nwith one another about it. This is the same phenomena we wish\\nto capture, where the agents can naturally vary their behaviours\\nand activities during the day to form a better mimicry of real-life\\nhumans. This includes variations that account for their specific\\ncharacterisations (i.e., identity), occupations, and interactions\\nwith immediate family members.\\n\\xe2\\x80\\xa2 We use the term Private LLM both in terms of their localised\\nnature, as well as their inherent privacy benefits. That is, rather\\nthan depend on cloud-based and costly implementations such as\\n664\\n', b'E-Energy \\xe2\\x80\\x9924, June 04\\xe2\\x80\\x9307, 2024, Singapore, Singapore\\nAuthors et al.\\nOpenAI\\xe2\\x80\\x99s ChatGPT, we strove for smaller models that can be run\\nwithin any organisation\\xe2\\x80\\x99s infrastructure. While the performance\\nof these local models may not be on par with their more famous\\nbrethren, better accessibility and lower costs presents a good\\ncounterbalance. In this way, the barrier to entry is lowered for\\nmost institutions when replicating our methods and results.\\n\\xe2\\x80\\xa2 Private also refers to the ability to keep an organisation\\xe2\\x80\\x99s valuable\\nintellectual property safer and only within its confines. There\\nis no need to share text prompts, techniques and potential seed\\ndata with any third-party service as everything is run on-premise\\nor within that organisation\\xe2\\x80\\x99s own cloud infrastructure.\\nThere is another dimension which delves further into privacy,\\nand is the reason why we chose household energy consumption at\\nthe first attempt. Simply put, it is difficult to obtain current, usable\\nand customisable datasets because it invariably impacts real-life\\nprivacy concerns. We cannot effectively measure the daily consump-\\ntion of a human household, let alone the hundreds or thousands\\nwithin a geographical region. In commercial settings, there are legal\\nconstraints on the collection of consumption data [13], given the in-\\ntrusiveness and sensitivities such gathering would entail. We often\\nhave to rely on observations, household demographics, presence of\\nappliances, and wider energy usage that are entirely self-reported\\n[1], with all the data quality issues that entails. In any case, electrical\\nload is typically only measured at the meter, requiring heuristics or\\nadditional hardware to ascertain the presence and consumption of\\nindividual appliances. It is impractical to measure consumption of\\neach appliance and outlet for an entire city, not to mention how in-\\nvasive and time-consuming this would be for any given household.\\nOur contributions may be summarised as follows:\\n\\xe2\\x80\\xa2 A novel addition to an LLM-enabled simulation engine to gen-\\nerate emergent daily routines of multiple agents, and the subse-\\nquent extraction of corresponding energy patterns.\\n\\xe2\\x80\\xa2 The customisation of an existing simulation engine with a pri-\\nvate LLM implementation called Mistral that can be run within\\nlocalised infrastructure and without costly ongoing access fees.\\n\\xe2\\x80\\xa2 Experimentation with our approach that exhibits promising abil-\\nity to replicate the consumption patterns seen in publicly avail-\\nable datasets that were discovered and analysed.\\nEthical Considerations\\nNo personally identifiable information (PII) or other sensitivities\\nwere found in the household electricity consumption data used\\nbeyond anonymised age-brackets, demographics and post-codes.\\n2\\nBACKGROUND & RELATED WORK\\n2.1\\nSynthetic Energy Data\\nThe use of GANs for generating energy time series data has recently\\ngained prominence, fueled by the increased use of time series data\\nacross various domains. One objective in generating time series is\\nto accurately capture temporal dynamics. Some earlier approaches,\\nsuch as C-RNN-GAN [9] and RGAN/RCGAN [4] have been de-\\nveloped to learn the temporal variations of data. However, these\\napproaches require real data for model train, which in turn poses\\nthe risk of leaking sensitive information in any generated outputs.\\n00:00\\n01:00\\n02:00\\n03:00\\n04:00\\n05:00\\n06:00\\n07:00\\n08:00\\n09:00\\n10:00\\n11:00\\n12:00\\n13:00\\n14:00\\n15:00\\n16:00\\n17:00\\n18:00\\n19:00\\n20:00\\n21:00\\n22:00\\n23:00\\nTime\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\nEnergy Consumption (Kwh)\\nDaily Energy Consumption\\nRandom Household\\nAggregated Mean of All Household\\nFigure 1: Snapshots of daily and aggregated mean of typical\\nhousehold energy consumption.\\nOur approach attempts to overcome this issue by conducting\\nhousehold activities within an LLM-powered simulation world, and\\nsubsequently extracting energy data from those activities. This\\nessentially avoids the use of any real data altogether and thus\\nminimising any privacy or sensitivity concerns. In other words,\\nthere is little to no risk of inadvertently replicating a household\\xe2\\x80\\x99s\\nexact consumption patterns, as the genesis of the data creation is\\nentirely independent from any real-life source.\\n2.2\\nLLM and LLM Agents\\nLarge Language Models, epitomised by groundbreaking models\\nlike OpenAI\\xe2\\x80\\x99s GPT-3[2], FaceBook\\xe2\\x80\\x99s Llama2[14] and Mistral[6] AI\\xe2\\x80\\x99s\\neponymous model have showcased strong abilities to interpret, gen-\\nerate, and simulate human-like text. It would be an understatement\\nto say that LLM technologies have gained popularity recently, with\\nvarious research looking at everything from teasing out hidden\\nmeanings in speech [5] to using their generation capabilities in\\nsoftware programming tasks [7].\\nBeyond the obvious chat-bot category of applications, there\\nis also a significant amount of research in LLM agents aimed at\\nimitating human behavior. A primary example of this is Simulacra\\n[11], which introduces a fusion between LLM and computationally\\ninteractive agents in a sandbox environment to enable believable\\nsimulations of human behavior. Similarly, MemGPT [8] uses LLM\\nas an operating system, allowing it to think, reflect, and produce\\nactions to interact with external devices. Here, we utilized the\\nMistral-7B private LLM to power the Simulacra agents instead of\\nthe original ChatGPT backend, due to cost and accessibility reasons.\\n2.3\\nBenchmark Datasets\\nThe Smart Grid Smart City Customer (SGSC) [1] data was collected\\nbetween 2010 and 2014 as part of a joint industry and government\\ninitiative. It was one of only a few openly available datasets on\\nhousehold energy consumption that we discovered. It contains\\n30-minute interval readings of electricity usage and generation\\n(measured in kWh) for 78,720 participating customers, of which we\\nonly had access to a subset of 13,735 households.\\nIn addition, we also discovered the Solar Cities dataset [12],\\nwhich contained energy consumption and generation information\\n665\\n', b'Can Private LLM Agents Synthesize Household Energy Consumption Data?\\nE-Energy \\xe2\\x80\\x9924, June 04\\xe2\\x80\\x9307, 2024, Singapore, Singapore\\nfor almost 38,000 homes in seven Australian cities, recorded at\\n30-minute intervals from 2005 to 2013. After an extensive pre-\\nprocessing step, we focused on only 4,332 households due to various\\ndata quality and accessibility issues. This data was collected mainly\\nas part of a governmental initiative design to measure the impact\\nof direct interventions on household usage patterns.\\nIn much the same way, there is the possibility to design for\\nand simulate interventions to consumption patterns within our pro-\\nposed approach. In [12], consumption was measured for households\\nbefore and after an intervention such as the installation of solar\\npanels, in an attempt to gauge their impacts to electricity demands.\\nSimilarly, we can use our LLM-enabled approach to simulate the\\nsame objective, by introducing interventions and appropriate pric-\\ning dynamics, and watching how the agents respond. There is also\\nthe ability to model the impact of incentives on the agents, and\\nmeasure the organic spread of interventions introduced gradually.\\nFigure 1 shows the daily energy consumption for randomly cho-\\nsen household (in blue) as well as aggregated mean of all house-\\nholds in our dataset (in red). Daily energy consumption varies for\\neach household, but we see the trend of two peaks in the morning\\naround 8am and evening around 7pm. We see the typical morning\\nand evening peaks of energy usage in the aggregated mean, which\\nis a well-known phenomena observed in individual households and\\nup to the wider grid. On the other hand we witness the expected\\nvariability in energy usage for a single household on any given day.\\nFigure 2: Example simulation step of two agents in their daily\\nactivities, with one using an electrical appliance (their TV).\\n3\\nAPPROACH\\nThe approach involves two stages, with the first stage consisting\\nof running the simulacra, and the second involving the extraction\\nof household energy data from the simulation using a variety of\\nmethods to perform this extraction. For each step of the simulation,\\ndescription of each persona\\xe2\\x80\\x99s actions and objects they are interact-\\ning with is generated. They are in the format of \"Persona A is doing\\nAction B at Location C\". From these, appliances which consumes\\nenergy are identified. The advantage of this two-stage approach\\nis its applicability onto any other LLM simulator. That is, simply\\nallow the LLM to record its actions at each step, and then proceed\\nwith extracting useful information from it.\\n3.1\\nPrivate LLM in Simulacra\\nUtilizing ChatGPT or any other API-based LLM can offer conve-\\nnience, reliability, and strong performance. However, this approach\\nmay entail transferring sensitive data and routing it through ex-\\nternal services. In contrast, the deployment of Private/Local LLM\\nenables the execution of the entire simulation within closed envi-\\nronments, ensuring the security and privacy of the data.\\nWe utilized Mistral-7B as it is one of the highest performing\\nsmaller models which are relatively fast when running iterative\\nexperiments, and more importantly, can be fit into more modest\\ncompute infrastructures. \\xe2\\x80\\x9c7B\\xe2\\x80\\x9d here refers to 7 Billion parameters,\\nwith some LLM models reaching 65B and more. Suffice it to say for\\nnow that the smaller the number of parameters, the more manage-\\nable it is to run within an organisation\\xe2\\x80\\x99s infrastructure.\\nThis smaller Mistral model is then plugged into the Simulacra\\nplatform [11] as its LLM engine. The authors recommend a ro-\\nbust LLM for the agents, ideally as proficient as or superior to\\nChatGPT-3.5 Turbo as using smaller models may degrade the sim-\\nulation quality. However, we\\xe2\\x80\\x99ve found that they still exhibit the\\nability to generate reasonable plans and actions, resulting in a plau-\\nsible human-like energy patterns. For simplicity, we\\xe2\\x80\\x99ve reused the\\nprompts within the original paper to condition our LLM agents.\\n3.2\\nEnergy Data Extraction\\n3.2.1\\nString Match. In this straightforward approach, we employ\\nstring matching to determine whether the descriptions of daily\\nactivities from the simulations include energy appliances. This\\nbaseline is perhaps the most accurate reflection of energy data\\nextraction from agent activities. For example in Figure 2, the simu-\\nlation step is described as \\xe2\\x80\\x9cMaria Lopez is watching TV@common\\nroom sofa, Klaus Mueller is having dinner@kitchen sink\\xe2\\x80\\x9d, the ap-\\npliance TV is easily matched and included as a proxy for energy use.\\nThe identified list of appliances in the simulations are as follows:\\nTV, shower, refrigerator, toaster, cooking area, microphone, piano,\\ngame console, computer desk, and computer.\\nSo, for every mention of an appliance in each time-step\\xe2\\x80\\x99s de-\\nscription, we mark that as an active use of that appliance. A simple\\ncounting of all the appliances used in the household is performed\\nfor each time-step in the simulated day. This allows us to get a mea-\\nsure of energy consumption purely from the activities of agents\\nwith regards to appliances, as they go about their day.\\n3.2.2\\nOther Approaches. In our quest to extract more accurate en-\\nergy usage, we also experimented with Semantic Embedding, which\\nemploys a text encoder to match the description of LLM generated\\ninteractions against appliances based on semantic meaning. One\\nadvantage with this method is its ability to capture synonyms or\\nphrasings describing the same appliance. For instance, when the de-\\nscription states \\xe2\\x80\\x9cMaria is streaming on Twitch\\xe2\\x80\\x9d, we can surmise that\\nthis would involve the use of either a game console or a computer.\\nThis semantic embedding method may better capture such cases\\ncompared to basic text matching. However, setting a threshold for\\nmatching top-K text embeddings was found to be challenging and\\nresulted in inaccuracies with the extraction of energy data.\\nWe also experimented with using the LLM engine itself to in-\\nfer appliance usage. LLMs have the capability to extract potential\\nappliance use even if they are not present in the environment, or\\nif the LLM agents are not explicitly described as interacting with\\nthem. Thus, with a line like \\xe2\\x80\\x9cMaria reads a book while listening to\\n666\\n', b'E-Energy \\xe2\\x80\\x9924, June 04\\xe2\\x80\\x9307, 2024, Singapore, Singapore\\nAuthors et al.\\n0\\n5\\n10\\n15\\n20\\nHours\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nState of Appliances\\nIsabella House and Cafe Daily Usage\\n0\\n5\\n10\\n15\\n20\\n25\\nHours\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nState of Appliances\\nMaria and Klaus Sharehouse Daily Usage\\n0\\n5\\n10\\n15\\n20\\n25\\nHours\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nState of Appliances\\nLin Household Daily Usage\\n0\\n5\\n10\\n15\\n20\\n25\\nHours\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nState of Appliances\\nMoreno Household Daily Usage\\nFigure 3: Single day energy usage for four simulated households\\nmusic\\xe2\\x80\\x9d, the LLM may infer that an appliance like a radio or TV is\\nalso turned on for the purposes of that background music. However,\\nthe challenge here is the occurrence of hallucinations, making it\\ndifficult to precisely control their outputs. After extensive analysis,\\nboth Semantic and LLM methods were abandoned in favour of the\\nbasic but most accurate string matching technique.\\n4\\nRESULTS\\nThe status of each appliance at every step is binary, indicating\\nwhether it is on or off. This information is consolidated across all\\nappliances to generate discrete values for energy usage. Further-\\nmore, we implemented a rolling mean with a 1-hour window to\\nsmooth out our data, aiming to alleviate the instantaneous rises\\nand drops in energy usage, given that energy usage is treated in\\nbinary states. Figure 3 depicts the daily energy usage patterns of our\\nfour simulated households, which include (clockwise from top-left)\\nIsabella, Lin, Moreno, and Maria and Klaus. This is represented\\nagainst the rolling mean of appliance states at each time-step.\\nAs can be seen, the daily energy data for each household varies\\naccording to their routines, occupations, and lifestyles. This is ex-\\nactly the variety in energy usage patterns we were aiming for, which\\nresembles the variances seen in recorded single-day usage seen in\\nFigure 1. The energy usage reflects the simulated activities of the\\nLLM agents. For example, the agent Isabella wakes up around 6 am,\\nopens her cafe at 8 am, works at the counter until 8 pm, closes the\\ncafe, and goes to bed around 11 pm. These activities correspond to\\nactions such as turning on the lights in the cafe, serving customers,\\nusing the refrigerator, toaster, and cooking area, which collectively\\ncontributes to the overall energy consumption pattern.\\nSimilarly, these variances are observed for the Lin and Moreno\\nhouseholds. The unique shapes of each household\\xe2\\x80\\x99s curve indicates\\nthat both the prompt of the LLMs and the simulation environment\\ninfluence their activities, resulting in their distinct energy consump-\\ntion curves. This suggests that by conditioning the prompt, one can\\ngenerate tunable synthetic energy data for the desired household,\\nwithin the limits of the simulated environment.\\nFurthermore, to determine the average daily energy usage per\\nperson, the mean is calculated from the combined data by summing\\nall the households\\xe2\\x80\\x99 consumption at each time-step. In diagram 4,\\nwe illustrate two peaks near morning and evening times, capturing\\nthe demand periods often seen on weekdays in many aggregated\\nenergy datasets. The fluctuations and small peaks during the day\\nmay arise from the nature of the simulation environments, such as\\nIsabella running a cafe (which is counted against her household\\xe2\\x80\\x99s\\ndata) and LLM agents returning home for lunch breaks.\\n5\\nCONCLUSION\\nThe proposed approach uses private LLMs to synthesize daily house-\\nhold energy consumption patterns. This is done with privacy in\\nmind, and by using the emergent properties of LLMs to arrive at\\nrealistic datasets that can be then be freely shared amongst the\\nenergy community. The focus is also on using less computationally\\nintensive and costly LLMs that allows for much easier adoption.\\n5.1\\nLimitations & Future Work\\nThe simulation results heavily depend on the capabilities of the LLM,\\nas more advanced LLMs would lead to more realistic simulations\\nand extracted energy data. While early results somewhat resembles\\nthe real data, the LLM simulation outputs only binary states of\\nappliances, which doesn\\xe2\\x80\\x99t capture continuously varying loads well,\\nand cannot simulate the majority of detailed activities in the cities\\nfrom which the real data was measured. The translation from state\\nto actual usage remains a potential area for future research.\\nFigure 4: Aggregated mean of energy usage from all four\\nhouseholds containing a total of eight agents.\\n667\\n', b'Can Private LLM Agents Synthesize Household Energy Consumption Data?\\nE-Energy \\xe2\\x80\\x9924, June 04\\xe2\\x80\\x9307, 2024, Singapore, Singapore\\nGenerating more detailed energy data will require integrating\\nmore detailed interactions with appliances, simulating many more\\nagents and will likely require integrating additional factors like\\nclimate, weather, traffic, seasons, demographics, and industries. Fur-\\nthermore, the simulation does not assess climate control systems\\n(e.g. air conditioning), lighting, or transport, despite their signifi-\\ncant electricity consumption at home. We posit that they could be\\ntreated as constant factors used while occupants are at home, thus\\nnot affecting the dynamic usage of appliances. Doubtlessly, there\\nremains a host of further experimentation and rigorous analysis to\\nprove the effectiveness of this approach. The hope is that the wider\\nenergy research community will see its value, and collaborate to\\ngenerate publicly available realistic synthetic datasets.\\nACKNOWLEDGMENTS\\nThis work was supported by resources provided by the Pawsey\\nSupercomputing Centre with funding from the Australian Govern-\\nment and the Government of Western Australia.\\nREFERENCES\\n[1] Energy Australian Government Department of Climate Change. 2014. Smart-Grid\\nSmart-City Customer Trial Data. https://www.data.gov.au/data/dataset/smart-\\ngrid-smart-city-customer-trial-data Last Modified: 2022-04-11T01:46:18.101034.\\n[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\\narXiv:2005.14165 [cs.CL]\\n[3] Vinton G. Cerf. 2023. Large Language Models. Commun. ACM 66, 8 (July 2023),\\n7. https://doi.org/10.1145/3606337\\n[4] Crist\\xc3\\xb3bal Esteban, Stephanie L Hyland, and Gunnar R\\xc3\\xa4tsch. 2017. Real-valued\\n(medical) time series generation with recurrent conditional gans.\\n[5] Fan Huang, Haewoon Kwak, and Jisun An. 2023. Is ChatGPT better than Human\\nAnnotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate\\nSpeech. In Companion Proceedings of the ACM Web Conference 2023 (WWW \\xe2\\x80\\x9923\\nCompanion). Association for Computing Machinery, New York, NY, USA, 294\\xe2\\x80\\x93297.\\nhttps://doi.org/10.1145/3543873.3587368\\n[6] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-\\nvendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, L\\xc3\\xa9lio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\\xc3\\xa9e Lacroix,\\nand William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL]\\n[7] Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma, Barbara J. Ericson, David\\nWeintrop, and Tovi Grossman. 2023. Studying the effect of AI Code Generators\\non Supporting Novice Learners in Introductory Programming. In Proceedings\\nof the 2023 CHI Conference on Human Factors in Computing Systems (CHI \\xe2\\x80\\x9923).\\nAssociation for Computing Machinery, New York, NY, USA, 1\\xe2\\x80\\x9323. https://doi.\\norg/10.1145/3544548.3580919\\n[8] MemGPT Community. 2024. Introduction. https://memgpt.readme.io/docs/index\\n[9] Olof Mogren. 2016. C-RNN-GAN: Continuous recurrent neural networks with\\nadversarial training.\\n[10] OpenAI Blog. 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt\\n[11] Joon Sung Park, Joseph O\\xe2\\x80\\x99Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy\\nLiang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simu-\\nlacra of Human Behavior. In Proceedings of the 36th Annual ACM Symposium on\\nUser Interface Software and Technology (UIST \\xe2\\x80\\x9923). Association for Computing\\nMachinery, New York, NY, USA, 1\\xe2\\x80\\x9322. https://doi.org/10.1145/3586183.3606763\\n[12] Saad Sayeef, Sam West, Stephen Lindsay, Brad Sparkes, and Kate Cavanagh.\\n2013. Solar Cities Data Analysis Final Report.\\nhttps://publications.csiro.au/\\nrpr/pub?list=SEA&pid=csiro:EP137924&sb=RECENT&expert=false&n=5&rpp=\\n25&page=1&tr=6&q=Solar%20Cities%20Data%20Analysis&dr=all\\nPublisher:\\nDepartment of Resources, Energy and Tourism.\\n[13] Ren\\xc3\\xa9 Schwermer, Jonas Buchberger, Ruben Mayer, and Hans-Arno Jacobsen.\\n2022. Federated office plug-load identification for building management systems.\\nIn Proceedings of the Thirteenth ACM International Conference on Future Energy\\nSystems (e-Energy \\xe2\\x80\\x9922). Association for Computing Machinery, New York, NY,\\nUSA, 114\\xe2\\x80\\x93126. https://doi.org/10.1145/3538637.3538845\\n[14] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\\nale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucu-\\nrull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia\\nGao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,\\nRui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,\\nTodor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,\\nJeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\\nEric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross\\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,\\nYuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Ro-\\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2:\\nOpen Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]\\n668\\n']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Text from paper1.pdf:\n",
            "[b'1\\nTowards Greener LLMs: Bringing Energy-Efficiency\\nto the Forefront of LLM Inference\\nJovan Stojkovic, Esha Choukse\\xe2\\x80\\xa0, Chaojie Zhang\\xe2\\x80\\xa0, Inigo Goiri\\xe2\\x80\\xa0, Josep Torrellas\\nUniversity of Illinois at Urbana-Champaign\\n\\xe2\\x80\\xa0Microsoft Azure Research - Systems\\nAbstract\\xe2\\x80\\x94With the ubiquitous use of modern large language\\nmodels (LLMs) across industries, the inference serving for these\\nmodels is ever expanding. Given the high compute and memory\\nrequirements of modern LLMs, more and more top-of-the-\\nline GPUs are being deployed to serve these models. Energy\\navailability has come to the forefront as the biggest challenge for\\ndata center expansion to serve these models. In this paper, we\\npresent the trade-offs brought up by making energy efficiency\\nthe primary goal of LLM serving under performance SLOs.\\nWe show that depending on the inputs, the model, and the\\nservice-level agreements, there are several knobs available to\\nthe LLM inference provider to use for being energy efficient.\\nWe characterize the impact of these knobs on the latency,\\nthroughput, as well as the energy. By exploring these trade-\\noffs, we offer valuable insights into optimizing energy usage\\nwithout compromising on performance, thereby paving the way\\nfor sustainable and cost-effective LLM deployment in data center\\nenvironments.\\nIndex Terms\\xe2\\x80\\x94Large Language Models, LLMs, energy effi-\\nciency, efficient AI\\nI. INTRODUCTION\\nM\\nODERN generative large language models (LLMs) are\\nturning ubiquitous in their use-cases, leading to large-\\nscale inference deployments. This has lead the datacenter\\nexpansion to hit an energy wall for the foreseeable future [6],\\nfurther delaying the green energy promises. At the same time,\\nsuch large scale deployments of these models present a unique\\nopportunity to optimize the service for energy efficiency with\\nhuge impacts. Previous work in the field of LLM inference\\nplatforms has focused on improving latency and throughput of\\nthe serving platforms. However, we note that just like any other\\nservice, even LLM inference has periods of lower utilization,\\nleading to slack time compared to the latency and throughput\\nSLOs. We use this insight to explore various energy efficiency\\nknobs available to the inference service provider.\\nLLM inference environments have various sources of inef-\\nficiency. Prior work attacked some of the largest ones, such\\nas inefficient request scheduling and batching [1], [30], [39],\\nmemory management and key-value caching of intermediate\\nresults [3], [22], speculative decoding [27] or model paral-\\nlelism [24].\\nHowever, one aspect that has been largely overlooked is the\\nenergy consumption of LLM inference servers. Despite the\\nwidespread use of LLMs and their serving engines, there is a\\nnotable absence of a comprehensive framework for managing\\nenergy in these systems. While existing research has high-\\nlighted the unique performance challenges of LLM inference\\nservers, understanding how these issues translate into power\\nand energy consumption, as well as designing effective power\\nmanagement strategies, remains largely unexplored. Advanc-\\ning research in this area is critical, as LLM services are an\\nincreasing fraction of data center loads [12], and data centers\\ncontribute substantially to the world energy consumption [4],\\n[26] and carbon footprint [14], [15].\\nTo address this shortcoming with current LLMs and infer-\\nence platforms, this paper performs a thorough characteriza-\\ntion of energy consumption in LLM inference environments\\nunder various settings. The goal of the characterization is\\nto generate datasets that can provide insights and guide the\\ndesign of future energy management frameworks specifically\\ndesigned for LLM inference servers.\\nOur characterization shows that LLM inference environ-\\nments pose a set of challenges not met by the existing power\\nand energy management schemes designed for traditional\\nlatency-critical data-center applications ( [7], [8], [16], [17],\\n[19], [25], [28], [41]). First, a fundamental challenge lies in\\nthe variable nature of loads encountered by LLM inference\\nservers, akin to user-facing applications. However, the requests\\nfor LLM models exhibit a remarkable diversity, with inputs\\nranging from short (a few tokens) to long (a few thousands\\nof tokens) and outputs displaying similar variability. Longer\\ninputs necessitate increased GPU parallelism, resulting in\\nextended prefill phases, while longer outputs induce multiple\\niterations and elongated decode phases. It is known that prefill\\nphase puts more pressure on the compute resources, while\\ndecode phase puts more pressure on the memory subsys-\\ntem [30], [31]. Consequently, bursts of requests of one type\\nmanifest distinct behaviors compared to those of another type,\\ncomplicating load and energy management strategies.\\nSecond, LLM inference servers can be organized into var-\\nious configurations concerning the degree and type of paral-\\nlelism, batch sizes, and GPU frequencies. Each configuration\\nmay be optimal for specific system states. For instance, low\\nloads of requests with short inputs and outputs may operate at\\nlower GPU frequencies and with fewer GPUs (smaller degrees\\nof tensor/pipeline parallelism). On the other hand, high loads\\nof requests with long inputs and outputs require high GPU\\nfrequencies and many GPUs (larger degrees of tensor/pipeline\\nparallelism). Yet the rapid fluctuations in load and software-\\nlevel overheads exacerbate the challenges associated with\\ntransitioning between configurations. For instance, adjusting\\nGPU frequency via the CPU controller incurs significant stalls,\\nwhile re-sharding the model into a new pipeline or tensor\\norganization proves to be prohibitively expensive in terms of\\ncomputational resources and time.\\narXiv:2403.20306v1  [cs.AI]  29 Mar 2024\\n', b'2\\nTo effectively manage energy consumption in LLM in-\\nference environments, it is imperative to develop strategies\\nthat accommodate the dynamic and heterogeneous nature of\\nworkload characteristics. These strategies must incorporate\\nmechanisms for adaptive resource allocation, ensuring that\\ncomputational resources are efficiently utilized in response\\nto evolving workload demands. Additionally, optimizations\\naimed at minimizing the overhead of configuration changes,\\nsuch as GPU frequency adjustments and model reorganization,\\nare vital for enhancing energy efficiency without compromis-\\ning inference performance. By addressing these challenges,\\nwe can pave the way for the development of sustainable and\\nenergy-efficient LLM inference systems, thereby facilitating\\ntheir widespread adoption across various domains.\\nContributions of this paper are as follows:\\n\\xe2\\x80\\xa2 Characterization of the LLM inference environments\\nfrom the perspective of energy efficiency.\\n\\xe2\\x80\\xa2 Analysis of available knobs to LLM inference servers and\\ntheir impact on the performace-energy trade-off.\\n\\xe2\\x80\\xa2 An outline of the requirements for an energy-efficient\\nLLM inference framework.\\nII. BACKGROUND\\nA. LLM overview\\nModern LLMs are predominantly built upon transformer\\narchitectures [38], which have revolutionized natural language\\nprocessing tasks. These transformer models leverage attention\\nmechanisms and multi-layer-perceptron (MLP) layers to ef-\\nfectively process inputs and generate corresponding outputs.\\nThe architecture of transformer-based LLMs can vary, with\\nconfigurations such as encoder-only [10], decoder-only [35],\\nor encoder-decoder [36] models. In encoder-only models, the\\ninput text is processed to create contextualized representations,\\nwhich are then utilized for downstream tasks. Conversely,\\ndecoder-only models focus on generating output sequences\\nbased on given inputs, leveraging the context encoded in the\\ninput embeddings. Encoder-decoder models, on the other hand,\\ncombine both encoder and decoder components, enabling\\ntasks like machine translation and text summarization where\\nthe model processes input text and generates corresponding\\noutput text. In this paper we focus on generative LLMs which\\nare usually either decoder-only or encoder-decoder models.\\nThe models are auto-regressive, where each output token is\\ngenerated sequentially with a forward pass of the model.\\nB. Batching and parallelism\\nLLM inference batching refers to the process of grouping\\nmultiple input sequences together and processing them simul-\\ntaneously during inference, exploiting this parallelism to im-\\nprove efficiency. Moreover, batching enables better hardware\\nutilization, leveraging the capabilities of modern computa-\\ntional resources such as GPUs and TPUs more effectively. By\\nenhancing inference speed and resource utilization, batching\\nplays a crucial role in scaling up LLM deployment for various\\napplications, and making it energy-efficient.\\nAnother widely used mechanism to increase throughput of\\nLLM inference is the parallelism or sharding of the model\\nacross GPUs. Tensor parallelism divides the model\\xe2\\x80\\x99s param-\\neters across multiple devices, such as GPUs or TPUs, for\\nparallel computation per layer in the model inference. This ap-\\nproach optimizes hardware utilization, accelerating inference\\nby distributing computations in each layer. Pipeline parallelism\\nsplits the model\\xe2\\x80\\x99s layers or modules into stages, executing\\nthem sequentially across different devices. By overlapping\\ncomputation and communication, it minimizes idle time and\\nmaximizes throughput. Together, tensor and pipeline paral-\\nlelism enable efficient and scalable LLM inference, handling\\nmassive textual data effectively.\\nC. SLOs in LLMs today\\nModern LLMs are assessed based on performance SLOs,\\nincluding Time to First Token (TTFT), Time Between Tokens\\n(TBT), and throughput. TTFT measures the time for the model\\nto generate the first token of the output sequence, especially\\nimportant for interactive and streaming responses. TBT quanti-\\nfies the time spent between each output token, as it is generated\\nin an auto-regressive manner. Meeting these SLOs is crucial\\nfor ensuring timely and efficient responses across a range of\\napplications, from chatbots to translation services, demanding\\ncareful optimization of model and hardware configurations.\\nD. Energy efficiency knobs in modern GPUs\\nModern GPUs do not offer the vast majority of efficiency\\nknobs that the modern CPUs offer [32]. For instance, voltage\\nscaling, fine-grained power gating, efficient modes, and fine-\\ngrained frequency scaling are not offered by the GPUs today.\\nHowever, they do offer frequency control at a GPU-wide\\ngranularity. Lowering the frequency during periods of low\\nactivity reduces power consumption without sacrificing a lot\\nof performance.\\nIII. METHODOLOGY\\nWe present detailed characterization results on a recent\\nopen-source LLM Llama-2 with 70 billion parameters [37].\\nPrevious work has shown that other models like BLOOM-\\n176B [34] closely correlate with each other in performance\\ntrends [30]. We run our experiments on an NVIDIA DGX-\\nH100 [29] using vLLM [23], a state-of-the-art open-source\\nLLM inference platform.\\nThe maximum frequency for NVIDIA H100 is 1980 MHz.\\nWe run our experiments on H100 with frequency varying\\nbetween 800 MHz to 1980 MHz in jumps of 200 MHz. Unless\\nspecified otherwise, the experiment runs using 8-way tensor\\nparallelism. For latency SLOs, we choose 5\\xc3\\x97 the TTFT and\\nTBT achieved when running the request alone, without any\\nbatching or queuing delay.\\nIV. ENERGY EFFICIENCY TRADE-OFFS\\nAt the platform level, we explore 3 levers: 1) workload type,\\n2) batching, and 3) model parallelism.\\nWe define the workload type as a combination of the\\ninput and output lengths of the queries, combined with the\\ntotal requests per second being sent to a model instance. We\\n', b'3\\nS,S\\nS,M\\nS,L\\nM,S\\nM,M\\nM,L\\nL,S\\nL,M\\nL,L\\nInput, Output Type\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nNormalized TTFT\\nFreq. [GHz]: \\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\nFig. 1.\\nNormalized TTFT varying GPU frequencies for different in-\\nputs/outputs.\\nS,S\\nS,M\\nS,L\\nM,S\\nM,M\\nM,L\\nL,S\\nL,M\\nL,L\\nInput, Output Type\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nNormalized TBT\\nFreq. [GHz]: \\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\nFig. 2. Normalized TBT varying GPU frequencies for different inputs/outputs.\\nS,S\\nS,M\\nS,L\\nM,S\\nM, M\\nM,L\\nL,S\\nL,M\\nL,L\\nInput, Output Type\\n0\\n10\\n20\\n30\\n40\\nMax. T-put [RPS]\\nFreq. [GHz]: \\n0.8\\n1.0\\n1.2\\n1.4\\n1.6\\n1.8\\n2.0\\nFig. 3.\\nMaximum throughput of an 8-way tensor-parallel GPU LLama2\\ninstance with different GPU frequencies for different input/output types.\\nS,S\\nS,M\\nS,L\\nM,S\\nM,M\\nM,L\\nL,S\\nL,M\\nL,L\\nInput, Output Type\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nNorm. Power\\nFreq. [GHz]: \\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\nFig. 4.\\nNormalized power consumption of an 8-way tensor-parallel GPU\\nLLama2 instance with different GPU frequencies for different request types.\\nS,S\\nS,M\\nS,L\\nM,S\\nM,M\\nM,L\\nL,S\\nL,M\\nL,L\\nInput, Output Type\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nNorm. Energy\\nFreq. [GHz]: \\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\nFig. 5.\\nNormalized energy consumption of an 8-way tensor-parallel GPU\\nLLama2 instance with different GPU frequencies for different request types.\\nconsider this a lever for the platform, since several scheduling\\nand scaling algorithms at the service\\xe2\\x80\\x99s cluster can define these\\nproperties as seen at a model instance. We combine these\\nlevers with the only energy-efficiency knob available to us\\nin modern GPUs at a node-level: frequency scaling.\\nA. Impact of workload type\\nWe divide the workload into buckets based on the input\\nand output length in number of tokens. Both input and output\\nlengths are divided into three buckets: Small (100 tokens for\\ninput and 50 tokens for output), Medium (500 and 128 tokens\\nfor input and output respectively), and Large (1024 tokens for\\ninput, 256 tokens for output). Combining the input and output\\nlengths from different buckets gives us 9 types of input/output\\nworkload types. We run a streaming workload through the\\nmodel instance.\\nLatency Figures 1 and 2 show the impact of the frequency\\nsetting on the TTFT and TBT latency metrics for the different\\nworkload types. As the input length increases, the computa-\\ntional intensity of the prefill phase increases. Therefore, we see\\na clear pattern, where the TTFT gets increasingly impacted by\\nfrequency and lowering as the prompt length increases. In fact,\\nthe large inputs increase the computational intensity enough\\nto cause throttling due to power overdraw - this reduces the\\nimpact of frequency capping on large inputs. Furthermore,\\nlonger output lengths running at lower frequency increase\\nthe queuing time, adding to the TTFT. On the other hand,\\nthe decode phase is memory bound, and growing the input\\nor output length has an imperceptible impact on the TBT\\xe2\\x80\\x99s\\nfrequency response (Figure 2).\\nThroughput Figure 3 shows the maximum throughput\\nachievable under the SLOs for various workload types while\\nchanging their frequency. We note that the throughput is\\nheavily affected by both the input and output lengths. Longer\\ninputs lead to higher TBT for the requests that get their decode\\nphase batched with the prefill phase. Longer outputs lead to\\nqueuing delay as the model instance spends more number\\nof iterations on each request. The small input and small\\noutput setup achieves the highest throughput and reducing the\\nfrequency by half, only reduces the throughput by \\xe2\\x88\\xbc20%.\\nEnergy Figure 4 shows the maximum power draw for differ-\\nent workload types with frequency capping. Each data point is\\nshown at a medium load the corresponding configuration can\\nsupport. Comparing Figure 4 with Figures 1 to 3 shows that\\nwe can achieve \\xe2\\x88\\xbc20% lower power for most configurations\\nwithout any impact to the latency or throughput. Furthermore,\\nif the workload is going through a low utilization phase, the\\npower required can be reduced even further, at no impact to\\nthe workload.\\nFigure 5 shows the corresponding energy consumption. It\\nis evident that optimizing for power vs energy vs performance\\nwould lead to very different frequency configurations for the\\ninference platform.\\nB. Impact of parallelism\\nNext, we vary the tensor parallelism degree across different\\nnumber of GPUs under medium load and medium input/output\\nworkload types. Tensor parallelism divides the KV heads of\\nthe model equally across the GPUs. Therefore, we use tensor\\nparallelism degrees of 2, 4, and 8 within a single DGX-H100\\nnode, and name these configurations TP2, TP4, and TP8.\\nLatency Figures 6 and 7 show that increasing parallelism\\nreduces both TTFT and TBT as tensor parallelism effectively\\nparallelizes computation within the layers. However, as com-\\nmunication overhead also grows, larger parallelism does not\\nachieve linear latency reduction. As computational intensity\\nremains high during prefill phase, TTFT exhibits similar\\n', b'4\\nfrequency responses across parallelisms. In contrast, increasing\\nparallelism reduces the computation on each GPU during\\ndecode phase, TBT\\xe2\\x80\\x99s frequency impact further decreases.\\nThroughput Figure 8 shows the maximum throughput\\nachievable by each of the parallelism configurations under\\nlatency SLOs. As expected, the increase in throughput going\\nfrom TP2 to TP4 is much higher (75%) compared to the\\nincrease from TP4 to TP8 (40%). In either case, if optimizing\\nfor cost per request, as long as TP2 meets the latency SLOs,\\nhaving 2 instances of TP2 is better than TP4. Similarly, two\\ninstances of TP4 would be better than TP8.\\nEnergy Figures 9 and 10 show the maximum power and\\ntotal energy consumption at medium load for each configura-\\ntion. These are particularly interesting, since the normalized\\ntotal energy for TP2 is only 40% lower than TP8, which\\nbeing able to serve only 60% fewer requests. Furthermore,\\nmost cloud environments today only allow full node access\\n(8 GPUs) to DGX-H100 nodes. This means that in times of\\nlower throughput needs, it is more energy-efficient to run TP8\\nthan TP2!\\nC. Impact of batching\\nAs mentioned before, we use mixed batching of prefill\\nand decode phases. We increase the maximum allowed batch\\nsize and observe the patterns in performance and energy with\\nfrequency scaling.\\nLatency Figures 11 and 12 show the TTFT and TBT\\nwhen batching. At maximum frequency, the TTFT slightly\\ndecreases as the maximum batch size is increased. This is due\\nto reduction in queuing of the request before it gets batched\\nin for inference. At lower frequencies, this effect is even more\\npronounced. With very low load, we see the opposite trend,\\nwhere the TTFT increases due to increased computational\\ncomplexity as the batch size increases. Since queuing delay\\ndoes not impact TBT due to no preemption in this setup, we\\ndo not see such impact on the decode phase. Additionally,\\ndecode phase being memory-bound does not experience slow\\ndown as the batch size increases up to 64.\\nThroughput Figure 13 shows the maximum throughput for\\ndifferent batch sizes. Since larger batches can lead to TTFT\\nSLO misses, the throughput increase on doubling the batch\\nsize is not double. In fact, under SLOs, a batch size of 64 has\\nonly 7\\xc3\\x97 higher throughput than a batch size of 4.\\nEnergy Figures 14 and 15 show the normalized maximum\\npower and total energy observations. For most batch sizes, run-\\nning the GPUs at 1.6GHz instead of 2GHz yields about the\\nsame throughput at less than 80% of the energy. Additionally,\\nduring phases of lower throughput needs in a service, reduce\\nthe maximum batch size can reduce consumed energy by up\\nto 15%.\\nV. RELATED WORK\\nLLM workload characterization A large body of work\\nhas characterized LLM inference workloads focusing on per-\\nformance and utilization [18], [40]. Moreover, some works\\nhave looked at LLM workloads from power [31], carbon [11],\\nand energy [33] perspective. In light of this, it becomes evident\\nTP2\\nTP4\\nTP8\\nTensor Parallelism\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\nNorm. TTFT\\nFreq. [GHz]: \\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\nFig. 6. TTFT of an GPU LLama2 instance under medium load and medium\\ninput/output request types with different GPU frequencies for different tensor\\nparallelism strategies.\\nTP2\\nTP4\\nTP8\\nTensor Parallelism\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nNorm. TBT\\nFreq. [GHz]: \\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\nFig. 7.\\nTBT of a GPU LLama2 instance under medium load and medium\\ninput/output request types with different GPU frequencies for different tensor\\nparallelism strategies.\\nTP2\\nTP4\\nTP8\\nTensor Parallelism\\n0\\n2\\n4\\n6\\n8\\nMax T-put [RPS]\\nFreq [GHz]: \\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\nFig. 8.\\nMaximum throughput of a LLama2 instance under medium in-\\nputs/outputs request types with different GPU frequencies for different tensor\\nparallelism strategies.\\nTP2\\nTP4\\nTP8\\nTensor Parallelism\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nNorm. Power\\nFreq. [GHz]: \\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\nFig. 9. Normalized per-GPU power consumption of a LLama2 instance under\\nmedium load and medium input/output request types with different GPU\\nfrequencies for different levels of tensor-parallelism.\\nTP2\\nTP4\\nTP8\\nTensor Parallelism\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nNorm. Energy\\nFreq. [GHz]: \\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\nFig. 10. Normalized total energy consumption of a LLama2 instance under\\nmedium load and medium input/output request types with different GPU\\nfrequencies for different levels of tensor-parallelism.\\nthat a holistic understanding of LLM inference workloads ne-\\ncessitates a comprehensive examination of their performance,\\nenergy efficiency, and environmental implications. Such an\\napproach not only enables better resource management but\\nalso facilitates the identification of actionable optimization\\nstrategies and fine-tuning mechanisms for LLM inference\\n', b'5\\n4\\n8\\n16\\n32\\n64\\nBatch Size\\n0\\n2\\n4\\n6\\n8\\nNorm. TTFT\\nFreq [GHz]: \\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\nFig. 11.\\nNormalized TTFT of an 8-way tensor-parallel LLama2 instance\\nunder medium load and medium inputs/outputs request types with different\\nGPU frequencies for different batch sizes.\\n4\\n8\\n16\\n32\\n64\\nBatch Size\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nNorm. TBT\\nFreq [GHz]: \\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\nFig. 12. Normalized TBT of an 8-way tensor-parallel LLama2 instance under\\nmedium load and medium inputs/outputs request types with different GPU\\nfrequencies for different batch sizes.\\n4\\n8\\n16\\n32\\n64\\nBatch Size\\n0\\n2\\n4\\n6\\n8\\nMax T-put [RPS]\\nFreq [GHz]: \\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\nFig. 13. Maximum throughput of an 8-way tensor-parallel LLama2 instance\\nunder medium inputs/outputs request types with different GPU frequencies\\nfor different batch sizes.\\n4\\n8\\n16\\n32\\n64\\nBatch Size\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nNorm. Power\\nFreq [GHz]: \\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\nFig. 14. Normalized power consumption of an 8-way tensor-parallel LLama2\\ninstance under medium load and medium inputs/outputs request types with\\ndifferent GPU frequencies for different batch sizes.\\n4\\n8\\n16\\n32\\n64\\nBatch Size\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nNorm. Energy\\nFreq [GHz]: \\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\nFig. 15. Normalized energy consumption of an 8-way tensor-parallel LLama2\\ninstance under medium load and medium inputs/outputs request types with\\ndifferent GPU frequencies for different batch sizes.\\nserving. By bridging the gap between performance metrics,\\nenergy efficiency considerations, and actionable optimization\\nstrategies, this paper provides insights to researchers and\\npractitioners to navigate the complexities of LLM inference\\nworkloads more effectively, ultimately contributing to more\\nsustainable and efficient deployment of these models in real-\\nworld applications.\\nEnergy efficiency with hardware accelerators Other\\nworks have proposed the use of different models [2], [13] and\\ndifferent hardware [5], [20], [21] for energy efficient execution\\nof transformer-based architectures. We focus on exposing and\\nunderstanding energy efficiency of ubiquitous LLM inference\\ninfrastructures and immediately actionable knobs that require\\nno changes in server\\xe2\\x80\\x99s hardware nor model\\xe2\\x80\\x99s architecture.\\nEfficient LLM inference serving Many recent works\\npropose to optimize cluster and node-level scheduling [1],\\n[30], [39], memory and key-value cache management [3],\\n[9], [23], and model parallelism [24] to improve inference\\nefficiency. While these works focus on latency and throughput\\nimprovement, energy-efficient LLM serving exhibits distinct\\ntrade-offs and thus requires comprehensive understanding and\\nsolutions.\\nVI. CONCLUSION AND FUTURE WORK\\nIn this work, we present a characterization of the impact\\nof energy-efficiency knobs on various levers available to a\\nmodern LLM serving platform. We offer valuable insights\\ninto the difference between performance-optimized vs energy-\\noptimized designs on the widely adopted model and hardware.\\nWe further show that there are platform decisions that can be\\nmade towards better energy-efficiency at no impact to the cost\\nand performance. With this, we pave the way for orchestration\\nand cluster and node-level scheduling work with more holistic\\noptimization functions in future.\\nREFERENCES\\n[1] A. Agrawal, A. Panwar, J. Mohan, N. Kwatra, B. S. Gulavani, and\\nR. Ramjee, \\xe2\\x80\\x9cSARATHI: Efficient LLM Inference by Piggybacking\\nDecodes with Chunked Prefills,\\xe2\\x80\\x9d 2023.\\n[2] V. Akhlaghi, A. Yazdanbakhsh, K. Samadi, R. K. Gupta, and H. Es-\\nmaeilzadeh, \\xe2\\x80\\x9cSnaPEA: Predictive Early Activation for Reducing Com-\\nputation in Deep Convolutional Neural Networks,\\xe2\\x80\\x9d in Proceedings of\\nthe ACM/IEEE 45th Annual International Symposium on Computer\\nArchitecture (ISCA), 2018.\\n[3] K. Alizadeh, I. Mirzadeh, D. Belenko, K. Khatamifard, M. Cho, C. C. D.\\nMundo, M. Rastegari, and M. Farajtabar, \\xe2\\x80\\x9cLLM in a flash: Efficient\\nLarge Language Model Inference with Limited Memory,\\xe2\\x80\\x9d 2024.\\n[4] A. Andrae and T. Edler, \\xe2\\x80\\x9cOn Global Electricity Usage of Communication\\nTechnology: Trends to 2030,\\xe2\\x80\\x9d Challenges, vol. 6, 2015.\\n[5] G. Armeniakos, G. Zervakis, D. Soudris, and J. Henkel, \\xe2\\x80\\x9cHardware\\nApproximate Techniques for Deep Neural Network Accelerators: A\\nSurvey,\\xe2\\x80\\x9d ACM Computing Surveys, vol. 55, no. 4, p. 1\\xe2\\x80\\x9336, Nov. 2022.\\n[Online]. Available: http://dx.doi.org/10.1145/3527156\\n[6] K. Blunt and J. Hiller, \\xe2\\x80\\x9cBig Tech\\xe2\\x80\\x99s Latest Obsession Is Finding\\nEnough Energy,\\xe2\\x80\\x9d https://www.wsj.com/business/energy-oil/big-techs-\\nlatest-obsession-is-finding-enough-energy-f00055b2.\\n[7] S. Chen, C. Delimitrou, and J. F. Mart\\xc2\\xb4\\xc4\\xb1nez, \\xe2\\x80\\x9cPARTIES: QoS-Aware\\nResource Partitioning for Multiple Interactive Services,\\xe2\\x80\\x9d in ASPLOS,\\n2019.\\n[8] S. Chen, A. Jin, C. Delimitrou, and J. F. Mart\\xc2\\xb4\\xc4\\xb1nez, \\xe2\\x80\\x9cReTail: Opting for\\nLearning Simplicity to Enable QoS-Aware Power Management in the\\nCloud,\\xe2\\x80\\x9d in HPCA, 2022.\\n[9] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R\\xc2\\xb4e, \\xe2\\x80\\x9cFlashAttention: Fast\\nand Memory-Efficient Exact Attention with IO-Awareness,\\xe2\\x80\\x9d 2022.\\n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \\xe2\\x80\\x9cBERT: Pre-training\\nof Deep Bidirectional Transformers for Language Understanding,\\xe2\\x80\\x9d 2019.\\n[11] A. Faiz, S. Kaneda, R. Wang, R. Osi, P. Sharma, F. Chen, and L. Jiangr,\\n\\xe2\\x80\\x9cLLMCarbon: Modeling the end-to-end Carbon Footprint of Large\\nLanguage Models,\\xe2\\x80\\x9d 2024.\\n[12] Forbes,\\n\\xe2\\x80\\x9cGenerative\\nAI\\nBreaks\\nThe\\nData\\nCenter,\\xe2\\x80\\x9d\\nhttps:\\n//www.forbes.com/sites/tiriasresearch/2023/05/12/generative-ai-\\nbreaks-the-data-center-data-center-infrastructure-and-operating-costs-\\nprojected-to-increase-to-over-76-billion-by-2028/?sh=5bca69067c15.\\n', b'6\\n[13] S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. D. Giorno,\\nS. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi,\\nA. Salim, S. Shah, H. S. Behl, X. Wang, S. Bubeck, R. Eldan, A. T.\\nKalai, Y. T. Lee, and Y. Li, \\xe2\\x80\\x9cTextbooks are all you need,\\xe2\\x80\\x9d 2023.\\n[14] U. Gupta, Y. Kim, S. Lee, J. Tse, H. S. Lee, G. Wei, D. Brooks,\\nand C. Wu, \\xe2\\x80\\x9cChasing Carbon: The Elusive Environmental Footprint of\\nComputing,\\xe2\\x80\\x9d in HPCA \\xe2\\x80\\x9921, 2021.\\n[15] U. Gupta, M. Elgamal, G. Hills, G.-Y. Wei, H.-H. S. Lee, D. Brooks,\\nand C.-J. Wu, \\xe2\\x80\\x9cACT: designing sustainable computer systems with an\\narchitectural carbon modeling tool,\\xe2\\x80\\x9d in ISCA, 2022.\\n[16] M. E. Haque, Y. He, S. Elnikety, T. D. Nguyen, R. Bianchini, and\\nK. S. McKinley, \\xe2\\x80\\x9cExploiting Heterogeneity for Tail Latency and Energy\\nEfficiency,\\xe2\\x80\\x9d in MICRO, 2017.\\n[17] C.-H. Hsu, Y. Zhang, M. A. Laurenzano, D. Meisner, T. Wenisch,\\nJ. Mars, L. Tang, and R. G. Dreslinski, \\xe2\\x80\\x9cAdrenaline: Pinpointing and\\nreining in tail queries with quick voltage boosting,\\xe2\\x80\\x9d in HPCA, 2015.\\n[18] Q. Hu, Z. Ye, Z. Wang, G. Wang, M. Zhang, Q. Chen, P. Sun, D. Lin,\\nX. Wang, Y. Luo, Y. Wen, and T. Zhang, \\xe2\\x80\\x9cCharacterization of Large\\nLanguage Model Development in the Datacenter,\\xe2\\x80\\x9d 2024.\\n[19] H. Kasture, D. B. Bartolini, N. Beckmann, and D. Sanchez, \\xe2\\x80\\x9cRubik: Fast\\nanalytical power management for latency-critical systems,\\xe2\\x80\\x9d in MICRO,\\n2015.\\n[20] H. Khan, A. Khan, Z. Khan, L. B. Huang, K. Wang, and L. He, \\xe2\\x80\\x9cNPE:\\nAn FPGA-based Overlay Processor for Natural Language Processing,\\xe2\\x80\\x9d\\nin Proceedings of the 2021 ACM/SIGDA International Symposium on\\nField-Programmable Gate Arrays, ser. FPGA \\xe2\\x80\\x9921, 2021.\\n[21] S. Kim, C. Hooper, T. Wattanawong, M. Kang, R. Yan, H. Genc,\\nG. Dinh, Q. Huang, K. Keutzer, M. W. Mahoney, Y. S. Shao, and\\nA. Gholami, \\xe2\\x80\\x9cFull Stack Optimization of Transformer Inference: a\\nSurvey,\\xe2\\x80\\x9d 2023.\\n[22] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez,\\nH. Zhang, and I. Stoica, \\xe2\\x80\\x9cEfficient Memory Management for Large\\nLanguage Model Serving with PagedAttention,\\xe2\\x80\\x9d in SOSP, 2023.\\n[23] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E.\\nGonzalez, H. Zhang, and I. Stoica, \\xe2\\x80\\x9cEfficient Memory Management for\\nLarge Language Model Serving with PagedAttention,\\xe2\\x80\\x9d in SOSP, 2023.\\n[24] Z. Li, L. Zheng, Y. Zhong, V. Liu, Y. Sheng, X. Jin, Y. Huang,\\nZ. Chen, H. Zhang, J. E. Gonzalez, and I. Stoica, \\xe2\\x80\\x9cAlpaServe: Statistical\\nMultiplexing with Model Parallelism for Deep Learning Serving,\\xe2\\x80\\x9d in\\nOSDI, 2023.\\n[25] D. Lo, L. Cheng, R. Govindaraju, L. A. Barroso, and C. Kozyrakis, \\xe2\\x80\\x9cTo-\\nwards energy proportionality for large-scale latency-critical workloads,\\xe2\\x80\\x9d\\nin ISCA, 2014.\\n[26] E. R. Masanet, A. Shehabi, N. Lei, S. J. Smith, and J. G. Koomey,\\n\\xe2\\x80\\x9cRecalibrating global data center energy-use estimates,\\xe2\\x80\\x9d Science, 2020.\\n[27] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, Z. Zhang,\\nR. Y. Y. Wong, A. Zhu, L. Yang, X. Shi, C. Shi, Z. Chen, D. Arfeen,\\nR. Abhyankar, and Z. Jia, \\xe2\\x80\\x9cSpecInfer: Accelerating Generative Large\\nLanguage Model Serving with Tree-based Speculative Inference and\\nVerification,\\xe2\\x80\\x9d 2024.\\n[28] R. Nishtala, V. Petrucci, P. Carpenter, and M. Sjalander, \\xe2\\x80\\x9cTwig: Multi-\\nAgent Task Management for Colocated Latency-Critical Cloud Ser-\\nvices,\\xe2\\x80\\x9d in HPCA, 2020.\\n[29] NVIDIA.\\nNVIDIA\\nDGX\\nH100.\\n[Online].\\nAvailable:\\nhttps:\\n//www.nvidia.com/en-us/data-center/dgx-h100/\\n[30] P. Patel, E. Choukse, C. Zhang, A. Shah, \\xc2\\xb4I\\xcb\\x9cnigo Goiri, S. Maleki, and\\nR. Bianchini, \\xe2\\x80\\x9cSplitwise: Efficient generative LLM inference using phase\\nsplitting,\\xe2\\x80\\x9d 2023.\\n[31] P. Patel, E. Choukse, C. Zhang, \\xc2\\xb4I\\xcb\\x9cnigo Goiri, B. Warrier, N. Mahalingam,\\nand R. Bianchini, \\xe2\\x80\\x9cPOLCA: Power Oversubscription in LLM Cloud\\nProviders,\\xe2\\x80\\x9d 2023.\\n[32] P. Patel, Z. Gong, S. Rizvi, E. Choukse, P. Misra, T. Anderson, and\\nA. Sriraman, \\xe2\\x80\\x9cTowards improved power management in cloud gpus,\\xe2\\x80\\x9d\\nIEEE Computer Architecture Letters, 2023.\\n[33] S. Samsi, D. Zhao, J. McDonald, B. Li, A. Michaleas, M. Jones,\\nW. Bergeron, J. Kepner, D. Tiwari, and V. Gadepally, \\xe2\\x80\\x9cFrom words\\nto watts: Benchmarking the energy costs of large language model\\ninference,\\xe2\\x80\\x9d 2023.\\n[34] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili\\xc2\\xb4c, D. Hesslow,\\nR. Castagn\\xc2\\xb4e, A. S. Luccioni, F. Yvon, M. Gall\\xc2\\xb4e et al., \\xe2\\x80\\x9cBLOOM:\\nA 176B-parameter open-access multilingual language model,\\xe2\\x80\\x9d arXiv\\npreprint arXiv:2211.05100, 2022.\\n[35] P.\\nSchmid,\\n\\xe2\\x80\\x9cLlama\\n2\\nis\\nhere\\n-\\nGet\\nit\\non\\nHugging\\nFace,\\xe2\\x80\\x9d\\nhttps://huggingface.co/blog/llama2, 2023.\\n[36] P. Schmid, O. Sanseviero, P. Cuence, and L. Tunstall, \\xe2\\x80\\x9cFine-tune\\nFLAN-T5 XL/XXL using DeepSpeed and Hugging Face Transformers,\\xe2\\x80\\x9d\\nhttps://www.philschmid.de/ fine-tune-flan-t5-deepspeed, 2023.\\n[37] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \\xe2\\x80\\x9cLlama\\n2: Open foundation and fine-tuned chat models,\\xe2\\x80\\x9d arXiv preprint\\narXiv:2307.09288, 2023.\\n[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nL. u. Kaiser, and I. Polosukhin, \\xe2\\x80\\x9cAttention is all you need,\\xe2\\x80\\x9d in Advances\\nin Neural Information Processing Systems, I. Guyon, U. V. Luxburg,\\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,\\nEds., vol. 30.\\nCurran Associates, Inc., 2017.\\n[39] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, \\xe2\\x80\\x9cOrca: A\\nDistributed Serving System for Transformer-Based Generative Models,\\xe2\\x80\\x9d\\nin OSDI, 2022.\\n[40] H. Zhang, A. Ning, R. Prabhakar, and D. Wentzlaff, \\xe2\\x80\\x9cA Hardware\\nEvaluation Framework for Large Language Model Inference,\\xe2\\x80\\x9d 2023.\\n[41] L. Zhou, L. N. Bhuyan, and K. K. Ramakrishnan, \\xe2\\x80\\x9cGemini: Learning to\\nManage CPU Power for Latency-Critical Search Engines,\\xe2\\x80\\x9d in MICRO,\\n2020.\\n']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Text from paper4.pdf:\n",
            "[b'Applied Energy 367 (2024) 123431\\nAvailable online 16 May 2024\\n0306-2619/\\xc2\\xa9 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\\nEPlus-LLM: A large language model-based computing platform for \\nautomated building energy modeling \\nGang Jiang a, Zhihao Ma a, Liang Zhang b, Jianli Chen a,* \\na The University of Utah, United States \\nb The University of Arizona, United States   \\nH I G H L I G H T S  \\n\\xe2\\x80\\xa2 This study represents the pioneering effort in customizing LLM for auto-modeling. \\n\\xe2\\x80\\xa2 A large language model is used to digest natural language descriptions for modeling. \\n\\xe2\\x80\\xa2 Integration of four prompts enhances the flexibility and versatility of our platform. \\n\\xe2\\x80\\xa2 Our platform provides a human-AI interface and reduces building modeling efforts.  \\nA R T I C L E  I N F O   \\nKeywords: \\nLarge language models \\nArtificial intelligence \\nMachine learning \\nBuilding energy modeling \\nAutomated simulation \\nA B S T R A C T   \\nEstablishing building energy models (BEMs) for building design and analysis poses significant challenges due to \\ndemanding modeling efforts, expertise to use simulation software, and building science knowledge in practice. \\nThese make building modeling labor-intensive, hindering its widespread adoptions in building development. \\nTherefore, to overcome these challenges in building modeling with enhanced automation in modeling practice, \\nthis paper proposes Eplus-LLM (EnergyPlus-Large Language Model) as the auto-building modeling platform, \\nbuilding on a fine-tuned large language model (LLM) to directly translate natural language description of \\nbuildings to established building models of various geometries, occupancy scenarios, and equipment loads. \\nThrough fine-tuning, the LLM (i.e., T5) is customized to digest natural language and simulation demands from \\nusers and convert human descriptions into EnergyPlus modeling files. Then, the Eplus-LLM platform realizes the \\nautomated building modeling through invoking the API of simulation software (i.e., the EnergyPlus engine) to \\nsimulate the auto-generated model files and output simulation results of interest. The validation process, \\ninvolving four different types of prompts, demonstrates that Eplus-LLM reduces over 95% modeling efforts and \\nachieves 100% accuracy in establishing BEMs while being robust to interference in usage, including but not \\nlimited to different tones, misspells, omissions, and redundancies. Overall, this research serves as the pioneering \\neffort to customize LLM for auto-modeling purpose (directly build-up building models from natural language), \\naiming to provide a user-friendly human-AI interface that significantly reduces building modeling efforts. This \\nwork also further facilitates large-scale building model efforts, e.g., urban building energy modeling (UBEM), in \\nmodeling practice.   \\n1. Introduction \\n1.1. Background \\nBuildings account for about 36% of total global energy [1]. Building \\nenergy models (BEMs) have increasingly stood as a crucial tool to \\nsimulate building energy use and indoor environment, serving various \\npurposes such as building retrofits, sustainability, and decarbonization \\n[2,3]. However, creating BEMs is time consuming, requiring skills of \\nsoftware usage, especially for large and complex buildings. To build an \\naccurate building simulation model, building modelers have to repeat\\xc2\\xad\\nedly modify design parameters and debug the models, in order to ensure \\nvalidity of modeling outcomes (e.g., energy use and indoor environment \\nstatus). A user-friendly and automated building simulation platform is \\n* Corresponding author. \\nE-mail address: jianli.chen@utah.edu (J. Chen).  \\nContents lists available at ScienceDirect \\nApplied Energy \\njournal homepage: www.elsevier.com/locate/apenergy \\nhttps://doi.org/10.1016/j.apenergy.2024.123431 \\nReceived 16 January 2024; Received in revised form 3 May 2024; Accepted 8 May 2024   \\n', b'Applied Energy 367 (2024) 123431\\n2\\nparticularly beneficial for facilitating simulation-intensive applications, \\ne.g., calibration and urban building energy modeling (UBEM). \\nIn recent years, pre-trained large language models (PTMs) have \\nfound remarkable success in natural language processing (NLP). Trained \\non extensive datasets, PTMs exhibit adaptability to a range of down\\xc2\\xad\\nstream tasks and applications through fine-tuning or context learning. \\nLarge language models (LLMs), with billions of parameters and unique \\nmodel architectures involving the attention mechanism, have developed \\na strong capability for natural language tasks. This enables them to \\ncapture subtle nuances and contextual information within language, \\nperforming well across various applications. Furthermore, these models \\nexhibit multimodal capabilities, excelling in processing diverse types of \\ninformation, including text, images, and sound. The versatility of LLMs \\nnot only positions them as powerful tools in NLP and computer vision \\n(CV), but also holds the potential to open up new possibilities and op\\xc2\\xad\\nportunities in the fields of building and simulation. The intersection of \\nLLMs with building simulation is anticipated to bring about revolu\\xc2\\xad\\ntionary advancements and innovative solutions and will offer fresh in\\xc2\\xad\\nsights and prospects for future technological development. \\n1.2. Existing practice and development of building modeling and LLM \\nEngineers and researchers begin to model buildings (or calculate \\nbuilding loads) using computers or computing methods ever since 19th \\ncentury. These early modeling were primarily based on simplified \\nmethods (e.g., Degree-day method and Bin method [4]) to estimate the \\nenergy demand of heating, ventilation, and air conditioning (HVAC) \\nsystems for building design and operation. With development of \\nmodeling engines, existing modeling software, such as ANSYS [5], \\nDymola [6], TRNSYS [7], DeST [8], EnergyPlus [9], ISO 13790 [10], \\nand VDI 6007 [11], are mostly based on heat balance principles and \\nordinary differential equation (ODE) in the modeling process. These \\napplications use building details, material properties, and climate data \\nas inputs to produce reliable building simulation results. Among them, \\nEnergyPlus, developed by the US Department of Energy (DOE), is one of \\nthe most widely used software in the field of building simulation. \\nEnergyPlus adopts the physics-based modeling method, simulating the \\ncomplex interactions between various components in the building (such \\nas walls, windows, HVAC systems, etc.) of different physical character\\xc2\\xad\\nistics. The high-fidelity physics principle enables EnergyPlus to reliably \\ncapture dynamics of building systems, providing reliable simulation \\nresults to support various building and energy applications. \\nDespite the development, modeling buildings are non-trivial. Accu\\xc2\\xad\\nrately characterizing and creating models for complex building systems \\ndemands substantial efforts, although graphical user interfaces (GUIs), \\nsuch as DesignBuilder [12] and OpenStudio [13], were developed to \\nsimplify the modeling process. Users still need to grapple with a plethora \\nof options and parameters in modeling through GUIs. To automate the \\ncreation and modification of building energy models, other researchers \\nhave developed Python libraries like Eppy [14]. However, it is crucial \\nto recognize that using these packages for automated building modeling \\nalso requires strong coding ability to effectively utilize these advanced \\ntools. \\nWith significant advancement in computation these years, the \\nemergence of LLMs presents as a promising solution to overcome \\ncomplexity and accessibility of building modeling. LLMs have achieved \\nremarkable success in NLP, leading to a paradigm shift from supervised \\nlearning to pre-training followed by fine-tuning [15]. As one of the \\npioneering works, Peters et al. [16] proposed embeddings of language \\nmodels to learn contextual word representations using bi-directional \\nLSTMs, and then applied the pre-trained embeddings to downstream \\ntasks. This approach demonstrated dramatic improvements in a wide \\nrange of NLP tasks. Generative Pre-Training (GPT) [17], which lever\\xc2\\xad\\nages transformers [18] for language modeling, also illustrated the great \\npotential of PTMs to support different downstream tasks. After the \\nsuccess of GPT, there has been significant interest in scaling up and \\nexploring pre-trained language models, leading to the development of \\nnumerous LLMs. Examples include BERT [19], text-to-text transfer \\ntransformers (T5) [20], PaLM [21], and LLaMA [22]. \\nBuilding on the success of PTMs described above, these models have \\nbeen increasingly used to support various downstream applications, \\nincluding but not limited to computer vision [23], speech processing \\n[24], etc. They have also been applied in generative tasks within do\\xc2\\xad\\nmains like chemistry [25], geography [26], and code generation [27]. \\nLLMs possess the unique ability to understand and generate human-like \\ntext, making them well-suited for interpreting natural language de\\xc2\\xad\\nscriptions. In the context of building modeling, this capability holds \\nsignificant potential to facilitate user\\xe2\\x80\\x99s modeling and reduce simulation \\nefforts. By leveraging LLMs in automated building simulation and \\ndesign, users can describe their simulation visions and requirements in a \\nmore intuitive manner: directly using natural language descriptions \\ninstead of complex design with simulation software. However, adapting \\nLLMs for building modeling with auto-model generation is challenging \\ndue to the following issues:  \\n(1) Precise Model Description \\nBuilding modeling is a complex process involving numerous inputs, \\nsimulation configurations, and closely coupled sets of equations in \\ncalculation. Even a slight error of punctuation (e.g., missing a decimal \\nsomewhere) or a small discrepancy of geometry (e.g., the edge of walls \\ndefined by coordinates has 0.01 m deviations from its connected walls) \\nwill lead to the simulation failure (the modeling engine will report se\\xc2\\xad\\nvere errors and terminate the simulation). Achieving this high level of \\naccuracy poses a significant challenge for LLMs, because general LLM \\noutputs typically exhibit varying degrees of randomness. Consequently, \\nLLMs for our building modeling task) must demonstrate remarkable \\nprecision during generation. Given that LLMs comprise billions of pa\\xc2\\xad\\nrameters, training a capable foundation model to effectively and accu\\xc2\\xad\\nrately generate building model descriptions demands significant \\ncomputational resources and time. The current diversity in LLM archi\\xc2\\xad\\ntectures further complicates the selection of appropriate models and \\nsizes to accurately cater to the requirements of building simulation. \\nBlindly opting for a larger model does not necessarily equate to better \\nperformance [28].  \\n(2) Customization of LLM for Building Modeling Tasks \\nAnother critical challenging lies in customizing LLMs for building \\nmodeling. This challenge arises from (1) the need for domain-specific \\nknowledge to customize LLMs for building modeling. LLMs have not \\nbeen pre-trained on building modeling tasks, making it difficult to be \\ndirectly used for automated building modeling, (2) The high level of \\nrequired consistency in building modeling tasks. Unlike generic NLP \\ntasks, auto-creation of building modeling files requires precision and \\nconsistency for the generated models to be valid. The necessity of un\\xc2\\xad\\nderstanding diverse language description formats further adds extra \\ncomplexity to maintain consistency in building modeling. Users may use \\nsynonymous terms, varied sentence structures, or even misspellings to \\nconvey identical building attributes, requiring LLMs to discern and \\nreconcile these linguistic subtleties to capture user intent accurately. \\nTo overcome these, the \\xe2\\x80\\x9cpre-training then fine-tuning\\xe2\\x80\\x9d paradigm has \\nemerged as an effective and flexible method for creating sub-models to \\ncustomize LLMs for specific applications. This paradigm is favored \\nbecause: (1) Pre-training models involve billions of parameters, \\nadvanced computational architectures, and large corpus datasets, \\nendowing LLMs with language flexibility, accuracy performance, and \\nlong context window for downstream tasks. (2) Fine-tuning process re\\xc2\\xad\\nlies on self-learning and domain-specific data (e.g., building simulation \\ndataset), enabling the model to grasp the relevance and focal points of \\ncontent within user inputs (i.e., building modeling requirements). \\nTherefore, fine-tuning is an effective way for our purposes in handling \\nG. Jiang et al.                                                                                                                                                                                                                                    \\n', b'Applied Energy 367 (2024) 123431\\n3\\ncomplex and highly customized building modeling demands. \\nInstructGPT and ChatGPT [28] are fine-tuned with effective \\nprompts, aligning with user intentions on various tasks. Notably, the \\n1.3B InstructGPT outperforms the 175B GPT-3 despite having 100 times \\nfewer parameters. ChatGPT excels in various downstream tasks post \\nfine-tuning. The fine-tuned T5 checkpoint, Flan-t5 [29], achieves su\\xc2\\xad\\nperior performance in evaluation benchmarks, even when compared to \\nlarger models. Pavlyshenko [30] fine-tuned Llama 2 for multitask \\nanalysis of financial news. CodeT5+ [31] is an encoder-decoder family \\ntailored for diverse downstream code tasks. Surprisingly small and cost- \\neffective, Alpaca [32], fine-tuned from Meta\\xe2\\x80\\x99s Llama 7B, achieves su\\xc2\\xad\\nperior performance. In conclusion, the superior performance of these \\nfine-tuned models underscores the effectiveness of the fine-tuning \\napproach in customizing language models (LLMs) for specific applica\\xc2\\xad\\ntions. A fine-tuned model can achieve high accuracy in various down\\xc2\\xad\\nstream tasks with minimal additional training data. By providing \\ndifferent prompts during fine-tuning, LLMs can gather more information \\ntailored to specific tasks, thereby guiding their generation with \\nprecision. \\n1.3. New opportunities for building model design and simulation \\nFor building simulation, despite the existence of numerous physical- \\nbased and data-driven modeling methods, practitioners are still plagued \\nby challenges such as the accessibility of software or technologies, data \\nreliability, and technological complexity. Creating a high-fidelity \\nbuilding model is typically time-consuming, even for experienced \\nmodelers, not to mention non-tech-savvy users. The emergence of LLMs \\npresents new opportunities to completely automate building design and \\nsimulation from natural language description of buildings by modelers. \\nLLMs are designed to comprehend natural language and generate sen\\xc2\\xad\\ntences through pre-training. Fine-tuning is then performed based on the \\npre-trained LLM, utilizing smaller field datasets and fewer computing \\nresources to further enhance its performance and availability in down\\xc2\\xad\\nstream tasks. Integrating LLMs into building design and modeling can \\nsignificantly reduce the efforts, including knowledge, technologies, \\ndata, hardware, manpower, and time requirements, needed to construct \\nphysics-based building models. This will further foster the widespread \\nadoption of advanced building modeling in practical applications. \\nHowever, despite the great potential shown by LLMs in natural language \\napplications, their application with the automation of building modeling \\nis still limited due to the difficulty of reaching the precision re\\xc2\\xad\\nquirements and effectively handling customization of building modeling \\ntasks as mentioned above. Hence, there still lacks a customized LLM- \\nbased platform for auto-building modeling. \\nIn this study, we proposed Eplus-LLM as the computing platform \\ncustomized for automated building modeling tasks, capable of directly \\nand automatically translating natural language descriptions of modelers \\ninto high-fidelity building models with precision (overcoming two \\nchallenges mentioned in Section 1.2). This platform integrates the LLM \\narchitecture, i.e., the Text-to-Text Transfer Transformer (T5), with the \\nphysics-based modeling engine, EnergyPlus, aiming to create a user- \\nfriendly and automated process to support modelers (e.g., from \\nconsulting firms, architecture firms, academia, building system design \\nengineers, etc.) in building modeling. Using Eplus-LLM platform is able \\nto (1) make modeling easier with AI-empowered communication be\\xc2\\xad\\ntween modeling software and modelers. The developed platform aims to \\nreduce burdens and required efforts of modelers in building modeling. \\nNatural language is the natural way of humans in communications, not \\nonly with other humans, but also with machines. LLM has demonstrated \\nsignificant potential to automate tasks under natural instruction of \\nhumans in diverse fields. However, this automation and intelligence are \\nnot fully leveraged in the building modeling field yet. The developed \\nplatform aims to realize the future goal of fully automated building \\nmodeling without significant efforts as in the current practice, and (2) \\nhandle more flexible inputs. Our platform supports structured and \\nunstructured ways of modeling inputs. For unstructured inputs, our \\nplatform can handle natural language with different tones, grammar \\nerrors, and omitted words. Users can also use structured data inputs, e. \\ng., specifying dimensions with height: xxx; width: xxx; length: xxx \\xe2\\x80\\xa6, \\nwithout worrying about deviations from expected format. \\nFor validation of the effectiveness of our proposed framework in \\nauto-building modeling, we conducted a total of 152 validation in\\xc2\\xad\\nstances to verify the effectiveness and stability of the developed \\ncomputing platform for auto and accurate building modeling from \\nnatural language descriptions of models, achieving 100% accuracy. We \\nalso assessed the robustness and anti-interference ability of the platform \\nin generation by introducing various types of noise and unseen prompts. \\nThe paper is organized as follows: Section 2 provides an overview, \\nmodel architecture, and training details of our proposed Eplus-LLM \\nplatform. In Section 3, we demonstrate the effectiveness, robustness, \\nand versatility of our platform through validation and evaluation. Sec\\xc2\\xad\\ntion 4 discusses insights gained during the exploration of LLM-based \\nauto-building modeling, and Section 5 concludes our study. \\n2. Methodology \\n2.1. Overview of proposed Eplus-LLM platform \\nFig. 1 illustrates the framework of the proposed Eplus-LLM, a fine- \\ntuned LLM capable of rapidly and automatedly building up building \\nmodels directly from natural language. The objective is to overcome the \\ncomplexities, necessity of specific knowledge, familiarity with software, \\nand challenges in human-modeling interaction of traditional building \\nsimulation and modeling. The Eplus-LLM platform integrates physics- \\nbased simulation software (i.e., EnergyPlus) with an LLM architecture \\n(i.e., T5), providing a human-AI interface that comprehends description \\nof building models by natural language and perform building modeling \\nfrom dialogue. \\nAs the core of the proposed framework, Eplus-LLM primarily consists \\nof three modules: Prompting, LLM Architecture, and Auto-simulation, as \\nshown in the Fig. 1. The Prompting module receives modeling com\\xc2\\xad\\nmands of users in natural language descriptions from the interface of \\nhuman-model interaction. Following tokenization, embedding, and po\\xc2\\xad\\nsition encoding, the natural language descriptions of specified simula\\xc2\\xad\\ntion demands are transformed into numerical and tokenized multi- \\ndimensional features (Fig. 2a). These features are then accepted and \\ncomprehended by the LLM, with the attention module serving as its core. \\nBy constructing Q (Query), K (Key), and V (Value) matrices (the Q \\nrepresents the specific information the model is focusing on; the K is the \\nfeatures of the input, determining the content related to the Q; the V is \\nused to provide the actual information content), the LLM is able to \\neffectively focus on the most significant information. In building \\nmodeling tasks, Q, K, and V matrices can be understood as multi- \\ndimensional relationships among various building input parameters \\nand interconnected relationships among building sub-systems. They \\nfacilitate the mapping of simulation instructions to model descriptions \\nand the generation of simulation files, i.e., EnergyPlus IDF files. After\\xc2\\xad\\nward, the API of simulation software is automatically activated through \\nscripting, then outputting modeling results, e.g., indoor temperature and \\nbuilding energy consumptions (Fig. 2 b). In Section 2.2, the foundation \\nmodel is elaborated in detail, explaining how natural language de\\xc2\\xad\\nscriptions are transformed into tokens recognizable by our Eplus-LLM. In \\nSection 2.3, the fine-tuning and customizing process for auto-building \\nmodeling tasks are discussed in detail, highlighting our data prepara\\xc2\\xad\\ntion and processing, the designed prompts, without fine-tuning baseline, \\nand provides insights into the fine-tuning process and details. \\n2.2. Foundation LLM \\nIn this work, we utilized the Flan-T5 model [29] as a foundation \\nmodel, fine-tuning it with training data derived from building modeling \\nG. Jiang et al.                                                                                                                                                                                                                                    \\n', b'Applied Energy 367 (2024) 123431\\n4\\nsoftware and domain-specific knowledge (Section 2.3). Flan-T5 is an \\ninstruction model fine-tuned on top of T5 architecture (Encoders-De\\xc2\\xad\\ncoders), which has demonstrated advanced performance and superb \\ngeneralization capability compared to T5 [29]. T5, known as \\xe2\\x80\\x9cthe \\ntransfer learning with a unified text-to-text transformer model\\xe2\\x80\\x9d is \\ndistinct from previous pre-trained models like BERT or GPT. It is \\ndesigned with the idea that all tasks are transformed into text-to-text \\nproblems through an encoder-decoder transformer structure, enabling \\nit to excel across a variety of tasks. Pre-trained on a large-scale text \\ncorpus, T5 learns the natural language representation. These pre-trained \\nweights are then fine-tuned for specific downstream application sce\\xc2\\xad\\nnarios. T5 has demonstrated remarkable suitability for generation tasks; \\nnotable T5-based code generation models include codeT5 and codeT5+\\n[27,31]. This performance allows us to create an LLM capable of auto- \\nbuilding modeling. The Foundation T5 mainly have two parts, \\nprompting of natural language to tokens and encoders-decoders with \\nattention mechanism. \\n2.2.1. Prompting of natural language for building modeling tasks \\nPrompt engineering is to design instructions or inputs for a genera\\xc2\\xad\\ntive AI model to perform specific tasks. A prompt is typically in a format \\nof natural language sentences that describe detailed task requirements \\nfor AI to follow [33]. It serves as a bridge for communication between \\nusers and LLMs, determining the direction and content of model outputs. \\nPrompts can take various formats, depending on the user\\xe2\\x80\\x99s intentions \\nand task requirements. By inputting a prompt, users can guide the model \\nto generate task results that meet their expectations. \\nIn the building simulation field, prompts from modelers can be \\nsimple or complicated. A simple prompt can be a short modeling \\nrequirement for generating a simple building model, for example, \\nsimulate a building that is 100.00 m long, 50.00 m wide, and 8.00 m \\nhigh. If complicated, more detailed information and requirements are \\nspecified. For example, simulate a building that is 100.00 m long, 50.00 \\nm wide, and 8.00 m high. The window-to-wall ratio is 0.50. The occu\\xc2\\xad\\npancy rate is 10.00 m2/people, the lighting level is 8.00 W/m2, and the \\nequipment power consumption is 5.00 W/m2. In this case, the prompt \\ncontains clear task directions and modeling details. The choice of \\nprompt directly affects building modeling outcomes. Users need to \\nconsider the nature of the task, the desired output, and the purpose of \\ninteracting with the model to choose the prompt appropriately. \\nAppropriate prompts can fully utilize the linguistic capabilities of an \\nLLM, which not only improves the accuracy of model generation, but \\nalso effectively guides the LLM to generate content satisfying needs of \\nusers. The combination of a reasonable and effective prompt design with \\nan appropriate LLM is the key to achieve satisfactory and accurate re\\xc2\\xad\\nsults in LLM-based auto building modeling. \\n2.2.2. Attention mechanism in LLM-based building modeling \\nThe self-attention mechanism is at the core of LLMs. In contrast to \\ntraditional recurrent neural network (RNN) or long-short-term memory \\nnetwork (LSTM), which rely on sequential processing, the self-attention \\nenables models to assign different attention weights at different loca\\xc2\\xad\\ntions in the input sequence. This mechanism allows the model to focus \\non different information while processing the input sequence. The self- \\nattention mechanism permits each word to adjust its importance to \\nthe context during the encoding process. It involves the dot-product the \\nencoded input with matrix queries (Q) and keys (K) of dimension dK, and \\nmatrix values (V) of dimension dV. Applying a softmax function to obtain \\nthe weights on the values, the output of attention matrix as [18]: \\nAtteention(Q, K, V) = softmax\\n(\\nQKT\\n\\xcc\\x85\\xcc\\x85\\xcc\\x85\\xcc\\x85\\xcc\\x85\\ndK\\n\\xe2\\x88\\x9a\\n)\\nV \\nIn building simulation scenarios, the Q, K, V matrices can be un\\xc2\\xad\\nderstood as representing the impact of different parameters on simula\\xc2\\xad\\ntion results in our downstream tasks (i.e., building modeling). For \\nexample, the impact of heating, cooling, and electricity on buildings. To \\nfurther enhance the model\\xe2\\x80\\x99s capability, attention is divided into multi\\xc2\\xad\\nple heads. These multi-heads are designed to capture various channels of \\ninformation in the input, especially when dealing with multiple types of \\ninputs and prompt formats. Each head presents a distinct type of rep\\xc2\\xad\\nresentation. The final outputs are obtained through concatenation or \\nFig. 1. The framework of our Eplus-LLM platform.  \\nG. Jiang et al.                                                                                                                                                                                                                                    \\n', b'Applied Energy 367 (2024) 123431\\n5\\nweighted averaging all heads. The output of the self-attention layer is \\npassed into a feed-forward neural network, which includes a fully con\\xc2\\xad\\nnected layer [34] and an activation function [35]. This helps to \\nintroduce the capability of thoroughly capturing nonlinear dynamics in \\nmodeling, for example, interrelationships among building system com\\xc2\\xad\\nponents in building modeling. Additionally, there is a residual connec\\xc2\\xad\\ntion [36] around each of the two sub-layers, followed by layer \\nnormalization [37], to reduce overfitting and gradient problems. These \\nare beneficial for model training. \\n2.3. Fine-tuning for customizing auto-building modeling tasks \\n2.3.1. Generating datasets for model fine-tuning \\nThe fine-tuning dataset, utilizing to customize the LLM for auto- \\nbuilding modeling, was generated through two main steps:  \\n(1) Constructing parameters-IDF scenario pairs \\nThe amount of data required for fine-tuning is substantial. To attain a \\nwide range of building modeling scenarios, we utilized a Latin \\nhypercube design [38] to sample diverse combinations of parameters, \\ne.g., different building geometries, window-to-wall ratios, and internal \\nload variations, and generated IDF files corresponding to different \\nparameter combinations, i.e., parameter-to-IDF file correspondence. \\nThis step yielded a comprehensive dataset comprising diverse building \\nmodel descriptions of various parameter settings, as essential to support \\nfine-tuning processes.  \\n(2) Constructing descriptions-IDF sentence pairs \\nAfter constructing the parameters-IDF scenario pairs, the next step is \\nto connect these IDF files (as targets) with corresponding natural lan\\xc2\\xad\\nguage descriptions (as prompts) for model fine-tuning. The prompt de\\xc2\\xad\\nscribes what the model should focus on or generate under user\\xe2\\x80\\x99s \\nsimulation requirements. For example, during simulation, the prompt \\nincludes a description of the building to be simulated, containing the \\nspecifications of geometry, window details, and internal loads. The \\ntarget part corresponds to the building model in IDF format for Ener\\xc2\\xad\\ngyPlus. With a Python script, we translated the building model param\\xc2\\xad\\neters to a description prompt of buildings. Four different prompt formats \\nFig. 2. An overview of Eplus-LLM platform to support auto-building simulation.  \\nG. Jiang et al.                                                                                                                                                                                                                                    \\n', b'Applied Energy 367 (2024) 123431\\n6\\nare designed to cover different descriptions of model requirements, \\nresulting in a total of 70,000 pairs of prompt-targets for fine-tuning. \\nTable 1 lists our four designed prompt formats with their simulation \\nrequirement parameters. Fig. 3 clarifies the window position within our \\nprompt formats. The term \\xe2\\x80\\x9cwindow height\\xe2\\x80\\x9d refers to the vertical \\ndimension of the window, while the \\xe2\\x80\\x9cwindow sill\\xe2\\x80\\x9d denotes the vertical \\ndistance between the window and the wall, influencing the window\\xe2\\x80\\x99s \\nvertical placement. The \\xe2\\x80\\x9cwindow jamb\\xe2\\x80\\x9d represents the horizontal dis\\xc2\\xad\\ntance between the window and the wall, affecting the window\\xe2\\x80\\x99s hori\\xc2\\xad\\nzontal placement. We maintain a constant value of 0.1 across all prompts \\nfor the window jamb with a window-wall ratio. \\nTo illustrate the correlation between natural language descriptions \\n(prompts) and IDF files, we provide a simple example (Fig. 4). The \\ngeometric configuration of building surfaces is delineated by co\\xc2\\xad\\nordinates, with interconnections among various surfaces. Due to the \\nextent of IDF details, presenting the entire content in the paper is un\\xc2\\xad\\nfeasible. Therefore, we have extracted parts of geometry, window \\nsetting, and internal load (e.g., wall, window, and electric equipment) \\nfrom the IDF and matched them with the corresponding parts in the \\nprompt, as shown in Fig. 4. \\n2.3.2. Fine-tuning and auto-simulation \\nAfter obtaining the processed dataset for fine-tuning, these dataset (i. \\ne., processed sentence pairs) were encoded using byte-pair encoding \\n[39], which has a shared source target vocabulary of ~32,000 tokens. \\nSentence pairs were batched together by the same sequence length and \\neach training batch contained 5 sets of sentence pairs for fine-tuning. \\nThe model weights are adjusted through back-propagation to mini\\xc2\\xad\\nmize the discrepancies between the generated IDF files and the actual \\nIDF files. Once the model achieves a satisfactory level of performance, it \\ncan be deployed with a building energy simulation engine, EnergyPlus, \\nto generate building models and produce results (Fig. 2. b). This fine- \\ntuned LLM can automatically generate building modeling files based \\non various requirements and input parameters. We fine-tuned our model \\non one machine with one NVIDIA A100 80G GPU. Each training step \\ntook ~1.5 s. We trained the model for a total of 32,000 steps with ~16 h. \\n3. Model validation and analysis \\nPrior to fine-tuning, we evaluated the performance of the foundation \\nmodel (original Flan T5) using direct generation and one-shot learning \\n(without fine-tuning) as a baseline for our building modeling task. \\nSubsequently, following fine-tuning of our model, we conducted a total \\nof 152 validation instances to verify the effectiveness and stability of the \\ndeveloped computing platform for auto and accurate building modeling \\nfrom natural language descriptions. All auto-generated models run \\nsuccessfully, achieving 100% accuracy with the ground truth. In Section \\n3.1, we examined the performance of the foundation model using direct \\ngeneration and one-shot learning. In Section 3.2, we tested the model\\xe2\\x80\\x99s \\ncapability to generate corresponding outputs by inputting different \\nFig. 2. (continued). \\nG. Jiang et al.                                                                                                                                                                                                                                    \\n', b'Applied Energy 367 (2024) 123431\\n7\\ntypes of prompts randomly and manually, ensuring its seamless inte\\xc2\\xad\\ngration into the simulation engine and successful execution. In Section \\n3.3, we compared the time efficiency of the LLM-based Eplus-LLM \\nplatform with manual approaches. In Section 3.4, we assessed the \\nrobustness and anti-interference ability of the model in generation by \\nintroducing various types of noise and unseen prompts. These steps aim \\nto guarantee the effectiveness, robustness, and versatility of the devel\\xc2\\xad\\noped platform in practical applications. \\n3.1. Comparison of fine-tuning, prompt tuning, and direct generation \\nTo assess performance of the fine-tuned LLM, we compare the \\naccuracy of auto-building modeling under specified prompts between \\nthe fine-tuned LLM and two baselines without fine-tuning, i.e., direct \\ngeneration (original LLM directly generates content based on the input \\nprompt) and one-shot learning (original LLM is fed with one example of \\nprompt-IDF pair, then instructing LLM for similar auto-modeling) [40]. \\nFour prompts were designed for comparison of auto-model generation \\nwith-out fine-tuning and the test results are presented in Table 2. For \\ndirect generation, we observed that regardless of the prompt complexity, \\nthe output results are irrelevant to the task content. This is not surprising \\nsince Flan-T5 was not trained with any knowledge of building modeling \\nor EnergyPlus software in the previous pre-training phase. As for the \\none-shot learning, it appears that the model only generates a few initial \\nwords with formatting errors, garbled content, and incomplete \\ncomprehension. Furthermore, due to Flan-T5 being trained only on \\nsentences with a maximum length of 512 tokens during the pre-training \\nphase, it is insufficient to produce a complete building description files \\nwith sufficient length. However, fine-tuning can overcome the limita\\xc2\\xad\\ntions of the foundation model\\xe2\\x80\\x99s token constraints. This further demon\\xc2\\xad\\nstrates the effectiveness of our fine-tuning process in model validation \\n(Section 3), which enables the model to adapt to downstream tasks and \\nachieve 100% accuracy in auto-generation of building model \\ndescription. \\n3.2. Validation of Eplus-LLM generation \\nAccording to the four prompts in Section 2.4, we randomly generated \\n10 instances and manually generated 10 instances for four types of \\nprompts in this validation process to verify the output produced by the \\nEplus-LLM, resulting in a total of 80 instances. The randomly and \\nmanually generated prompt examples are presented in Table 3. The \\nvalidation of the accuracy is based on the match of the generated IDF \\nfile, the corresponding model, and simulation results, including indoor \\ntemperature and energy consumptions (Fig. 5). The validation results \\nindicate that all 80 instances can be correctly invoked by the EnergyPlus \\nengine. Additionally, they perfectly matched the ground truth models \\nand simulation results, achieving 100% accuracy. The first prompts can \\nonly generate building models with a default WWR (i.e., WWR = 0.3). \\nThe second prompt can generate different WWR but cannot specify the \\nposition of the window. The third and fourth prompts allow for speci\\xc2\\xad\\nfying the window position. \\n3.3. Comparison of time efficiency between manual and LLM-based \\nmodeling approaches \\nIn this section, to evaluate the time efficiency of the LLM-based \\nmodeling method, we compared the modeling time of using the Eplus- \\nLLM platform with two manual modeling approaches, i.e., directly \\nusing EnergyPlus IDF Editor and through OpenStudio GUI for BEM. We \\nmeasured the time taken for modeling across 24 instances covered in \\nSection 3.2, which includes 8 prompts*3 instances of building models in \\ndifferent complexity. Specific experiment results are shown in Table 4. \\nFor manual modeling using EnergyPlus, users need to define model \\ngeometry with coordinates. This necessitates users to pre-calculate po\\xc2\\xad\\nsition of different points on building surfaces based on the design \\nfloorplan and determine window coordinates according to the WWR, \\ntaking approximately 70% of the entire modeling time. Particularly \\nwhen the model precision extends to centimeters, manual calculation \\nnot only becomes burdensome but also prone to errors, significantly \\nprolonging the modeling process. Additionally, inputting other required \\ninformation such as space types and thermal zones in the software to \\nensure alignment with internal loads occupies ~30% of the modeling \\ntime as remaining. Overall, depending on the model complexity, con\\xc2\\xad\\nstructing an instance model in EnergyPlus typically takes between 35 \\nand 56 min. To make modeling process easier, OpenStudio is the \\ngraphical user interface that enables users to construct geometry directly \\nthrough drawing, helping reduce the modeling efforts. However, this \\nTable 1 \\nFour types of prompt formats.  \\nPrompt \\nFormat \\nSimulation requirement \\nparameters \\nPrompt \\n1 \\nLength, weight, height \\n\\xe2\\x80\\x9cSimulate a building that is xx \\nmeters long, xx meters wide, and xx \\nmeters high.\\xe2\\x80\\x9d \\n2 \\nLength, weight, height, WWR, \\n\\xe2\\x80\\x9cSimulate a building that is xx \\nmeters long, xx meters wide, and xx \\nmeters high. The window-to-wall \\nratio is xx.\\xe2\\x80\\x9d \\n3 \\nLength, weight, height, WWR, \\nwindow position, \\n\\xe2\\x80\\x9cSimulate a building that is xx \\nmeters long, xx meters wide, and xx \\nmeters high. The window-to-wall \\nratio is xx, the window sill height is \\nxx meters, the window height is xx \\nmeters, and the window jamb width \\nis xx meters.\\xe2\\x80\\x9d \\n4 \\nLength, weight, height, WWR, \\nwindow position, occupant, \\nlighting, equipment \\n\\xe2\\x80\\x9cSimulate a building that is xx \\nmeters long, xx meters wide, and xx \\nmeters high. The window-to-wall \\nratio is xx, the window sill height is \\nxx meters, the window height is xx \\nmeters, and the window jamb width \\nis xx meters. The occupancy rate is \\nxx m2/people, the lighting level is xx \\nW/m2, and the equipment power \\nconsumption is xx W/m2.\\xe2\\x80\\x9d  \\nFig. 3. Explanation of the window position in the prompts.  \\nG. Jiang et al.                                                                                                                                                                                                                                    \\n', b'Applied Energy 367 (2024) 123431\\n8\\nstill requires establishing a spacing grid and drawing (~30% of the time \\nto use OpenStudio in modeling), assigning various information such as \\nstories, thermal zones, space types, constructions, and windows in the \\ngeometry interface tab (~30% of the time), and pre-setting space types, \\nthermal zones, and internal loads (~40% of the time). Constructing an \\ninstance in OpenStudio takes from 11 to 20 min for different cases. \\nLastly, establishing a building model in the Eplus-LLM platform only \\nrequires users to write a natural language prompt in the platform \\ninterface, after which Eplus-LLM automatically generates the corre\\xc2\\xad\\nsponding building model in IDF format for modeling (Fig. 6). This pro\\xc2\\xad\\ncess takes approximately 1 min, including about 30 s to write prompts \\nand another 30s for LLM generation. \\nIn conclusion, utilizing Eplus-LLM for auto-modeling through natu\\xc2\\xad\\nral language can significantly reduce the modeling efforts by over 95% \\nwhile ensuring modeling accuracy. Moreover, for beginners, manual \\nmodeling of buildings using software programs has a steeper learning \\ncurve, including initial model establishment and troubleshooting. It is \\nforeseeable that as building models grow in complexity, the efficiency \\ngains from using LLM-based auto-modeling methods is expected to \\nbecome more pronounced. \\n3.4. Robustness evaluation of Eplus-LLM in auto-modeling \\nIn order to evaluate the robustness of our prompts, we introduced \\nfour types of noises for every prompt format: user\\xe2\\x80\\x99s tone styles, spelling \\nmistakes, omitted words, and extra words. We conducted two tests for \\neach prompt under every noise condition, resulting in a total of 4*4*2 =\\n32 instances. Table 5 lists the examples of different noises. The test re\\xc2\\xad\\nsults indicate that our model exhibits high robustness, flexibility, and \\nresistance to the noise of user commands in practice. Even under varying \\ndegrees of noise influence, it can still generate results that meet user \\nrequirements with 100% accuracy in generation. \\nIn addition, during our testing, we observed that Eplus-LLM exhibits \\na certain degree of self-learning, specifically, the ability to process \\npreviously unseen information (Table 6). Through fine-tuning in \\ntraining, Eplus-LLM acquired new concise expressions for prompts. \\nThese new prompts were not included in the fine-tuning datasets. For \\nExample, for the unseen Prompt 4, Eplus-LLM learned the ability to \\nidentify window locations. Consequently, modelers only need to specify \\nthe window sill and window height, eliminating the need to additionally \\nspecify the WWR and window jamb, as required previously. For the \\nunseen Prompt 1, 2 and 3, Eplus-LLM can adapt to new prompt struc\\xc2\\xad\\ntures by learning the interrelationships between various prompts \\n(Prompt 3 and 4). This illustrates the versatility, adaptability, and \\nscalability of Eplus-LLM. We conducted 10 tests for each unseen prompt. \\n4. Discussion and further work \\n4.1. Model structure and mechanism for LLM-based auto-building \\nmodeling \\nWhile current LLMs employ various model structures, e.g., decoders- \\nonly, encoders-only, or decoders-encoders for diverse tasks, there is no \\nconsensus on the optimal model structure for the task of IDF generation. \\nAlthough PTMs that rely solely on decoders or encoders, such as BERT \\nand GPT, have achieved some success, they are suboptimal in compre\\xc2\\xad\\nhending tasks or processing code due to the inherent constraints and \\nlimited flexibility associated with decoder or encoder model structures \\n[27,31]. The auto building modeling task to generate model description \\nin length with specific required structure (as the advantage of decoders). \\nAlso, the utilized LLM needs to thoroughly comprehend the user\\xe2\\x80\\x99s \\nmodeling description (as the advantage of encoders), ensuring a precise \\nunderstanding and reasoning of the corresponding IDF details, as even \\nminor errors can lead to simulation failure. The encoders-decoders \\nmodel proves to be particularly advantageous in tasks requiring intri\\xc2\\xad\\ncate mapping and capturing of element relationships between input and \\noutput sequences; that is why we chose T5 with encoders-decoders \\nstructure as the foundation model for our auto-modeling. \\nWhile the working mechanism of LLMs remains a black box, posing \\nchallenges in understanding how distinct models excel in various \\nFig. 4. An example of a specific prompt with detailed IDF.  \\nG. Jiang et al.                                                                                                                                                                                                                                    \\n', b'Applied Energy 367 (2024) 123431\\n9\\ndownstream tasks and the rationale behind specific outcomes, one cer\\xc2\\xad\\ntainty prevails: attention is the key to the success of LLMs [41]. In this \\nstudy, the Eplus-LLM utilizes attention to focus on different parts of the \\nuser simulation command, capture complex relationships in natural \\nlanguage descriptions of building models, and map these descriptions to \\nbuilding model files. Improvements of attention mechanism, as listed \\nbelow, are expected to further boost the LLM applications in auto- \\nbuilding modeling:  \\n(1) Capability to Handle Longer Context in Attention \\nCurrent LLMs typically support context lengths of 2048 or 4096 to\\xc2\\xad\\nkens. Longer text content acceptable by LLM indicates its increasing \\ncapability to support modeling of more complex buildings and enhance \\nhuman-AI interaction in the auto modeling process. This further in\\xc2\\xad\\ncreases the flexibility of auto-building modeling. Block-sparse Fla\\xc2\\xad\\nshAttention [42], attention sink [43], and related methods expected to \\nserve as potential techniques for scaling up the acceptable tokens.  \\n(2) Efficient Attention and Model Architecture \\nModifying the conventional attention architecture (e.g., through the \\naddition or combination of diverse layers) as exemplified by approaches \\nsuch as SOLAR [44], MOE [45], and Mistral [46] is anticipated to \\nyield more efficient models. These modifications aim to improve \\ncomputational efficiency, ultimately facilitating an efficient and precise \\ninference process. \\n4.2. Prompts and instructions to boost LLMs \\nIn this study, we chose Flan-T5 (encoders-decoders structure) as the \\nfoundation model for our Eplus-LLM platform, achieving satisfactory \\nperformance in auto-model generation. Besides the model structure, the \\ninherent scaling prompt and instruction process used to produce Flan-T5 \\nis also a key factor contributing to its success. By scaling the number of \\ntasks, scaling the model size, and finetuning on chain-of-thought data, \\nthe performance of the model is greatly enhanced, enabling it to attain \\nstrong capability even compared to larger LLMs. This makes Flan-T5 \\nmore suitable for various downstream tasks that require fine-tuning, \\nsuch as auto-building modeling in this study. \\nWith limited computational resources, using high-quality prompt \\nand instruction for model fine tuning is an important approach to \\nimprove model performance [32]. By skillfully designing prompts and \\ninstructions, we can improve model performance by directing the model \\nto focus on important information in the building design and simulation \\ntask. In our study, we carefully designed four types of prompts to \\nenhance the flexibility, versatility, and robustness of the fine-tuned LLM \\nTable 2 \\nBaseline results without fine-tuning.  \\nPrompt \\nformat \\nInput prompt \\nOutput (Direct \\ngeneration) \\nOutput (One-shot \\nlearning) \\n1 \\n\\xe2\\x80\\x9cGenerate IDF using \\nEnergyPlus: Simulate a \\nbuilding that is 20.00 \\nmeters long, 40.00 \\nmeters wide, and 4.00 \\nmeters high.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cIt will use a total \\nof 0.002 \\nkilowatts.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cBuildingSurface: \\nDetailed, face 0, floor, \\nextslabcarpet 4in\\xe2\\x80\\x9d \\n2 \\n\\xe2\\x80\\x9cGenerate IDF using \\nEnergyPlus: Simulate a \\nbuilding that is 20.00 \\nmeters long, 40.00 \\nmeters wide, and 4.00 \\nmeters high. The \\nwindow-to-wall ratio is \\n0.3.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cThe total number \\nof windows is.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cBuildingSurface: \\nDetailed, face 0, floor, \\nextslabcarpet 4in\\xe2\\x80\\x9d \\n3 \\n\\xe2\\x80\\x9cGenerate IDF using \\nEnergyPlus: Simulate a \\nbuilding that is 150.00 \\nmeters long, 50.00 \\nmeters wide, and \\n20.00 meters high. The \\nwindow-to-wall ratio is \\n0.60, the windowsill \\nheight is 4.00 meters, \\nthe window height is \\n16.00 meters, and the \\nwindow jamb width is \\n0.01 meters.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cThe total IDF for \\nthe building is.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cBuildingSurface: \\nDetailed, face 0, floor, \\nextslabcarpet 4in\\xe2\\x80\\x9d \\n4 \\n\\xe2\\x80\\x9cGenerate IDF using \\nEnergyPlus: Simulate a \\nbuilding that is 150.00 \\nmeters long, 50.00 \\nmeters wide, and \\n20.00 meters high. The \\nwindow-to-wall ratio is \\n0.60, the windowsill \\nheight is 4.00 meters, \\nthe window height is \\n16.00 meters, and the \\nwindow jamb width is \\n0.01 meters. The \\noccupancy rate is 5.00 \\nm2/people, the lighting \\nlevel is 10.00 W/m2, \\nand the equipment \\npower consumption is \\n20.00 W/m2.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cThe building uses \\na total of.01 m3 of \\nenergy per square \\nmeter.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cBuildingSurface: \\nDetailed, face 0, floor, \\nextslabcarpet 4in\\xe2\\x80\\x9d  \\nTable 3 \\nExamples of model generated for random and manual testing.  \\nRandom input \\nManual input \\nGenerated \\nmodel \\nPrompt \\nGenerated \\nmodel \\nPrompt \\n\\xe2\\x80\\x9cSimulate a building \\nthat is 172.70 meters \\nlong, 337.90 meters \\nwide, and 56.60 meters \\nhigh.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cSimulate a building \\nthat is 20.00 meters \\nlong, 20.00 meters \\nwide, and 5.00 meters \\nhigh.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cSimulate a building \\nthat is 44.60 meters \\nlong, 279.20 meters \\nwide, and 90.50 meters \\nhigh. The window-to- \\nwall ratio is 0.50.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cSimulate a building \\nthat is 80.00 meters \\nlong, 50.00 meters \\nwide, and 10.00 meters \\nhigh. The window-to- \\nwall ratio is 0.30.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cSimulate a building \\nthat is 121.90 meters \\nlong, 44.50 meters \\nwide, and 42.10 meters \\nhigh. The window-to- \\nwall ratio is 0.40, the \\nwindow sill height is \\n12.63 meters, the \\nwindow height is 29.47 \\nmeters, and the window \\njamb width is 0.01 \\nmeters.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cSimulate a building \\nthat is 200.00 meters \\nlong, 100.00 meters \\nwide, and 10.00 meters \\nhigh. The window-to- \\nwall ratio is 0.40, the \\nwindowsill height is \\n3.00 meters, the \\nwindow height is 7.00 \\nmeters, and the window \\njamb width is 0.01 \\nmeters.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cSimulate a building \\nthat is 390.00 meters \\nlong, 217.90 meters \\nwide, and 35.30 meters \\nhigh. The window-to- \\nwall ratio is 0.30, the \\nwindow sill height is \\n12.35 meters, the \\nwindow height is 22.94 \\nmeters, and the window \\njamb width is 0.01 \\nmeters. The occupancy \\nrate is 25.60 m2/ \\npeople, the lighting level \\nis 10.50 W/m2, and \\nthe equipment power \\nconsumption is 5.60 \\nW/m2.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cSimulate a building \\nthat is 150.00 meters \\nlong, 50.00 meters \\nwide, and 20.00 meters \\nhigh. The window-to- \\nwall ratio is 0.60, the \\nwindowsill height is \\n4.00 meters, the \\nwindow height is 16.00 \\nmeters, and the window \\njamb width is 0.01 \\nmeters. The occupancy \\nrate is 5.00 m2/people, \\nthe lighting level is \\n10.00 W/m2, and the \\nequipment power \\nconsumption is 20.00 \\nW/m2.\\xe2\\x80\\x9d  \\nG. Jiang et al.                                                                                                                                                                                                                                    \\n', b'Applied Energy 367 (2024) 123431\\n10\\nin auto-building modeling. Prompts usually contain information about \\nthe context of the task, as crucial to help the LLM comprehend the re\\xc2\\xad\\nquirements of the task. By designing the prompt appropriately, models \\ncan successfully focus on key information in the input sequence (e.g., \\ninformation about the building geometry and internal loads). On the \\nother hand, instructions with more specific and detailed input guidance \\ncan tell the model how to handle the task. Instruction can be designed to \\nemphasize specific patterns, regularities, or features that help the model \\nlearn key information about the task. For example, in our task, in\\xc2\\xad\\nstruction is set as \\xe2\\x80\\x9csimulation\\xe2\\x80\\x9d to tell the model to perform a simulation \\ntask. Providing explicit guidance helps the model to fully utilize the pre- \\ntrain data to make the model fine-tuning converge faster, learn key as\\xc2\\xad\\npects of the task, and reduce its sensitivity to noise. \\nNotably, by flexibly combining and designing prompts and in\\xc2\\xad\\nstructions according to the simulation requirements, models become \\nbetter adaptable to a variety of unseen prompt types rather than being \\nlimited to prompts from the training dataset (initial prompts). This \\nflexibility is essential to improve the generality and robustness of the \\ndeveloped auto-modeling platform. For building design and simulation \\nas a downstream task, designing excellent prompts and instructions \\nbecomes an effective means to improve model performance and \\nFig. 5. Simulation results, including indoor temperature, heating, cooling, and electricity consumptions.  \\nTable 4 \\nEstimated time range of manual and LLM-based modeling.   \\nEnergyPlus \\nOpenStudio \\nEplus-LLM \\nModeling time \\n35\\xe2\\x80\\x9356 min \\n11\\xe2\\x80\\x9320 min \\n40\\xe2\\x80\\x9374 s  \\nFig. 6. Input a prompt to Eplus-LLM platform for automated modeling.  \\nG. Jiang et al.                                                                                                                                                                                                                                    \\n', b'Applied Energy 367 (2024) 123431\\n11\\ndiversity. The key to this design lies in a profound understanding of the \\nrelationship between simulation tasks and model predictions, ensuring \\nthat the prompt and instruction boost the model\\xe2\\x80\\x99s reasoning ability. \\n4.3. Empowering the entire building life cycle with LLM \\nThis study represents a pioneering effort to automate building model \\ncreation directly from natural language descriptions provided by mod\\xc2\\xad\\nelers. This is especially useful to assist architects and modelers to assess \\nbuilding design in the conceptual design stage through quick and \\nautomated generation of building models. Furthermore, the LLMs have \\nthe potential to support building development in its full lifecycle (such \\nas construction and operation), not limited to the model design phase in \\nthis paper. For example, during the construction phase, LLM can facili\\xc2\\xad\\ntate interaction between construction personnel and AI to provide real- \\ntime decision support, optimize construction schedules, and predict \\npotential risks. In the operation phase, LLMs-based methods will facili\\xc2\\xad\\ntate building control or community-level control through real-time \\nhuman-AI interaction, maximizing occupant comfort, and the poten\\xc2\\xad\\ntial for building energy efficiency and decarbonization. Overall, the \\napplication of LLMs across the entire building lifecycle has the potential \\nto further improve efficiency, reduce costs, enhance decision support, \\nand steer the building sector toward a smart and sustainable future. \\n5. Conclusion \\nIn this study, we introduce and demonstrate the successful devel\\xc2\\xad\\nopment of Eplus-LLM as the first LLM-based automated building \\nmodeling platform. The platform provides a user-friendly human-AI \\ninterface, allowing users to conduct building simulation directly from \\nnatural language, without requiring in-depth knowledge of complex \\nbuilding science and simulation software. The Eplus-LLM understands \\nhuman language through tokenization and embedding techniques, and \\novercomes the complexity of auto-file generation for building modeling \\nthrough self-attention. Then, it can output building models and simu\\xc2\\xad\\nlation results by invoking EnergyPlus as the simulation engine. This \\ninnovation greatly reduces the modeling efforts and dependency of \\nsoftware in modeling. In order to meet the simulation needs of different \\nusers, we also designed four different prompts to make Eplus-LLM more \\nadaptable and versatile. \\nTo validate the effectiveness of our developed platform in support \\nmodeling practice, we presented a total of 152 test cases. The validation \\nTable 5 \\nExamples of different noised prompts.  \\nFormat \\nInitial prompt \\nDifferent Tone styles \\nSpelling mistakes \\nOmitted words \\nExtra words \\n1 \\n\\xe2\\x80\\x9cSimulate a building that is \\n30.00 meters long, 40.00 meters \\nwide, and 3.50 meters high.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cCreate a model representing a \\nbuilding with dimensions of \\n30.00 meters in length, 40.00 \\nmeters in width, and a height of \\n3.50 meters.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cSimulatte a bilding that is \\n30.00 meteres long, 40.00 \\nmeter wide, and 3.50 meters \\nhi.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cbuilding 30.00 meters long, \\n40.00 meters wide, 3.50 meters \\nhigh.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cSimulate a giraffe building that is \\n30.00 marshmallow meters long, \\n40.00 kazoo meters wide, and 3.50 \\ntrampoline meters high.\\xe2\\x80\\x9d \\n2 \\n\\xe2\\x80\\x9cSimulate a building that is \\n33.30 meters long, 455.50 \\nmeters wide, and 8.80 meters \\nhigh. The window-to-wall ratio is \\n0.33.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cGenerate a simulation for a \\nstructure with dimensions of \\n33.30 meters in length, 455.50 \\nmeters in width, and a height of \\n8.80 meters. The window-to-wall \\nratio is set at 0.33.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cSimulate aa buidling that is \\n33.30 meters long, 455.50 \\nmetrs wide, and 8.80 meters \\nhigh. The window-to-wll ratio \\nis 0.33.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cbuilding 33.30 meters long, \\n455.50 meters wide, 8.80 \\nmeters high. The window wall is \\n0.33.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cSimulate a pineapple trampoline \\ngiraffe building that is 33.30 kazoo \\nmeters long, 455.50 marshmallow \\nxylophone meters wide, and 8.80 \\nsombrero meters high. The \\nbubblegum window-to-pickle wall \\nratio is 0.33.\\xe2\\x80\\x9d \\n3 \\n\\xe2\\x80\\x9cSimulate a building that is \\n36.50 meters long, 326.00 \\nmeters wide, and 55.50 meters \\nhigh. The window-to-wall ratio is \\n0.44, the window sill height is \\n8.72 meters, the window height is \\n49.39 meters, and the window \\njamb width is 0.01 meters.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cDevelop a simulation for a \\nstructure with dimensions of \\n36.50 meters in length, 326.00 \\nmeters in width, and a height of \\n55.50 meters. Integrate a \\nwindow-to-wall ratio of 0.44, \\nwith a window sill height of 8.72 \\nmeters, a window height of \\n49.39 meters, and a window \\njamb width of 0.01 meters.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cSimullate a building that is \\n36.50 meters long, 326.00 \\nmeters wide, and 55.50 meters \\nhiegh. The window-to-wal ratio \\nis 0.44, the window sill hieght is \\n8.72 meters, the window hieght \\nis 49.39 metrs, and the window \\njamb width is 0.01 meters.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cbuilding 36.50 meters long, \\n326.00 meters wide, 55.50 \\nmeters high. Window-wall is \\n0.44, the window sill is 8.72 \\nmeters, the window height is \\n49.39 meters, window jamb is \\n0.01 meters.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cSimulate an extraordinarily \\npurple building that is 36.50 \\nextremely meters long, 326.00 \\npeculiarly meters wide, and 55.50 \\nfascinatingly meters high. The \\nwindow-to-wall ratio is 0.44, the \\nastonishing window sill height is \\n8.72 delightfully meters, the \\nwindow height is 49.39 \\nmysteriously meters, and the \\nwindow jamb width is 0.01 \\nexcessively meters.\\xe2\\x80\\x9d \\n4 \\n\\xe2\\x80\\x9cSimulate a building that is \\n83.50 meters long, 55.00 meters \\nwide, and 16.00 meters high. \\nThe window-to-wall ratio is \\n0.35, the window sill height is \\n3.20 meters, the window height is \\n12.80 meters, and the window \\njamb width is 0.01 meters. The \\noccupancy rate is 5.50 m2/ \\npeople, the lighting level is 18.00 \\nW/m2, and the equipment power \\nconsumption is 10.00 W/m2.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cCreate a model representing a \\nbuilding measuring 83.50 meters \\nin length, 55.00 meters in width, \\nand 16.00 meters in height. \\nIncorporate specific window \\nfeatures like a window-to-wall \\nratio of 0.35, a window sill \\nheight of 3.20 meters, a window \\nheight of 12.80 meters, and a \\nwindow jamb width of 0.01 \\nmeters. Take into account an \\noccupancy rate of 5.50 square \\nmeters per person, a lighting level \\nof 18.00 watts per square meter, \\nand equipment power \\nconsumption of 10.00 watts per \\nsquare meter.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cSimullate a bilding that is \\n83.50 meters long, 55.00 metrs \\nwide, and 16.00 meters hiegh. \\nThe window-to-wll ratio is \\n0.35, the window sill hight is \\n3.20 metrs, the window hieght \\nis 12.80 meters, and the \\nwindow jamb widht is 0.01 \\nmetrs. The occupancy rate is \\n5.50 m2/people, the lightnng \\nlevel is 18.00 W/m2, and the \\nequipmnt power consuption is \\n10.00 W/m2\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cBuilding 83.50 meters long, \\n55.00 meters wide, 16.00 \\nmeters high. The window-to- \\nwall 0.35, the sill height is 3.20 \\nmeters, the window height is \\n12.80 meters, and the window \\njamb is 0.01 meters. \\nOccupancy 5.50 m2/people, \\nlighting 18.00 W/m2, and \\nequipment is 10.00 W/m2.\\xe2\\x80\\x9d \\n\\xe2\\x80\\x9cSimulate a futuristic building that \\nis 83.50 meters long, 55.00 meters \\nwide, and 16.00 meters high. The \\nwindow-to-wall ratio, a key \\nelement of its design, is \\nmeticulously set at 0.35. The \\nwindow sill height gracefully \\nextends to 3.20 meters, while the \\nsoaring window height reaches an \\nimpressive 12.80 meters, with a \\nsleek window jamb width of 0.01 \\nmeters. The occupancy rate, \\ncarefully calculated, stands at \\n5.50 m2/people, ensuring a \\nharmonious balance within its \\nspace. Illuminating the \\nsurroundings, the lighting level \\nradiates at 18.00 W/m2, creating \\na vibrant atmosphere. \\nFurthermore, the building\\xe2\\x80\\x99s \\nefficiency is evident as the \\nequipment power consumption is \\nmaintained at a sustainable 10.00 \\nW/m2.\\xe2\\x80\\x9d  \\nG. Jiang et al.                                                                                                                                                                                                                                    \\n', b'Applied Energy 367 (2024) 123431\\n12\\nresults demonstrate that Eplus-LLM not only achieved 95% time effi\\xc2\\xad\\nciency and 100% accuracy aligning with manual expert modeling, but \\nalso exhibited robustness and adaptability to various noises and unseen \\nprompts in application. This robustness underscores the effectiveness of \\nour approach as basis for further applications, such as UBEM and cali\\xc2\\xad\\nbration. In the discussion section, we introduce and explore the di\\xc2\\xad\\nrections for model selection and attention mechanism in LLMs. \\nAdditionally, we discuss the strategies to boost the LLM performance \\nthrough prompts and instructions. Lastly, we project the future impacts \\nof generative AI with LLMs to support building development for the \\nentire life cycle of buildings. \\nAs to limitations, our developed platform is currently subject to \\nobjective conditions (e.g., GPUs, training time, and LLM performance) \\nand is only able to handle relatively simple modeling cases under regular \\nsettings (e.g., rectangular building shape with WWR) as representative \\nexamples. In practical building modeling, various systems with complex \\ngeometries, different zones, and schedules are expected, requiring a \\nfurther enhancement of the developed platform for automated \\nmodeling. Moreover, our platform has not been able to process in\\xc2\\xad\\nterdependencies such as \\xe2\\x80\\x9cplacing this window in the xxx position of the \\nsouth wall\\xe2\\x80\\x9d since it requires the LLM to obtain more semantic infor\\xc2\\xad\\nmation and make corresponding changes. \\nFuture research directions will include further exploring and \\napplying LLMs to enhance their potential in various real-world appli\\xc2\\xad\\ncations. We advocate for investigations on more flexible and complex \\nmodeling scenarios, such as buildings with complex zoning or prompts \\nfor auto-modeling containing semantic description of buildings (e.g., \\nsouth/north wall). These efforts will further advance the development of \\nauto building modeling platform, providing more powerful (AI) tools for \\nfuture building design and intelligent building management. \\nCRediT authorship contribution statement \\nGang Jiang: Writing \\xe2\\x80\\x93 original draft, Validation, Methodology, \\nInvestigation, Conceptualization. Zhihao Ma: Validation, Software, \\nMethodology. Liang Zhang: Writing \\xe2\\x80\\x93 review & editing, Software, \\nFormal analysis. Jianli Chen: Writing \\xe2\\x80\\x93 review & editing, Writing \\xe2\\x80\\x93 \\noriginal draft, Supervision, Resources, Methodology, Formal analysis, \\nConceptualization. \\nDeclaration of competing interest \\nThe authors declare that they have no known competing financial \\ninterests or personal relationships that could have appeared to influence \\nthe work reported in this paper. \\nData availability \\nThe authors are unable or have chosen not to specify which data has \\nbeen used. \\nAcknowledgement \\nWe would like to acknowledge the funding provided by the US Na\\xc2\\xad\\ntional Science Foundation (NSF). Award title: Elements: A Convergent \\nPhysics-based and Data-driven Computing Platform for Building \\nModeling (#2311685). \\nReferences \\n[1] IEA \\xe2\\x80\\x93 International Energy Agency. IEA. https://www.iea.org; 2024 (n.d. accessed \\nJanuary 14, 2024). \\n[2] Ding Y, Han S, Tian Z, Yao J, Chen W, Zhang Q. Review on occupancy detection \\nand prediction in building simulation. Build Simul 2022;15:333\\xe2\\x80\\x9356. https://doi. \\norg/10.1007/s12273-021-0813-8. \\n[3] Zhou X, Liu R, Tian S, Shen X, Yang X, An J, et al. A review of validation methods \\nfor building energy modeling programs. Build Simul 2023;16:2027\\xe2\\x80\\x9347. https:// \\ndoi.org/10.1007/s12273-023-1050-0. \\n[4] Al-Homoud MS. Computer-aided building energy analysis techniques. Build \\nEnviron 2001;36:421\\xe2\\x80\\x9333. https://doi.org/10.1016/S0360-1323(00)00026-3. \\n[5] Ansys | Engineering Simulation Software. n.d. https://www.ansys.com/. [Accessed \\n14 January 2024]. \\n[6] Dymola. Dassault Syst`emes. https://www.3ds.com/products/catia/dymola; 2023 \\n(accessed January 14, 2024). \\n[7] Welcome | TRNSYS : Transient System Simulation Tool. n.d. https://www.trnsys. \\ncom/. [Accessed 14 January 2024]. \\n[8] Yan D, Xia J, Tang W, Song F, Zhang X, Jiang Y. DeST \\xe2\\x80\\x94 An integrated building \\nsimulation toolkit part I: fundamentals. Build Simul 2008;1:95\\xe2\\x80\\x93110. https://doi. \\norg/10.1007/s12273-008-8118-8. \\n[9] EnergyPlus. n.d. https://energyplus.net/. [Accessed 14 January 2024]. \\n[10] 14:00\\xe2\\x80\\x93-17:00. ISO 13790:2008. ISO; 2024. n.d. https://www.iso.org/standard/4 \\n1974.html (accessed January 14, 2024) \\n[11] VDI 6007 Blatt 1 - Calculation of transient thermal response of rooms and buildings \\n- Modelling of rooms. 2015. \\n[12] DesignBuilder Software Ltd - Home. n.d. https://designbuilder.co.uk/. [Accessed \\n14 January 2024]. \\n[13] OpenStudio. n.d. https://openstudio.net/; 2024 (accessed January 14, 2024) \\nTable 6 \\nUnseen types of prompt format.   \\nFormat \\nInitial simulation requirement \\nparameters \\nSimulation requirement \\nparameters \\nUnseen prompt \\nNew \\nprompt \\n1 \\nLength, weight, height \\nLength, \\nweight, \\nheight \\n+\\nwindow position \\n\\xe2\\x80\\x9cSimulate a building that is xx meters long, xx meters wide, and xx meters high. The \\nwindow sill height is xx meters, the window height is xx meters\\xe2\\x80\\x9d \\n2 \\nLength, weight, height \\nLength, \\nweight, \\nheight \\n+\\nlighting, occupancy, \\nequipment \\n\\xe2\\x80\\x9cSimulate a building that is xx meters long, xx meters wide, and xx meters high. The \\noccupancy rate is xx m2/people, the lighting level is xx W/m2, and the equipment \\npower consumption is xx W/m2.\\xe2\\x80\\x9d \\n3 \\nLength, weight, height \\nLength, \\nweight, \\nheight \\n+\\nWWR, \\nlighting, occupancy, \\nequipment \\n\\xe2\\x80\\x9cSimulate a building that is xx meters long, xx meters wide, and xx meters high. The \\nwindow-to-wall ratio is xx. The occupancy rate is xx m2/people, the lighting level is \\nxx W/m2, and the equipment power consumption is xx W/m2.\\xe2\\x80\\x9d \\nConcise \\nprompt \\n4 \\nLength, weight, height, WWR, window \\nposition, occupant, lighting, equipment \\nLength, \\nweight, \\nheight, \\nwindow position, lighting, \\noccupancy, equipment \\n\\xe2\\x80\\x9cSimulate a building that is xx meters long, xx meters wide, and xx meters high. The \\nwindow sill height is xx meters, the window height is xx meters. The occupancy rate is \\nxx m2/people, the lighting level is xx W/m2, and the equipment power consumption \\nis xx W/m2.\\xe2\\x80\\x9d  \\nG. Jiang et al.                                                                                                                                                                                                                                    \\n', b'Applied Energy 367 (2024) 123431\\n13\\n[14] eppy. PyPI. https://pypi.org/project/eppy/; 2022 (accessed January 14, 2024). \\n[15] Wang H, Li J, Wu H, Hovy E, Sun Y. Pre-trained language models and their \\napplications. Engineering 2023;25:51\\xe2\\x80\\x9365. https://doi.org/10.1016/j. \\neng.2022.04.024. \\n[16] Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, et al. Deep \\ncontextualized word representations. 2018. https://doi.org/10.48550/ \\narXiv.1802.05365. \\n[17] Radford A, Narasimhan K, Salimans T, Sutskever I. Improving language \\nunderstanding by generative pre-training. 2024. n.d. \\n[18] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention \\nis all you need. 2023. https://doi.org/10.48550/arXiv.1706.03762. \\n[19] Devlin J, Chang M-W, Lee K, Toutanova K. BERT: Pre-training of Deep \\nBidirectional Transformers for Language Understanding. 2019. https://doi.org/ \\n10.48550/arXiv.1810.04805. \\n[20] Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, et al. Exploring the \\nlimits of transfer learning with a unified text-to-text transformer. 2023. https://doi. \\norg/10.48550/arXiv.1910.10683. \\n[21] Chowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts A, et al. PaLM: \\nScaling Language Modeling with Pathways. 2022. https://doi.org/10.48550/ \\narXiv.2204.02311. \\n[22] Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M-A, Lacroix T, et al. LLaMA: \\nOpen and Efficient Foundation Language Models. 2023. https://doi.org/10.48550/ \\narXiv.2302.13971. \\n[23] Donahue J, Jia Y, Vinyals O, Hoffman J, Zhang N, Tzeng E, et al. DeCAF: A Deep \\nConvolutional Activation Feature for Generic Visual Recognition. 2013. https:// \\ndoi.org/10.48550/arXiv.1310.1531. \\n[24] Schneider S, Baevski A, Collobert R, Auli M. wav2vec: Unsupervised Pre-training \\nfor Speech Recognition. 2019. https://doi.org/10.48550/arXiv.1904.05862. \\n[25] Grisoni F. Chemical language models for de novo drug design: challenges and \\nopportunities. Curr Opin Struct Biol 2023;79:102527. https://doi.org/10.1016/j. \\nsbi.2023.102527. \\n[26] Zhang Y, Zhang F, Chen N. Migratable urban street scene sensing method based on \\nvision language pre-trained model. Int J Appl Earth Obs Geoinf 2022;113:102989. \\nhttps://doi.org/10.1016/j.jag.2022.102989. \\n[27] Wang Y, Wang W, Joty S, Hoi SCH. CodeT5: Identifier-aware Unified Pre-trained \\nEncoder-Decoder Models for Code Understanding and Generation. 2021. https:// \\ndoi.org/10.48550/arXiv.2109.00859. \\n[28] Ouyang L, Wu J, Jiang X, Almeida D, Wainwright CL, Mishkin P, et al. Training \\nlanguage models to follow instructions with human feedback. 2022. https://doi. \\norg/10.48550/arXiv.2203.02155. \\n[29] Chung HW, Hou L, Longpre S, Zoph B, Tay Y, Fedus W, et al. Scaling instruction- \\nfinetuned language models. 2022. https://doi.org/10.48550/arXiv.2210.11416. \\n[30] Pavlyshenko BM. Financial news analytics using fine-tuned Llama 2 GPT Model. \\n2023. https://doi.org/10.48550/arXiv.2308.13032. \\n[31] Wang Y, Le H, Gotmare AD, Bui NDQ, Li J, Hoi SCH. CodeT5+: open code large \\nlanguage models for code understanding and generation. 2023. https://doi.org/ \\n10.48550/arXiv.2305.07922. \\n[32] Stanford CRFM. n.d. https://crfm.stanford.edu/2023/03/13/alpaca.html. \\n[Accessed 14 January 2024]. \\n[33] Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Language models are \\nunsupervised multitask learners. 2024. n.d. \\n[34] Basha SHS, Dubey SR, Pulabaigari V, Mukherjee S. Impact of fully connected layers \\non performance of convolutional neural networks for image classification. \\nNeurocomputing 2020;378:112\\xe2\\x80\\x939. https://doi.org/10.1016/j. \\nneucom.2019.10.008. \\n[35] Agarap AF. Deep Learning using Rectified Linear Units (ReLU). 2019. https://doi. \\norg/10.48550/arXiv.1803.08375. \\n[36] He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. 2015. \\nhttps://doi.org/10.48550/arXiv.1512.03385. \\n[37] Ba JL, Kiros JR, Hinton GE. Layer normalization. 2016. https://doi.org/10.48550/ \\narXiv.1607.06450. \\n[38] Petelet M, Iooss B, Asserin O, Loredo A. Latin hypercube sampling with inequality \\nconstraints. 2010. https://doi.org/10.48550/arXiv.0909.0329. \\n[39] Britz D, Goldie A, Luong M-T, Le Q. Massive exploration of neural machine \\ntranslation architectures. 2017. https://doi.org/10.48550/arXiv.1703.03906. \\n[40] Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, et al. Language \\nmodels are few-shot learners. arXivOrg, https://arxiv.org/abs/2005.14165v4; \\n2020. [Accessed 31 March 2024]. \\n[41] Guo M-H, Xu T-X, Liu J-J, Liu Z-N, Jiang P-T, Mu T-J, et al. Attention mechanisms \\nin computer vision: a survey. Comput Vis Media 2022;8:331\\xe2\\x80\\x9368. https://doi.org/ \\n10.1007/s41095-022-0271-y. \\n[42] Dao T, Fu DY, Ermon S, Rudra A, R\\xc2\\xb4e C. FlashAttention: Fast and Memory-Efficient \\nExact Attention with IO-Awareness. 2022. https://doi.org/10.48550/ \\narXiv.2205.14135. \\n[43] Xiao G, Tian Y, Chen B, Han S, Lewis M. Efficient streaming language models with \\nattention sinks. 2023. https://doi.org/10.48550/arXiv.2309.17453. \\n[44] Kim D, Park C, Kim S, Lee W, Song W, Kim Y, et al. SOLAR 10.7B: Scaling large \\nlanguage models with simple yet effective depth up-scaling. 2023. https://doi.org/ \\n10.48550/arXiv.2312.15166. \\n[45] Shen S, Hou L, Zhou Y, Du N, Longpre S, Wei J, et al. Mixture-of-experts meets \\ninstruction tuning: a winning combination for large language models. 2023. \\nhttps://doi.org/10.48550/arXiv.2305.14705. \\n[46] Jiang AQ, Sablayrolles A, Mensch A, Bamford C, Chaplot DS, Casas D de las, et al. \\nMistral 7B. 2023. https://doi.org/10.48550/arXiv.2310.06825. \\nG. Jiang et al.                                                                                                                                                                                                                                    \\n']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Text from paper7.pdf:\n",
            "[b'Hybrid Heterogeneous Clusters Can Lower the Energy\\nConsumption of LLM Inference Workloads\\nGrant Wilkins\\ngfw27@cam.ac.uk\\nUniversity of Cambridge\\nCambridge, UK\\nSrinivasan Keshav\\nsk818@cam.ac.uk\\nUniversity of Cambridge\\nCambridge, UK\\nRichard Mortier\\nrmm1002@cam.ac.uk\\nUniversity of Cambridge\\nCambridge, UK\\nABSTRACT\\nBoth the training and use of Large Language Models (LLMs) require\\nlarge amounts of energy. Their increasing popularity, therefore,\\nraises critical concerns regarding the energy efficiency and sus-\\ntainability of data centers that host them. This paper addresses the\\nchallenge of reducing energy consumption in data centers running\\nLLMs. We propose a hybrid data center model that uses a cost-based\\nscheduling framework to dynamically allocate LLM tasks across\\nhardware accelerators that differ in their energy efficiencies and\\ncomputational capabilities. Specifically, our workload-aware strat-\\negy determines whether tasks are processed on energy-efficient\\nprocessors or high-performance GPUs based on the number of in-\\nput and output tokens in a query. Our analysis of a representative\\nLLM dataset, finds that this hybrid strategy can reduce CPU+GPU\\nenergy consumption by 7.5% compared to a workload-unaware\\nbaseline.\\nCCS CONCEPTS\\n\\xe2\\x80\\xa2 Computer systems organization \\xe2\\x86\\x92Heterogeneous (hybrid)\\nsystems; \\xe2\\x80\\xa2 Hardware \\xe2\\x86\\x92Impact on the environment.\\nKEYWORDS\\nsustainable computing, heterogeneous computing, large language\\nmodels, artificial intelligence\\nACM Reference Format:\\nGrant Wilkins, Srinivasan Keshav, and Richard Mortier. 2024. Hybrid Het-\\nerogeneous Clusters Can Lower the Energy Consumption of LLM Inference\\nWorkloads. In The 15th ACM International Conference on Future and Sustain-\\nable Energy Systems (E-Energy \\xe2\\x80\\x9924), June 04\\xe2\\x80\\x9307, 2024, Singapore, Singapore.\\nACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3632775.3662830\\n1\\nINTRODUCTION\\nLarge Language Models (LLMs) such as OpenAI\\xe2\\x80\\x99s GPT-4 [24] and\\nGoogle\\xe2\\x80\\x99s PaLM [4] have become emblematic of the AI revolution,\\ndriving significant advancements not only in natural language un-\\nderstanding, generation, and translation but also in summarizing\\nand contextualizing large volumes of textual data. Characterized by\\ntheir extensive scale and depth, their deployment demands substan-\\ntial computational resources and hence poses significant challenges\\nThis work is licensed under a Creative Commons Attribution International\\n4.0 License.\\nE-Energy \\xe2\\x80\\x9924, June 04\\xe2\\x80\\x9307, 2024, Singapore, Singapore\\n\\xc2\\xa9 2024 Copyright held by the owner/author(s).\\nACM ISBN 979-8-4007-0480-2/24/06\\nhttps://doi.org/10.1145/3632775.3662830\\nin terms of energy consumption and operational efficiency [38].\\nThe increasing application of LLMs across diverse sectors further\\ncompounds these challenges, because datacenters, which are re-\\nsponsible for a considerable portion of global electricity consump-\\ntion, must balance performance targets for LLM tasks running on\\nheterogeneous hardware with the need for energy efficiency [7, 21].\\nIncreasing the energy efficiency of LLMs thus emerges as both a\\ntechnical challenge and an environmental imperative [22].\\nTraditional data center designs often struggle to best exploit the\\ncapabilities of heterogeneous hardware-based LLMs, particularly\\nwhen trying to minimize energy consumption without sacrific-\\ning output quality and latency [6]. However, this challenge also\\npresents an opportunity to innovate in datacenter architecture and\\nmanagement. We show that by rethinking how GPU resources are\\nallocated and managed, there is potential to significantly reduce the\\nenergy footprint of LLM deployments while maintaining or even\\nenhancing computational performance.\\nWe find that a dynamic task-scheduling model that assigns LLM\\ntasks to GPUs based on the resulting energy efficiency can reduce\\noverall energy. Moreover, implementing a workload-aware system\\nfor input and output token processing can further reduce energy\\nusage. Thus, a hybrid datacenter task allocation model, which al-\\nlocates different tasks to different hardware accelerators based on\\ntheir system demands, can reduce the overall energy consumption\\nof LLM inference compared to a workload-unaware baseline.\\nOur contributions are as follows:\\n(1) We analyze the energy consumption and runtime of several\\n7B-parameter LLMs\\xe2\\x80\\x99 across various hardware configurations.\\n(2) We propose and evaluate a workload-aware scheduler for\\nLLMs that optimizes energy efficiency based on the size of\\ninput and output token loads, demonstrating a 7.5% decrease\\nin energy consumption over non-workload-aware baselines.\\n(3) We release a comprehensive dataset and benchmark suite for\\nevaluating the energy efficiency of LLM inference, enabling\\nresearchers and practitioners to assess the impact of their\\ndesign choices.\\nThrough these contributions, we hope to support more sustain-\\nable and cost-effective AI inference deployments.\\nThe remainder of this paper is as follows: Section 2 provides\\nbackground information on LLM inference and energy consump-\\ntion in AI systems. Section 3 formulates the problem and introduces\\nour cost function. Section 4 details the methods used for bench-\\nmarking LLM inference on diverse systems. Section 5 presents the\\nperformance results of LLM inference across multiple hardware con-\\nfigurations. Section 6 proposes and evaluates our energy-optimal\\nhybrid data center design. Finally, Section 7 discusses related works,\\nand Section 8 summarizes the conclusions of the paper.\\n506\\n', b'E-Energy \\xe2\\x80\\x9924, June 04\\xe2\\x80\\x9307, 2024, Singapore, Singapore\\nGrant Wilkins, Srinivasan Keshav, and Richard Mortier\\n2\\nBACKGROUND\\n2.1\\nInference Using Large Language Models\\nTransformer-based neural network architectures have led to im-\\npressive gains in the performance of LLMs for language under-\\nstanding and generation [5]. LLMs such as OpenAI\\xe2\\x80\\x99s GPT-4 [24]\\nand Google\\xe2\\x80\\x99s Gemini [32] have demonstrated human-level profi-\\nciency on many language benchmarks while requiring billions of\\nparameters and massive datasets for training. The inference phase\\nof LLMs involves utilizing a trained model to make predictions\\nbased on new, unseen data. Unlike the training phase, which is\\ntypically a one-time, compute-intensive process that occurs offline,\\ninference is an ongoing, real-time process that directly impacts\\nend-user experiences [7]. This phase is critical as it represents the\\npoint at which AI capabilities become accessible to users.\\nInference in LLMs can be computationally expensive due to sev-\\neral factors: (1) Model Size: The sheer size of these models, often\\nbillions of parameters, necessitates significant computational power\\nto process each query [38]. (2) Latency Expectations: Many appli-\\ncations based on LLMs, such as digital assistants, automated writing\\naids, and real-time translators, require low-latency responses [35].\\n(3) Scalability: The ability to scale inference operations to accom-\\nmodate varying user demands without degradation in response\\ntimes is crucial.\\n2.2\\nEnergy Consumption in AI Systems\\nRecent reports have found that the computational requirements for\\nstate-of-the-art AI entail massive energy consumption and carbon\\nemissions [7, 21, 26, 29, 38]. The energy intensity of AI systems\\ncan be broadly divided into the energy required for training versus\\ninference after models are deployed [13]. Training complex models\\non massive datasets is an energy-intensive process, with estimates\\nfinding that training GPT-3 required 1,287 megawatt-hours of en-\\nergy [26]. LLMs can also have huge emissions depending on deploy-\\nment scale and hardware efficiency [29]. For example, over a year\\nof use, inference by LLMs on cloud infrastructure can consume over\\n25\\xc3\\x97 more energy than training a model [7]. Optimizing software\\nand hardware specifically for AI workloads is thus essential [3].\\n2.3\\nHeterogeneous Systems for Efficient\\nComputing\\nModern systems demonstrate a complex interplay between scale,\\narchitecture, workload behavior and efficiency objectives. The ar-\\nchitecture of compute nodes can significantly impact the energy\\nefficiency and processing capabilities of large-scale computing sys-\\ntems [18]. Conventional server architectures based on multicore\\nCPUs face energy proportionality and scalability limitations for\\nmodern data-intensive workloads [20]. Several researchers have\\nexplored heterogeneous server configurations to improve energy ef-\\nficiency [12, 15, 16, 19]. Distributed solutions can translate to lower\\nenergy efficiency, as communication overheads dominate [9]. Still,\\nspecialized clusters like NVIDIA\\xe2\\x80\\x99s DGX show 4x better performance\\nper watt over conventional servers [30].\\n3\\nPROBLEM FORMULATION\\nTo model the operational demands of a hybrid, heterogeneous data-\\ncenter hosting LLMs, we define a cost function to reflect the work-\\nload distribution across different systems. We define a cost function\\n\\xf0\\x9d\\x91\\x88(\\xf0\\x9d\\x91\\x9a,\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x91\\xa0) that accounts for both energy consumption and runtime:\\n\\xf0\\x9d\\x91\\x88(\\xf0\\x9d\\x91\\x9a,\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x91\\xa0) = \\xf0\\x9d\\x9c\\x86\\xf0\\x9d\\x90\\xb8(\\xf0\\x9d\\x91\\x9a,\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x91\\xa0) + (1 \\xe2\\x88\\x92\\xf0\\x9d\\x9c\\x86)\\xf0\\x9d\\x91\\x85(\\xf0\\x9d\\x91\\x9a,\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x91\\xa0),\\nwhere \\xf0\\x9d\\x91\\x9aand \\xf0\\x9d\\x91\\x9bdenote the number of input and output tokens,\\nrespectively. \\xf0\\x9d\\x9c\\x86\\xe2\\x88\\x88[0, 1] is a tunable parameter that balances the\\nweight of energy efficiency versus speed. \\xf0\\x9d\\x90\\xb8(\\xf0\\x9d\\x91\\x9a,\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x91\\xa0) is the energy\\nconsumed by system \\xf0\\x9d\\x91\\xa0to process \\xf0\\x9d\\x91\\x9ainput tokens and generate \\xf0\\x9d\\x91\\x9b\\noutput tokens, measured in joules. \\xf0\\x9d\\x91\\x85(\\xf0\\x9d\\x91\\x9a,\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x91\\xa0) is the time required to\\nprocess these tokens on system \\xf0\\x9d\\x91\\xa0, measured in seconds.\\nOur objective is to minimize the total cost across all tasks and\\nsystems:\\nmin\\n{\\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x91\\xa0}\\xf0\\x9d\\x91\\xa0\\xe2\\x88\\x88\\xf0\\x9d\\x91\\x86\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\n\\xf0\\x9d\\x91\\xa0\\xe2\\x88\\x88\\xf0\\x9d\\x91\\x86\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\n(\\xf0\\x9d\\x91\\x9a,\\xf0\\x9d\\x91\\x9b)\\xe2\\x88\\x88\\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x91\\xa0\\n\\xf0\\x9d\\x91\\x88(\\xf0\\x9d\\x91\\x9a,\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x91\\xa0)\\n(1)\\ns.t.\\n\\xc3\\x98\\n\\xf0\\x9d\\x91\\xa0\\xe2\\x88\\x88\\xf0\\x9d\\x91\\x86\\n\\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x91\\xa0= \\xf0\\x9d\\x91\\x84\\n(2)\\n\\xe2\\x88\\x80\\xf0\\x9d\\x91\\xa0: \\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x91\\xa0\\xe2\\x88\\xa9\\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x91\\xa0\\xe2\\x80\\xb2 = \\xe2\\x88\\x85for \\xf0\\x9d\\x91\\xa0\\xe2\\x89\\xa0\\xf0\\x9d\\x91\\xa0\\xe2\\x80\\xb2\\n(3)\\nwhere \\xf0\\x9d\\x91\\x86is the set of all systems, \\xf0\\x9d\\x91\\x84is the total set of queries, \\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x91\\xa0is\\nthe subset of queries assigned to system \\xf0\\x9d\\x91\\xa0.\\nThis model ensures that each query is processed exactly once,\\noptimizing for energy efficiency or quick response times, depending\\non the operational needs, as parameterized by \\xf0\\x9d\\x9c\\x86. We note, however,\\nthat certain systems may be better suited to specific tasks, based on\\nthe workload characteristics, such as the need for rapid response\\ntimes. Adjustments in \\xf0\\x9d\\x9c\\x86allow the datacenter to shift its focus be-\\ntween minimizing energy consumption and reducing runtime as\\noperational priorities change.\\n4\\nMETHODS\\nHere, we describe the methods and tools we use to benchmark LLM\\ninference. In all cases, we use Huggingface\\xe2\\x80\\x99s Accelerate [11] to stan-\\ndardize hardware optimization for inference across all platforms. T\\nhis library takes advantage of the available accelerator resources\\nand shards models accordingly to minimize intermediate commu-\\nnication and maximize the distributed capabilities for computation\\nacross the devices.\\n4.1\\nModel Selection\\nOur study employs three 7B-parameter, open-source LLMs for their\\ncapabilities and ability to run on diverse hardware efficiently: (1)\\nFalcon [2], (2) Llama-2 [33], and (3) Mistral [17]. These models\\nwere selected to represent a spectrum of architectures and training\\ncorpora. We subject each model to a series of standardized NLP\\ntasks to evaluate their energy consumption during inference.\\n4.1.1\\nFalcon. The Falcon (7B) [2] model utilizes multi-query atten-\\ntion, significantly reducing memory requirements and increasing\\nprocessing speed. The model\\xe2\\x80\\x99s training on the bilingual RefinedWeb\\ndataset enhances its applicability across diverse linguistic contexts.\\n4.1.2\\nLlama-2. We select Llama-2 (7B) for its optimization in di-\\nalogue tasks and its improvements in safety and helpfulness. The\\n507\\n', b'Hybrid Heterogeneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads\\nE-Energy \\xe2\\x80\\x9924, June 04\\xe2\\x80\\x9307, 2024, Singapore, Singapore\\nmodel\\xe2\\x80\\x99s unique pretraining methodologies and advanced architec-\\ntural features, such as grouped-query attention, make it an ideal\\ncandidate for analyzing energy efficiency in complex language\\ntasks.\\n4.1.3\\nMistral. We include Mistral (7B) [17] for its grouped-query\\nattention and sliding window attention mechanisms, contributing\\nto fast and efficient inference. Its superior performance in vari-\\nous benchmarks, especially in reasoning, mathematics, and code\\ngeneration, makes it an essential model for our analysis.\\n4.2\\nEnergy Profiling of Diverse Systems\\nDepending on the platform, we profile each system\\xe2\\x80\\x99s energy con-\\nsumption during inference using customized setups that capture\\nruntime and energy or power metrics. Here, we describe how\\nwe monitor the energy usage of NVIDIA GPUs, Apple Silicon\\nCPU/GPU, Intel CPUs, and AMD CPUs.\\n4.2.1\\nNVIDIA GPUs. We use PyJoules [27], a Python-based en-\\nergy measurement library, to quantify the energy consumption\\nassociated with inference on NVIDIA GPUs. PyJoules provides an\\ninterface to NVML [23], providing a software-defined energy usage\\nassessment for targeted NVIDIA devices. This tool offers real-time\\nenergy consumption of GPUs for a given tracked process, which is\\na critical component of our analysis given the GPU-heavy compu-\\ntation involved in LLM inference.\\n4.2.2\\nApple Silicon CPU/GPU. No standard energy measurement\\ntools are available for profiling energy and power usage for Ap-\\nple Silicon through an API like PyJoules or RAPL. Therefore, we\\nemploy a daemon-based approach to poll macOS\\xe2\\x80\\x99 powermetrics\\nutility, providing a detailed view of the energy usage during model\\ninference. To capture the energy consumption of the M1 GPU, we\\nexecute the powermetrics command through a Python subprocess.\\nThis command returns the percentage of the CPU power each CPU\\ntop process uses and the total CPU and GPU power consumption\\nin 200ms intervals. This interval was chosen after testing to find\\nthe finest granularity measurement without incurring a significant\\nCPU overhead for the I/O of buffering the large powermetrics\\noutput into memory.\\nThe energy monitoring is conducted concurrently with the LLM\\ninference. A separate thread is dedicated to running the powermetrics\\ncommand, ensuring real-time data collection. Post-inference, the\\ncollected data is processed to extract the recorded power data and\\nthen find the energy consumption through integration over the\\nruntime. The GPU energy consumption, \\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x99,\\xf0\\x9d\\x90\\xba\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x88, is straightfor-\\nward to calculate for each recorded power value, \\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x90\\xba\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x88,\\xf0\\x9d\\x91\\x96, at each\\ntimestep \\xce\\x94\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x96.\\n\\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x99,\\xf0\\x9d\\x90\\xba\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x88=\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\n\\xf0\\x9d\\x91\\x96\\n\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x90\\xba\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x88,\\xf0\\x9d\\x91\\x96\\xce\\x94\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x96.\\nThe CPU power draw data is less clear, as many processes run on the\\nCPU. However, an \"energy impact factor\" through powermetrics\\nallows us to infer how much power our Python inference process\\nuses. Therefore, we calculate the CPU energy, \\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x99,\\xf0\\x9d\\x90\\xb6\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x88, by mul-\\ntiplying \\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x90\\xb6\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x88,\\xf0\\x9d\\x91\\x96by the \"energy impact factor,\" which we denote as\\n\\xf0\\x9d\\x9b\\xbc\\xf0\\x9d\\x91\\x96, at each timestep:\\n\\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x99,\\xf0\\x9d\\x90\\xb6\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x88=\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\n\\xf0\\x9d\\x91\\x96\\n(\\xf0\\x9d\\x9b\\xbc\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x90\\xb6\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x88,\\xf0\\x9d\\x91\\x96)\\xce\\x94\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x96.\\n4.2.3\\nIntel CPUs. For Intel CPUs, we leverage PyJoules, a Python-\\nbased energy measurement library similar to our approach for\\nNVIDIA GPUs. This tool supports RAPL (Running Average Power\\nLimit) interfaces, enabling us to obtain fine-grained energy con-\\nsumption data [36]. We focus on two primary RAPL domains: Pack-\\nage 0 and Package 1, which correspond to the entire CPU package\\xe2\\x80\\x99s\\nenergy consumption, including all cores in the package.\\nPyJoules allows us to capture the energy usage of these domains\\nin real time, enabling us to profile the energy consumption specif-\\nically during model inference tasks. To account for base energy\\nconsumption unrelated to our inference process, we conduct a pre-\\nanalysis phase to measure the CPU\\xe2\\x80\\x99s average idle power draw. This\\nidle measurement is then subtracted from the total energy con-\\nsumption during inference to accurately determine the net energy\\nexpenditure attributable to the inference process.\\nWe instrument our code to query the RAPL readings at the start\\nand end of the inference task, calculating the energy consumption\\nas follows:\\n\\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x99,\\xf0\\x9d\\x90\\xb6\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x88=\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\n\\xf0\\x9d\\x91\\x96\\n\\x12 \\x10\\n\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x98\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x94\\xf0\\x9d\\x91\\x92\\xe2\\x88\\x920,\\xf0\\x9d\\x91\\x96\\xe2\\x88\\x92\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x98\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x94\\xf0\\x9d\\x91\\x92\\xe2\\x88\\x920,\\xf0\\x9d\\x90\\xbc\\xf0\\x9d\\x91\\x91\\xf0\\x9d\\x91\\x99\\xf0\\x9d\\x91\\x92\\n\\x11\\n+\\n\\x10\\n\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x98\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x94\\xf0\\x9d\\x91\\x92\\xe2\\x88\\x921,\\xf0\\x9d\\x91\\x96\\xe2\\x88\\x92\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x98\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x94\\xf0\\x9d\\x91\\x92\\xe2\\x88\\x921,\\xf0\\x9d\\x90\\xbc\\xf0\\x9d\\x91\\x91\\xf0\\x9d\\x91\\x99\\xf0\\x9d\\x91\\x92\\n\\x11 \\x13\\n\\xce\\x94\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x96,\\nwhere \\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x98\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x94\\xf0\\x9d\\x91\\x92\\xe2\\x88\\x920,\\xf0\\x9d\\x91\\x96and \\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x98\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x94\\xf0\\x9d\\x91\\x92\\xe2\\x88\\x921,\\xf0\\x9d\\x91\\x96, represent the power draw\\nfrom Package 0 and Package 1, respectively, and \\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x98\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x94\\xf0\\x9d\\x91\\x92\\xe2\\x88\\x920,\\xf0\\x9d\\x90\\xbc\\xf0\\x9d\\x91\\x91\\xf0\\x9d\\x91\\x99\\xf0\\x9d\\x91\\x92\\nand \\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x98\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x94\\xf0\\x9d\\x91\\x92\\xe2\\x88\\x921,\\xf0\\x9d\\x90\\xbc\\xf0\\x9d\\x91\\x91\\xf0\\x9d\\x91\\x99\\xf0\\x9d\\x91\\x92represent the average idle power draw of the\\nCPU packages, respectively.\\n4.2.4\\nAMD CPUs. We adopt a different strategy for AMD CPUs\\ndue to the absence of a Python API. Instead, we utilize AMD\\xf0\\x9d\\x9c\\x87Prof\\xe2\\x80\\x99s\\ntimechart feature, which provides detailed power draw metrics\\nfor every core on the chip at fine-grained intervals. By polling\\nAMD\\xf0\\x9d\\x9c\\x87Prof at 100ms intervals, we can capture the power draw of\\neach physical core throughout the model inference process.\\nTo ensure we accurately attribute the energy consumption to our\\ninference task, we monitor the CPU core residency through psutil.\\nThis information allows us to identify and record the specific cores\\nactively engaged in the inference process at each time step. The total\\nenergy consumption for the inference task is then calculated by\\nsumming the power usage across all active cores and summing over\\nthe product of the power usage and time of inference, as follows:\\n\\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x99,\\xf0\\x9d\\x90\\xb6\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x88=\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\n\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x92\\n \\xe2\\x88\\x91\\xef\\xb8\\x81\\n\\xf0\\x9d\\x91\\x96\\n\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x92,\\xf0\\x9d\\x91\\x96\\xce\\x94\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x96\\n!\\nwhere \\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x92,\\xf0\\x9d\\x91\\x96represents the power draw of an individual core at\\neach time step, \\xf0\\x9d\\x91\\x96.\\n5\\nLLM INFERENCE PERFORMANCE ON\\nDIVERSE CLUSTERS\\n5.1\\nHardware and Software Versions\\nThe systems we profile are shown in Table 1. We consider these sys-\\ntems as they demonstrate three prominent CPU manufactures and\\ndifferent generations of GPUs. We utilize PyTorch v2.0.1, Torchvi-\\nsion v0.15.2, Numpy v1.26.0, Huggingface v0.20.2, and Accelerate\\nv0.26.1.\\n508\\n', b'E-Energy \\xe2\\x80\\x9924, June 04\\xe2\\x80\\x9307, 2024, Singapore, Singapore\\nGrant Wilkins, Srinivasan Keshav, and Richard Mortier\\nSystem Name\\nCPU\\nGPU(s) per Node\\nDRAM per Node\\nVRAM per GPU\\nMacbook Pro\\n10-core M1 Pro\\n14-core M1 Pro\\n32GB\\n-\\nSwing AMD+A100\\n2\\xc3\\x9764-core AMD EPYC 7742\\n8\\xc3\\x97NVIDIA A100\\n1TB\\n40GB\\nPalmetto Intel+V100\\n40-Core Intel Xeon 6148G\\n2\\xc3\\x97NVIDIA V100\\n376GB\\n16GB\\nTable 1: Our System Configurations\\nWe note that the M1-Pro results only include the Llama-2 (7B)\\nand Mistral (7B) results, as Falcon (7B) generally did not complete\\ntasks in less than two orders of magnitude greater runtime.\\n5.2\\nExperimental Strategy\\nTo comprehensively evaluate the performance of different system\\nconfigurations across various models, we conducted a series of\\ncontrolled experiments. We systematically varied the number of\\ninput and output tokens to measure their effects on runtime and\\nenergy consumption under two main experimental conditions. In\\neach experiment we do not allow for key-value caches to be re-used\\nto ensure our testing environment is standardized.\\n5.2.1\\nVary Input Tokens. For the first experimental condition, we\\nexecuted inference requests with increasing input token sizes, rang-\\ning from 8 to 2048 tokens, while maintaining a fixed output token\\nsize of 32. This setup allowed us to isolate the impact of input size\\non the system\\xe2\\x80\\x99s performance and energy efficiency.\\n5.2.2\\nVary Output Tokens. In the second set of experiments, we\\nvaried the output token limit from 8 to 4096 tokens, keeping the\\ninput token size constant at 32. This approach helped us understand\\nhow increasing output demands affect the runtime and energy\\nconsumption of the systems tested.\\n5.2.3\\nRandomization and Stopping Criteria. Each experiment was\\nconducted in a randomized order to mitigate any potential bias\\nintroduced by the sequence of tests. To ensure the reliability of our\\nresults, we adhered to strict criteria for statistical confidence. Each\\nconfiguration was tested repeatedly until either of two conditions\\nwas met: (1) The measured runtime had to be within 0.5 seconds of\\nthe actual mean runtime with 95% confidence. (2) A maximum of\\n25 trials were conducted for each setting if the first condition could\\nnot be met.\\n5.3\\nInput Token Analysis\\nHere, we present the impacts on runtime, energy consumption per\\ntoken, and throughput for LLMs across different hardware config-\\nurations while varying the number of input tokens. We perform\\nthese experiments using the suite of systems outlined in Table 1\\nwith the models outlined in Section 4.1. In our experiments on the\\nPalmetto Intel+V100 system, the V100 GPU had an out-of-memory\\nerror beyond 1024 output tokens for Falcon (7B).\\nOur runtime measurements show a significant increase as in-\\nput tokens grow. As depicted in Figure 1(a), all systems exhibit a\\nnonlinear escalation in runtime with increasing token counts, with\\nthe M1-Pro system showing the most significant magnitude. This\\ntrend highlights the computational burden imposed by larger input\\nsizes, particularly on smaller systems that are not as well designed\\nto handle extensive workloads.\\nFor all systems, we notice that throughput follows a \\xe2\\x80\\x9croofline\\nmodel\" with increasing input tokens [37]. Figure 1(b) illustrates\\nthese dynamics, indicating an increase in throughput for all systems\\nuntil a certain point where inference becomes bound by compute\\nand not by the overhead of the software, as described by roofline\\nperformance models [37].\\nEnergy efficiency varies markedly across different systems. The\\nM1-Pro demonstrates consistently low energy consumption per to-\\nken, particularly for smaller input sizes, as shown in Figure 1(c). This\\nefficiency reflects the M1-Pro\\xe2\\x80\\x99s design optimization for low-power\\noperations. In contrast, the Swing AMD+A100, while capable of\\nhandling more significant token inputs more efficiently, consumed\\nmore energy per token for small workloads yet became more en-\\nergy efficient at larger input token sizes, underscoring a trade-off\\nbetween workload size and energy efficiency.\\n5.4\\nOutput Token Analysis\\nHere we examine the performance trends associated with increasing\\nthe number of output tokens for our LLMs and systems of interest,\\nspecifically focusing on runtime, energy consumption per token,\\nand throughput. In our experiments, the M1-Pro also could not\\ngenerate more than 512 output tokens without significant runtime\\npenalties. For the Palmetto Intel+V100 system, the V100 GPU had\\nan OOM error beyond 1024 output tokens for Falcon (7B) and for\\nall models beyond 2048 tokens.\\nRuntime significantly increases with the number of output to-\\nkens across all systems. As illustrated in Figure 2(a), the escala-\\ntion in runtime is pronounced, particularly as the output token\\ncount reaches higher magnitudes. This increase is indicative of\\nthe substantial computational effort required by LLMs to generate\\nsuccessive tokens.\\nIn Figure 2(b), we observe a decrease in throughput across all\\nsystems as the number of output tokens increases. This trend high-\\nlights the inherent computational complexity involved in generat-\\ning larger sequences of tokens in LLM tasks. As the output token\\ncount grows, the system must process each additional token, re-\\ncalculating the context and updating internal model states [34].\\nThis not only increases the total computation per query but also\\nleads to a greater accumulation of processing time per token, which\\nconsequently lowers the overall throughput.\\nEnergy consumption per token also shows an increasing trend\\nas the number of output tokens grows. Displayed in Figure 2(c),\\nthis trend underscores the energy-intensive nature of producing\\nlarger outputs. Systems such as the M1-Pro, while generally more\\nenergy-efficient, begin to consume more energy per token as output\\ndemands increase, reflecting the intensive processing involved in\\noutput generation.\\n509\\n', b'Hybrid Heterogeneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads\\nE-Energy \\xe2\\x80\\x9924, June 04\\xe2\\x80\\x9307, 2024, Singapore, Singapore\\n2\\n3\\n2\\n4\\n2\\n5\\n2\\n6\\n2\\n7\\n2\\n8\\n2\\n9 2\\n10 2\\n11\\nNumber of Input Tokens\\n10\\n\\xe2\\x88\\x921\\n10\\n0\\n10\\n1\\n10\\n2\\n10\\n3\\nRuntime (s)\\n(a) Runtime\\n2\\n3\\n2\\n4\\n2\\n5\\n2\\n6\\n2\\n7\\n2\\n8\\n2\\n9 2\\n10 2\\n11\\nNumber of Input Tokens\\n10\\n0\\n10\\n1\\n10\\n2\\n10\\n3\\nThroughput (tokens/s)\\n(b) Throughput\\n2\\n3\\n2\\n4\\n2\\n5\\n2\\n6\\n2\\n7\\n2\\n8\\n2\\n9 2\\n10 2\\n11\\nNumber of Input Tokens\\n10\\n0\\n10\\n1\\n10\\n2\\nEnergy per Token (J/tokens)\\nSystem\\nSwing AMD+A100\\nPalmetto Intel+V100\\nM1-Pro\\nModel\\nFalcon (7B)\\nLlama-2 (7B)\\nMistral (7B)\\n(c) Energy per Token\\nFigure 1: Performance of Various Systems and Models for Processing Variable Input Tokens\\xe2\\x80\\x93Due to the low variance in the\\ndata, error bars are too small to be visible.\\n2\\n3 2\\n4 2\\n5 2\\n6 2\\n7 2\\n8 2\\n9 2\\n10 2\\n11 2\\n12\\nNumber of Output Tokens\\n10\\n\\xe2\\x88\\x921\\n10\\n0\\n10\\n1\\n10\\n2\\n10\\n3\\n10\\n4\\nRuntime (s)\\n(a) Runtime\\n2\\n3 2\\n4 2\\n5 2\\n6 2\\n7 2\\n8 2\\n9 2\\n10 2\\n11 2\\n12\\nNumber of Output Tokens\\n10\\n\\xe2\\x88\\x921\\n10\\n0\\n10\\n1\\n10\\n2\\n10\\n3\\nThroughput (tokens/s)\\n(b) Throughput\\n2\\n3 2\\n4 2\\n5 2\\n6 2\\n7 2\\n8 2\\n9 2\\n10 2\\n11 2\\n12\\nNumber of Output Tokens\\n10\\n\\xe2\\x88\\x921\\n10\\n0\\n10\\n1\\n10\\n2\\n10\\n3\\nEnergy per Token (J/tokens)\\nSystem\\nSwing AMD+A100\\nPalmetto Intel+V100\\nM1-Pro\\nModel\\nFalcon (7B)\\nLlama-2 (7B)\\nMistral (7B)\\n(c) Energy per Token\\nFigure 2: Performance of Various Systems and Models for Processing Variable Output Tokens\\xe2\\x80\\x93Missing data points in M1-Pro\\nand Palmetto Intel+V100 are due to CUDA out of memory errors. Due to the low variance in the data, error bars are too small\\nto be visible.\\n5.5\\nComparing the Input and Output Analyses\\nWhen comparing Figure 1(a) and Figure 2(a), we observe that in-\\ncreases in the number of output tokens result in a more considerable\\nincrease in runtime than increases in input tokens. The computa-\\ntional complexity of processing input tokens primarily involves\\nencoding the input context, which occurs once per input sequence\\nand follows a more linear computational trajectory. In contrast,\\ngenerating output tokens is inherently more complex and iterative.\\nEach new output token requires the model to run through all its\\nlayers to predict the next token based on an ever-expanding context,\\nwhich includes both the initial input and all previously generated\\ntokens [34]. This ongoing computation involves recalculating atten-\\ntion across an increasing number of tokens, updating hidden states,\\nand generating a probability distribution over the vocabulary for\\neach new token. Consequently, as the number of output tokens\\ngrows, the computational load increases significantly, leading to\\nmore significant runtime increases than processing input tokens.\\nThe impacts on runtime also translate to the throughput, de-\\npicted in Figure 1(b) and Figure 2(b). There is a noticeable decline\\nin throughput as output tokens increase, more so than input to-\\nkens. The decrease in throughput for output tokens is primarily\\ndue to the heightened computational requirements for generating\\nsubsequent tokens, where each token\\xe2\\x80\\x99s generation slows down as\\nthe sequence lengthens. Furthermore, the energy per token also\\nincreases as output tokens grow, as shown in our analysis. The\\nenergy required to generate each output token becomes significant\\ndue to longer passes through the transformer network. We contrast\\nthis with the energy consumption when processing input tokens,\\nwhich, despite increasing, does so at a less steep rate.\\n6\\nENERGY-OPTIMAL HYBRID DATACENTER\\nFOR LLM INFERENCE\\nConsidering the performance results we collect from LLM inference\\nacross multiple systems, we notice that there is an energy-optimal\\nway to construct a hybrid datacenter with a combination of M1 Pro\\xe2\\x80\\x99s\\nand A100s. The intuition behind this is that the energy expended\\nper token for the M1 Pro is lower than that of the A100 up to a\\ncertain point in the number of input and output tokens as seen in\\nFigures 1(c) and 2(c). However, the energy efficiency characteristics\\nare different when varying the number of input and output tokens,\\nand therefore, we will proceed with separate analyses.\\n6.1\\nNumber of Input Tokens Analysis\\nSuppose we have a hybrid data center with M1-Pros and A100s.\\nThen, we have some workload for an LLM, a set of queries with\\nsome outputs. In such a configuration, we implement a scheduling\\nheuristic based on a cutoff threshold, \\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b, for input token length.\\n510\\n', b'E-Energy \\xe2\\x80\\x9924, June 04\\xe2\\x80\\x9307, 2024, Singapore, Singapore\\nGrant Wilkins, Srinivasan Keshav, and Richard Mortier\\nThis heuristic dictates that queries with \\xf0\\x9d\\x91\\x9b\\xe2\\x89\\xa4\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9btokens are pro-\\ncessed on M1 Pro systems, which we have shown have good energy\\nefficiency with handling smaller computational loads. Conversely,\\nqueries with \\xf0\\x9d\\x91\\x9b> \\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9btokens leverage the greater computational abil-\\nity of A100 GPUs, which offer greater energy-per-token advantages\\nfor larger tasks despite their higher power usage. We point out that\\nthis is the same method mentioned in the problem formulation in\\nEqn. 1, where our queries \\xf0\\x9d\\x91\\x84are partitioned into \\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x91\\x801 and \\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x90\\xb4100\\nstrictly on input and output size.\\nTo find an optimal threshold \\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9bempirically, we analyze the to-\\nken distribution in prompts from the Alpaca [31] dataset, a bench-\\nmark dataset frequently used in model fine-tuning. This dataset\\ncomprises 52K prompts, offering a diverse range of lengths akin to\\na typical workload in systems like GPT-4 [24]. The distribution of\\ninput tokens, visualized in our analysis (see Fig. 3(a)), serves as a\\nproxy for understanding the variegated nature of LLM workloads.\\n0\\n20\\n40\\n60\\n80\\n100\\nNumber of Input Tokens\\n0\\n2000\\n4000\\n6000\\n8000\\nFrequency\\n(a) Input Tokens\\n0\\n200\\n400\\n600\\nNumber of Output Tokens\\n0\\n2000\\n4000\\n6000\\n8000\\nFrequency\\n(b) Output Tokens\\nFigure 3: Distribution of Token Counts for Alpaca [31]\\nThe energy component of our cost function, split over the token\\nthreshold, is as follows:\\n\\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x99,\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b=\\n\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\n\\xf0\\x9d\\x91\\x9a=1\\n\\xf0\\x9d\\x91\\x9a\\xf0\\x9d\\x91\\x93\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b(\\xf0\\x9d\\x91\\x9a)\\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x91\\x801,\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b(\\xf0\\x9d\\x91\\x9a) +\\n\\xf0\\x9d\\x91\\x80\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\n\\xf0\\x9d\\x91\\x9a=\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b+1\\n\\xf0\\x9d\\x91\\x9a\\xf0\\x9d\\x91\\x93\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b(\\xf0\\x9d\\x91\\x9a)\\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x90\\xb4100,\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b(\\xf0\\x9d\\x91\\x9a),\\nwhere \\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x99,\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9brepresents the total energy consumption for a given\\ndataset of input lengths \\xf0\\x9d\\x91\\x9awith corresponding frequencies \\xf0\\x9d\\x91\\x93\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b(\\xf0\\x9d\\x91\\x9a),\\nand \\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x91\\x801,\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b(\\xf0\\x9d\\x91\\x9a) and \\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x90\\xb4100,\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b(\\xf0\\x9d\\x91\\x9a) denote the mean energy per token\\nfor varying the input token size for the M1-Pro and A100 systems,\\nrespectively. Utilizing this model with our dataset enables the ap-\\nproximation of total energy consumption for various threshold\\nsettings, offering insights into the energy dynamics of hybrid dat-\\nacenter operation. In Figure 4, we show the energy and runtime\\nsimulation results of performing inference for the input token sizes\\nfrom the Alpaca dataset.\\nOur findings indicate that a threshold of 32 tokens strikes an\\noptimal balance, significantly reducing energy consumption by\\nrelegating the inference of shorter queries to the more energy-\\nefficient M1 Pro systems. This policy not only capitalizes on the\\ninherent energy efficiency of the M1 Pro for smaller tasks but\\nalso reserves the computational might of the A100 for queries that\\nnecessitate its robust capabilities. However, it\\xe2\\x80\\x99s important to note\\nthat this energy optimization comes at the cost of increased runtime.\\n6.2\\nNumber of Output Tokens Analysis\\nWe want to use the same scheduling heuristic and performance\\nmodel to determine a threshold \\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1for the number of output\\n2\\n3\\n2\\n4\\n2\\n5\\n2\\n6\\n2\\n7\\n2\\n8\\n2\\n9\\n2\\n10\\n2\\n11\\nThreshold\\n0.76\\n0.78\\n0.80\\nTotal Energy (kWh)\\nM1-Pro Only\\nSwing AMD+A100 Only\\nHybrid System\\n(a) Energy Consumption for Changing \\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\n2\\n3\\n2\\n4\\n2\\n5\\n2\\n6\\n2\\n7\\n2\\n8\\n2\\n9\\n2\\n10\\n2\\n11\\nThreshold\\n0.5\\n1.0\\nRuntime (s)\\n1e7\\nM1-Pro Only\\nSwing AMD+A100 Only\\nHybrid System\\n(b) Runtime for Changing \\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\nFigure 4: Performance of Hybrid Datacenter for Input\\nTokens Processing Alpaca\\xe2\\x80\\x93Dashed line shows the value for\\nusing only one kind of hardware for inference\\ntokens. Except this time, we have different frequencies \\xf0\\x9d\\x91\\x93\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1(\\xf0\\x9d\\x91\\x9b)\\nfor the \\xf0\\x9d\\x91\\x9boutput tokens and different mean energy per token for\\nvarying the output token size, \\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x91\\x801,\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1(\\xf0\\x9d\\x91\\x9b) and \\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x90\\xb4100,\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1(\\xf0\\x9d\\x91\\x9b). We\\nalso utilize the distribution of the number of output tokens in the\\nAlpaca dataset (see Fig. 3(b)). We revise our performance model as\\nfollows:\\n\\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x99,\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1=\\n\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\n\\xf0\\x9d\\x91\\x9b=1\\n\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x93\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1(\\xf0\\x9d\\x91\\x9b)\\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x91\\x801,\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1(\\xf0\\x9d\\x91\\x9b)\\n+\\n\\xf0\\x9d\\x91\\x81\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\n\\xf0\\x9d\\x91\\x9b=\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1+1\\n\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x93\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1(\\xf0\\x9d\\x91\\x9b)\\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x90\\xb4100,\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1(\\xf0\\x9d\\x91\\x9b).\\nAs the M1 Pro could only generate up to 512 tokens of a response,\\nwe only test\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1up until this point. In Figure 5, we show the energy\\nand runtime simulation results of performing inference for the input\\ntoken sizes from the Alpaca dataset.\\nFig. 5(b) and Fig. 2(c) assess the energy consumption and runtime\\nimplications of various threshold settings for output generation.\\nOur findings suggest that although higher thresholds may leverage\\nthe M1 Pro\\xe2\\x80\\x99s energy efficiency for smaller outputs, there is an opti-\\nmal point at 32 output tokens that minimizes energy consumption.\\n6.3\\nBalancing Energy Efficiency and Runtime\\nPerformance\\nOur analysis of both input and output token processing within a\\nhybrid, heterogeneous datacenter framework has led to the identifi-\\ncation that with certain thresholds at \\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x91\\x9d\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1= 32 and \\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x9d\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1= 32,\\n511\\n', b'Hybrid Heterogeneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads\\nE-Energy \\xe2\\x80\\x9924, June 04\\xe2\\x80\\x9307, 2024, Singapore, Singapore\\n2\\n3\\n2\\n4\\n2\\n5\\n2\\n6\\n2\\n7\\n2\\n8\\n2\\n9\\nThreshold\\n0.66\\n0.68\\n0.70\\nTotal Energy (kWh)\\nM1-Pro Only\\nSwing AMD+A100 Only\\nHybrid System\\n(a) Energy Consumption for Changing \\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1\\n2\\n3\\n2\\n4\\n2\\n5\\n2\\n6\\n2\\n7\\n2\\n8\\n2\\n9\\nThreshold\\n0.5\\n1.0\\n1.5\\nRuntime (s)\\n1e7\\nM1-Pro Only\\nSwing AMD+A100 Only\\nHybrid System\\n(b) Runtime for Changing \\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1\\nFigure 5: Performance of Hybrid Datacenter for Output\\nTokens Processing Alpaca \\xe2\\x80\\x93 Dashed line shows the value for\\nusing only one kind of hardware for inference\\nwe can strategically allocate tasks to M1 Pro systems or A100 GPUs\\nbased on token count, optimizing for energy efficiency.\\nShifting the token distribution leverages the M1 Pro\\xe2\\x80\\x99s superior\\nenergy efficiency for input and output tasks up to the threshold,\\nbeyond which we utilize the A100\\xe2\\x80\\x99s computational power. This\\npolicy saves energy as smaller-token tasks are handled by the more\\nefficient M1 Pro for outputs up to the threshold. However, this\\nenergy optimization comes at the expense of increased runtime,\\nwhich is particularly noticeable in output token generation where\\nthe M1 Pro, despite its efficiency, does not match the A100\\xe2\\x80\\x99s speed.\\nThe energy-runtime trade-off presents a favorable scenario for\\napplications that have low runtime sensitivity. For instance, batch\\nprocessing of LLM tasks, such as overnight data analyses or non-\\ntime-critical computations, can benefit significantly from this energy-\\nefficient configuration. Similarly, free or not directly monetized\\nservices, where the cost of computation impacts operational sus-\\ntainability, stand to gain from minimizing energy expenditures even\\nat the cost of longer processing times.\\nThis approach also opens discussions on Quality of Service (QoS)\\nfor LLMs, an area that still needs to be explored [1, 35]. Traditional\\nQoS metrics often prioritize speed and reliability, but energy effi-\\nciency may also become a critical QoS dimension for LLM applica-\\ntions, particularly in energy-constrained or cost-sensitive scenarios.\\n7\\nRELATED WORK\\n7.1\\nHybrid and Energy Efficient Heterogeneous\\nData Centers\\nRecent studies in optimizing data center architectures for deep learn-\\ning have highlighted the necessity of energy-efficient scheduling\\nand task allocation across diverse hardware. Gu et al. [10] explore\\nGPU clusters\\xe2\\x80\\x99 energy-efficient scheduling, revealing substantial im-\\nprovements in power utilization without considering diverse GPU\\ntypes for different task requirements. This work highlights a gap\\nin understanding how various GPU configurations could enhance\\nenergy efficiency further. Similarly, Patel et al. [25] demonstrate\\nthe benefits of hybrid computing environments, emphasizing FPGA\\nover GPU diversity. This focus leaves room to explore the specific\\nimpacts of different GPU classes in such settings.\\nIn the realm of LLMs, Zhao et al. [39] introduce strategies like\\nphase-aware partitioning and adaptive quantization in heteroge-\\nneous clusters but do not integrate energy considerations into their\\nanalysis, which is crucial for understanding the real-world appli-\\ncability of these models in power-sensitive environments. On the\\nother hand, Radovanovi\\xc4\\x87 et al. [28] and Chien et al. [7] discuss\\nbroader aspects of carbon-aware computing and reducing the car-\\nbon impact of AI inference, respectively. These works emphasize\\nthe importance of node/device-level energy metrics, often over-\\nlooked in typical LLM deployment strategies, thus underscoring\\nthe need for detailed energy consumption profiling across different\\nmodels and hardware types.\\n7.2\\nLLM Inference as a Service\\nFurther focusing on energy consumption, Hu et al. [14] analyze\\ndeep learning workloads in GPU datacenters, offering insights into\\nenergy conservation strategies through workload scheduling. This\\nresearch aligns with our objectives by confirming the critical role\\nof scheduling in reducing energy footprints. Anderson et al. [3]\\npropose carbon-aware datacenter software that could complement\\nphysical hardware adjustments by making energy and carbon met-\\nrics visible to application developers, encouraging more energy-\\nefficient coding practices.\\nAddressing service quality, Wang et al. [35] study the efficiency\\nand reliability of LLM serving, highlighting the challenges of main-\\ntaining high-quality service while managing computational loads\\neffectively. This perspective is pertinent as it underscores the trade-\\noff between performance and energy efficiency, which is central to\\nour study. Lastly, Desislavov et al. [8] provide a timely examination\\nof trends in AI inference energy consumption, arguing that while\\nperformance has increased dramatically, energy consumption has\\nnot escalated at the same pace, thanks to hardware optimizations\\nand algorithmic innovations. This outlook is necessary as it sug-\\ngests the potential for further optimizations in LLM inference tasks,\\nwhich are typically energy-intensive.\\n8\\nCONCLUSIONS AND FUTURE WORK\\nFuture work will explore minimizing the energy and runtime and\\nmaximizing the accuracy of serving differently-sized LLMs. Larger\\nmodels are generally more accurate but come at the expense of\\nrequiring more hardware accelerators and often greater runtime;\\ntherefore, exploring this trade-off is highly relevant. Also, we plan to\\nmake our solution for energy-optimal routing of incoming queries\\nan online decision-making heuristic to increase its efficacy. Simi-\\nlarly, we aim to extend our energy model to reflect carbon awareness\\nand water consumption to decrease the environmental impact of\\nLLM inference further.\\n512\\n', b'E-Energy \\xe2\\x80\\x9924, June 04\\xe2\\x80\\x9307, 2024, Singapore, Singapore\\nGrant Wilkins, Srinivasan Keshav, and Richard Mortier\\nBy carefully analyzing the energy and runtime of heterogeneous\\ncompute hardware to host LLMs, we show that a hybrid, hetero-\\ngeneous datacenter and a cost-based scheduling framework can\\nallocate LLM tasks to accelerators that are best suited to run them\\nin terms of energy efficiency and computational performance. This\\ndecision is based simply on the size of input and output tokens,\\nmaking the decision process easy to integrate into existing work-\\nloads.\\nACKNOWLEDGMENTS\\nWe gratefully acknowledge the computing resources provided on\\nSwing and Palmetto, both high-performance computing clusters\\noperated by the Laboratory Computing Resource Center at Argonne\\nNational Laboratory and Clemson University, respectively. During\\nthis work GW was supported by a Churchill Scholarship.\\nREFERENCES\\n[1] Megha Agarwal, Asfandyar Qureshi, Linden Li Nikhil Sardana, Julian Quevedo,\\nand Daya Khudia. 2023.\\nLLM Inference Performance Engineering: Best\\nPractices.\\nhttps://www.databricks.com/blog/llm-inference-performance-\\nengineering-best-practices\\n[2] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, et al. 2023. The\\nFalcon Series of Language Models: Towards Open Frontier Models. (2023).\\n[3] Thomas Anderson, Adam Belay, Mosharaf Chowdhury, Asaf Cidon, and Irene\\nZhang. 2023. Treehouse: A Case For Carbon-Aware Datacenter Software. SIGEN-\\nERGY Energy Inform. Rev. 3, 3 (oct 2023), 64\\xe2\\x80\\x9370. https://doi.org/10.1145/3630614.\\n3630626\\n[4] Rohan Anil, Andrew M. Dai, Orhan Firat, et al. 2023. PaLM 2 Technical Report.\\narXiv:2305.10403 [cs.CL]\\n[5] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, and et al. 2022. On the Oppor-\\ntunities and Risks of Foundation Models. arXiv:2108.07258 [cs.LG]\\n[6] Le Chen, Nesreen K. Ahmed, Akash Dutta, et al. 2024. The Landscape and\\nChallenges of HPC Research and LLMs. arXiv:2402.02018 [cs.LG]\\n[7] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and\\nRajini Wijayawardana. 2023. Reducing the Carbon Impact of Generative AI\\nInference (Today and in 2035). In Proceedings of the 2nd Workshop on Sustainable\\nComputer Systems (Boston, MA, USA) (HotCarbon \\xe2\\x80\\x9923). Association for Computing\\nMachinery, New York, NY, USA, Article 11, 7 pages.\\nhttps://doi.org/10.1145/\\n3604930.3605705\\n[8] Radosvet Desislavov, Fernando Mart\\xc3\\xadnez-Plumed, and Jos\\xc3\\xa9 Hern\\xc3\\xa1ndez-Orallo.\\n2023. Trends in AI inference energy consumption: Beyond the performance-vs-\\nparameter laws of deep learning. Sustainable Computing: Informatics and Systems\\n38 (2023), 100857. https://doi.org/10.1016/j.suscom.2023.100857\\n[9] Yiannis Georgiou, David Glesser, and Denis Trystram. 2015. Adaptive Resource\\nand Job Management for Limited Power Consumption. In Proceedings of the\\n2015 IEEE International Parallel and Distributed Processing Symposium Workshop\\n(IPDPSW \\xe2\\x80\\x9915). IEEE Computer Society, USA, 863\\xe2\\x80\\x93870. https://doi.org/10.1109/\\nIPDPSW.2015.118\\n[10] Diandian Gu, Xintong Xie, Gang Huang, Xin Jin, and Xuanzhe Liu. 2023. Energy-\\nEfficient GPU Clusters Scheduling for Deep Learning. arXiv:2304.06381 [cs.DC]\\n[11] Sylvain Gugger, Lysandre Debut, Thomas Wolf, et al. 2022. Accelerate: Training\\nand inference at scale made simple, efficient and adaptable. https://github.com/\\nhuggingface/accelerate.\\n[12] Yuxiong He and Sameh Elnikety. 2011. Position paper: embracing heterogeneity-\\nimproving energy efficiency for interactive services. In Proceedings of the 8th AAAI\\nConference on AI for Data Center Management and Cloud Computing (AAAIWS\\xe2\\x80\\x9911-\\n08). AAAI Press, New York, NY, 11\\xe2\\x80\\x9314.\\n[13] Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and\\nJoelle Pineau. 2020. Towards the Systematic Reporting of the Energy and Carbon\\nFootprints of Machine Learning. J. Mach. Learn. Res. 21, 1, Article 248 (jan 2020),\\n43 pages.\\n[14] Qinghao Hu, Peng Sun, Shengen Yan, Yonggang Wen, and Tianwei Zhang.\\n2021. Characterization and prediction of deep learning workloads in large-\\nscale GPU datacenters. In Proceedings of the International Conference for High\\nPerformance Computing, Networking, Storage and Analysis (<conf-loc>, <city>St.\\nLouis</city>, <state>Missouri</state>, </conf-loc>) (SC \\xe2\\x80\\x9921). Association for\\nComputing Machinery, New York, NY, USA, Article 104, 15 pages.\\nhttps:\\n//doi.org/10.1145/3458817.3476223\\n[15] Xiaoxuan Hu, Peng Li, and Yanfei Sun. 2021. Minimizing energy cost for green\\ndata center by exploring heterogeneous energy resource. Journal of Modern\\nPower Systems and Clean Energy 9, 1 (2021), 148\\xe2\\x80\\x93159.\\n[16] Mehboob Hussain, Lian-Fu Wei, Abdullah Lakhan, Samad Wali, Soragga Ali,\\nand Abid Hussain. 2021. Energy and performance-efficient task scheduling in\\nheterogeneous virtualized cloud computing. Sustainable Computing: Informatics\\nand Systems 30 (2021), 100517.\\n[17] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, and et al. 2023. Mistral\\n7B. arXiv:2310.06825 [cs.CL]\\n[18] Willis Lang, Jignesh M. Patel, and Srinath Shankar. 2010. Wimpy node clusters:\\nwhat about non-wimpy workloads?. In Proceedings of the Sixth International\\nWorkshop on Data Management on New Hardware (Indianapolis, Indiana) (DaMoN\\n\\xe2\\x80\\x9910). Association for Computing Machinery, New York, NY, USA, 47\\xe2\\x80\\x9355. https:\\n//doi.org/10.1145/1869389.1869396\\n[19] Wenyu Liu, Yuejun Yan, Yimeng Sun, Hongju Mao, Ming Cheng, Peng Wang,\\nand Zhaohao Ding. 2023. Online job scheduling scheme for low-carbon data\\ncenter operation: An information and energy nexus perspective. Applied Energy\\n338 (2023), 120918.\\n[20] David Lo, Liqun Cheng, Rama Govindaraju, et al. 2015.\\nHeracles: Improv-\\ning resource efficiency at scale. In 2015 ACM/IEEE 42nd Annual International\\nSymposium on Computer Architecture (ISCA). ACM, New York, NY, 450\\xe2\\x80\\x93462.\\nhttps://doi.org/10.1145/2749469.2749475\\n[21] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Es-\\ntimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model.\\narXiv:2211.02001 [cs.LG]\\n[22] David Mytton and Masa\\xc2\\xafo Ashtine. 2022. Sources of data center energy estimates:\\nA comprehensive review. Joule 6, 9 (2022), 2032\\xe2\\x80\\x932056. https://doi.org/10.1016/j.\\njoule.2022.07.011\\n[23] NVIDIA. Accessed 2024. NVIDIA-NVML. https://docs.nvidia.com/deploy/nvml-\\napi/index.html. Available online.\\n[24] OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, et al. 2023. GPT-4\\nTechnical Report. arXiv:2303.08774 [cs.CL]\\n[25] Pratyush Patel, Katie Lim, Kushal Jhunjhunwalla, Ashlie Martinez, Max Demoulin,\\nJacob Nelson, Irene Zhang, and Thomas Anderson. 2023. Hybrid Computing for\\nInteractive Datacenter Applications. arXiv:2304.04488 [cs.DC]\\n[26] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,\\nDaniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon Emissions\\nand Large Neural Network Training. arXiv:2104.10350 [cs.LG]\\n[27] powerapi ng. 2024. PyJoules: Python-based energy measurement library for\\nvarious domains including NVIDIA GPUs. https://github.com/powerapi-ng/\\npyJoules. Accessed: 2024-01-10.\\n[28] Ana Radovanovi\\xc4\\x87, Ross Koningstein, Ian Schneider, Bokan Chen, Alexandre\\nDuarte, Binz Roy, Diyue Xiao, Maya Haridasan, Patrick Hung, Nick Care, et al.\\n2022. Carbon-aware computing for datacenters. IEEE Transactions on Power\\nSystems 38, 2 (2022), 1270\\xe2\\x80\\x931280.\\n[29] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas,\\nMichael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay\\nGadepally. 2023. From Words to Watts: Benchmarking the Energy Costs of Large\\nLanguage Model Inference. arXiv:2310.03003 [cs.CL]\\n[30] Matej \\xc5\\xa0pe\\xc5\\xa5ko, Ond\\xc5\\x99ej Vysock\\xc3\\xbd, Branislav Jans\\xc3\\xadk, and Lubom\\xc3\\xadr \\xc5\\x98\\xc3\\xadha. 2021. DGX-\\nA100 Face to Face DGX-2\\xe2\\x80\\x94Performance, Power and Thermal Behavior Evaluation.\\nEnergies 14, 2 (2021). https://doi.org/10.3390/en14020376\\n[31] R. Taori, I. Gulrajani, T. Zhang, and et al. 2024. Stanford alpaca: An instruction\\nfollowing llama model. https://github.com/tatsu-lab/stanford_alpaca. Accessed:\\n2024-01-15.\\n[32] Google Gemini Team. 2024. Gemini: A Family of Highly Capable Multimodal\\nModels. arXiv:2312.11805 [cs.CL]\\n[33] Hugo Touvron, Louis Martin, Kevin Stone, and et al. 2023.\\nLlama 2: Open\\nFoundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]\\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. 2017. Attention is all you\\nneed. In Proceedings of the 31st International Conference on Neural Information\\nProcessing Systems (Long Beach, California, USA) (NIPS\\xe2\\x80\\x9917). Curran Associates\\nInc., Red Hook, NY, USA, 6000\\xe2\\x80\\x936010.\\n[35] Yuxin Wang, Yuhan Chen, Zeyu Li, Zhenheng Tang, Rui Guo, Xin Wang, Qiang\\nWang, Amelie Chi Zhou, and Xiaowen Chu. 2024. Towards Efficient and Reliable\\nLLM Serving: A Real-World Workload Study. arXiv:2401.17644 [cs.DC]\\n[36] Vincent M. Weaver, Matt Johnson, Kiran Kasichayanula, James Ralph, Piotr\\nLuszczek, Dan Terpstra, and Shirley Moore. 2012. Measuring Energy and Power\\nwith PAPI. In 2012 41st International Conference on Parallel Processing Workshops.\\n262\\xe2\\x80\\x93268. https://doi.org/10.1109/ICPPW.2012.39\\n[37] Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: an\\ninsightful visual performance model for multicore architectures. Commun. ACM\\n52, 4 (apr 2009), 65\\xe2\\x80\\x9376. https://doi.org/10.1145/1498765.1498785\\n[38] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, and et al. 2022. Sustainable\\nai: Environmental implications, challenges and opportunities. Proceedings of\\nMachine Learning and Systems 4 (2022), 795\\xe2\\x80\\x93813.\\n[39] Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. LLM-\\nPQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and\\nAdaptive Quantization. arXiv preprint arXiv:2403.01136 (2024).\\n513\\n']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Text from paper5.pdf:\n",
            "[b'DynamoLLM: Designing LLM Inference Clusters\\nfor Performance and Energy Efficiency\\nJovan Stojkovic, Chaojie Zhang\\xe2\\x80\\xa0, \\xc2\\xb4I\\xcb\\x9cnigo Goiri\\xe2\\x80\\xa0, Josep Torrellas, Esha Choukse\\xe2\\x80\\xa0\\nUniversity of Illinois at Urbana-Champaign\\n\\xe2\\x80\\xa0Microsoft Azure Research - Systems\\nAbstract\\xe2\\x80\\x94The rapid evolution and widespread adoption of\\ngenerative large language models (LLMs) have made them a\\npivotal workload in various applications. Today, LLM inference\\nclusters receive a large number of queries with strict Service\\nLevel Objectives (SLOs). To achieve the desired performance,\\nthese models execute on power-hungry GPUs causing the in-\\nference clusters to consume large amount of energy and, conse-\\nquently, result in excessive carbon emissions. Fortunately, we find\\nthat there is a great opportunity to exploit the heterogeneity in\\ninference compute properties and fluctuations in inference work-\\nloads, to significantly improve energy-efficiency. However, such\\na diverse and dynamic environment creates a large search-space\\nwhere different system configurations (e.g., number of instances,\\nmodel parallelism, and GPU frequency) translate into different\\nenergy-performance trade-offs. To address these challenges, we\\npropose DynamoLLM, the first energy-management framework\\nfor LLM inference environments. DynamoLLM automatically\\nand dynamically reconfigures the inference cluster to optimize for\\nenergy and cost of LLM serving under the service\\xe2\\x80\\x99s performance\\nSLOs. We show that at a service-level, DynamoLLM conserves\\n53% energy and 38% operational carbon emissions, and reduces\\n61% cost to the customer, while meeting the latency SLOs.\\nI. INTRODUCTION\\nThe exponential growth in the adoption of generative large\\nlanguage models (LLMs) has positioned them at the core\\nof numerous technological advancements and applications.\\nToday, we see use-cases of LLMs in various domains, such\\nas healthcare [52], developer productivity [13], data analyt-\\nics [68], education [5] and other. As the popularity of LLMs\\nincreases among users, the inference clusters receive millions\\nof queries per day [27] resulting in large infrastructures with\\nsophisticated software and expensive hardware systems.\\nTo meet these ever increasing computing demands, re-\\nsearchers proposed various software [9], [26], [35], [73], [81]\\nand hardware [4], [50], [78] techniques. Such techniques\\nimprove the performance efficiency of LLM inference clusters.\\nHowever, one aspect that has been largely overlooked is the\\nenergy consumption of these environments [58], [60]. The\\nsubstantial energy requirements of serving LLMs running on\\npower-hungry GPUs have emerged as a significant concern. As\\nthese models become integral to various services, minimizing\\ntheir energy consumption and, consequently, carbon emissions\\nwhile maintaining high performance is paramount.\\nTo address this gap, this paper starts by characterizing the\\nenergy-efficiency properties of LLM inference workloads. Our\\ncharacterization underscores that such environments present\\na distinct set of challenges, divergent from existing energy\\nmanagement schemes tailored for traditional datacenters appli-\\ncations [7], [17], [21], [31], [61], [80]. Specifically, we observe\\nthat heterogeneity in LLM inference compute properties and\\nfluctuations in LLM inference workloads create a dynamic en-\\nvironment with large variations. Such variations arise from: (1)\\nrequests with varying input/output token lengths, (2) distinct\\ncompute properties of different LLMs, and (3) different SLOs\\nrequired by the services using an LLM.\\nRequests with a large number of input tokens are compute\\nintensive, thus, sensitive to GPU frequency. Conversely, re-\\nquests with a few input tokens and many output tokens have\\nlow compute, but high memory requirements. Reducing their\\nGPU frequency would save the energy without significantly\\nimpacting the performance. Moreover, the number of model\\nparameters also affects the LLM\\xe2\\x80\\x99s sensitivity to the number of\\nGPUs and GPU frequency. Finally, depending on the service\\ncurrently using the LLM, the SLO requirements can be strict\\nrequiring high-performance configurations, or loose allowing\\nfor lower-performance but more energy-efficient configura-\\ntions. Importantly, these characteristics rapidly change due to\\nload fluctuations and dynamic distributions of requests. Such\\ndynamic changes cause a system configuration that is energy-\\nefficient at a given point, to quickly become sub-optimal. This\\nrequires a dynamic approach to resource management.\\nTo pave the way towards energy-efficient and sustainable\\nLLM inference clusters, this paper introduces DynamoLLM,\\nthe first energy-management framework for LLM inference\\nenvironments. DynamoLLM exploits the unique properties of\\nLLM inference workloads to reduce their energy consump-\\ntion while meeting the performance SLOs. The system uses\\nenergy-performance profiles of models and their workloads\\nto automatically and dynamically select the energy-efficient\\nconfiguration. It leverages multiple knobs, including scaling\\nin/out the number of server instances, model parallelism across\\nGPUs, and GPU frequency scaling.\\nTo handle workload heterogeneity, DynamoLLM maintains\\ndifferently configured pools of LLM instances that are op-\\ntimal for different types of incoming requests. For instance,\\ncompared to a request with many input and output tokens,\\na request that processes and outputs fewer tokens runs more\\nefficiently on a model parallelized across fewer GPUs running\\nat a lower frequency. As request distribution varies over\\ntime, DynamoLLM dynamically sizes the pools. These pools\\ncan be merged into fewer pools or divided into multiple\\npools over time, providing a balance between right-sizing\\nand fragmentation of resources. To efficiently manage the\\n1\\narXiv:2408.00741v1  [cs.AI]  1 Aug 2024\\n', b'resources, DynamoLLM uses a hierarchy of controllers that\\nreduces computation complexity and eliminates centralized\\nbottlenecks. The controller at each level operates under the\\nconditions imposed by the upper level, computes its dedicated\\nknob, and forwards further constraints to the controllers at a\\nlower level. Finally, to enable frequent and smooth transition\\nacross different configurations, DynamoLLM includes tech-\\nniques to minimize or hide the reconfiguration overheads. As\\na result, the system maintains high levels of efficiency and\\nservice quality under changing workload demands.\\nAn evaluation of DynamoLLM with a large GPU cluster\\nrunning production-level traces from a major cloud provider\\nshows that DynamoLLM is very effective: it conserves 53%\\nenergy, 38% operational carbon emissions, and reduces 61%\\ncost to the customer, while meeting the latency SLOs.\\nThe contributions of this paper are as follows:\\n\\xe2\\x80\\xa2 An analysis of the opportunities for energy-efficient LLM\\nserving, rooting in the heterogeneity and fluctuations within\\nthe inference workloads.\\n\\xe2\\x80\\xa2 Design and implementation of DynamoLLM, a high perfor-\\nmance and energy-optimized framework for LLM inference.\\n\\xe2\\x80\\xa2 An evaluation of DynamoLLM on a large-scale platform\\nusing production-level traces.\\nII. BACKGROUND\\nComputational phases of LLMs Generative LLMs [34], [38],\\n[56], [67], [80] are auto-regressive: they process the whole\\ninput in parallel, and serially generate the output tokens. This\\nproperty leads to two computationally distinct phases [49],\\n[50]. First is the prefill phase, where the input tokens are\\ncomputed in parallel. This is a compute-intensive phase and\\nscales with the number of input tokens. Second is the decode\\nphase, where each output token is generated serially, based on\\nall the tokens seen so far. This is a memory-intensive phase,\\nand scales with the number of output tokens.\\nPerformance metrics for LLMs To evaluate the performance,\\nwe use: time to first token (TTFT), time between tokens\\n(TBT), and throughput\\n[50], [63]. TTFT is the latency of\\ngenerating the first output token; while TBT is the latency\\nto generate each new output token. To quantify the energy\\nefficiency, we measure the energy consumption in Watt-hours\\n(Wh) while meeting certain latency SLOs. The SLOs vary\\ndepending on their use cases for different tasks. For latency-\\nsensitive tasks, both TTFT and TBT are important metrics\\nwith strict SLOs. We define SLOs for TTFT and TBT based\\non maximum achievable performance, described in Table IV.\\nLLM parallelism A single model can be divided across GPUs\\nto improve performance and allow larger memory footprints.\\nLLM inference typically uses pipeline and tensor parallelism.\\nPipeline parallelism (PP) partitions the LLM layers among\\nGPUs, while keeping all the operators/tensors of a layer on\\nthe GPU. GPUs then communicate only in between two\\nconsecutive stages. Tensor parallelism (TP) allocates a slice\\nof each layer to each GPU. This requires aggregation across\\nall the GPU for each layer, in turn needing high bandwidth\\ncommunication. TP performs better for GPUs within the same\\nserver, connected with high bandwidth interconnects (e.g.,\\nNVLink [45]), while PP is preferred across servers. Since most\\nopen source models [34], [38], [67] fit on 8 GPUs in a single\\nserver, we consider only TP in the rest of the paper; the ideas\\ncan easily extend to PP. We denote tensor parallelism across\\n2, 4 and 8 GPUs as TP2, TP4 and TP8, respectively.\\nPower and energy in datacenters A rich body of work\\nexplored power/energy efficiency in traditional datacenters [7],\\n[29], [31], [62]. However, the rapid growth of LLMs has posed\\nnew challenges that have not yet been extensively studied.\\nLLM inference workloads comprise a swiftly increasing per-\\ncentage of datacenter load [49]. This, coupled with the power-\\ndense hardware like DGX A100s and H100s being deployed\\nto serve these workloads makes them power, energy, and\\ncarbon-intensive [12], [49], [58]. To effectively address this\\nchallenge, it is important to have a comprehensive framework\\nfor managing energy in these systems.\\nIII. OPPORTUNITIES FOR ENERGY EFFICIENCY\\nTo understand the energy-efficiency properties of LLM\\ninference environments, we characterize open-source mod-\\nels [33], [38], [39], [66] on an NVIDIA DGX H100 server [44]\\nusing vLLM [26] inference engine. We analyze the energy\\nproperties of LLMs by varying the request lengths, request\\nload, model, and service SLO. Additionally, we analyze how\\nthe profiled variables change over time in a real-production\\nenvironment using the invocation traces of two LLM services\\nfrom Azure: Coding and Conversation. The traces include a\\nsubset of invocations received by the profiled services during\\none week, and contain the timestamp of the invocation, along\\nwith the number of input and output tokens. These traces are a\\nsuper-set of our open-source traces for the same services [50].\\nA. Heterogeneous Energy-Performance Profiles\\nRequest lengths The prefill and decode phases in an LLM\\ninference exhibit distinct execution behaviors (Section II),\\nsuggesting that requests of different input and output lengths\\npossess different compute and energy characteristics. We cate-\\ngorize the requests based on the number of input/output tokens\\ninto 9 buckets: SS (short input, short output), SM (short input,\\nmedium output), SL (short input, long output), MS, MM,\\nML, LS, LM, and LL. Table IV shows the thresholds and\\ncorresponding TTFT/TBT SLOs. We set the thresholds for\\nrequest lengths using the 33rd, 66th and 100th percentiles of\\nthe input/output lengths from a trace for a Conversation service\\nfrom Azure. We set the SLOs to 5\\xc3\\x97 the latency of a single\\nrequest running isolated on a system [30].\\nWe use these categories to characterize the energy consump-\\ntion of different request types running the Llama2-70B [33]\\nmodel with a medium system load of 2000 tokens per second\\n(TPS) under various GPU frequencies and model parallelisms.\\nTable I shows our results in the form of a heat map. Since\\nshorter requests are not computationally intensive, they meet\\ntheir SLOs with any tensor parallelism, and generally at lower\\n2\\n', b'Tensor Parallelism\\nTP2\\nTP4\\nTP8\\nGPU Frequency (GHz)\\n0.8\\n1.2\\n1.6\\n2.0\\n0.8\\n1.2\\n1.6\\n2.0\\n0.8\\n1.2\\n1.6\\n2.0\\nInput\\nOutput\\nShort\\nShort\\n0.77\\n0.97\\n1.03\\n0.94\\n0.79\\n0.91\\n1.01\\n1.35\\n1.19\\n1.29\\n1.49\\nShort\\nMedium\\n2.78\\n3.45\\n3.68\\n3.39\\n2.82\\n3.37\\n3.81\\n4.55\\n4.15\\n4.43\\n4.74\\nShort\\nLong\\n4.84\\n4.17\\n4.97\\n5.52\\n6.37\\n5.62\\n5.59\\n6.95\\nMedium\\nShort\\n1.02\\n1.09\\n1.08\\n1.07\\n1.20\\n1.51\\n1.29\\n1.34\\n1.73\\nMedium\\nMedium\\n4.23\\n3.91\\n4.08\\n5.34\\n4.39\\n4.56\\n5.44\\nMedium\\nLong\\n4.99\\n4.66\\n4.53\\n6.86\\n5.79\\n6.52\\n7.12\\nLong\\nShort\\n1.51\\n1.64\\n1.76\\n2.55\\n2.53\\n2.83\\n2.94\\nLong\\nMedium\\n7.71\\n8.81\\n9.17\\nLong\\nLong\\n12.99\\n11.89\\n13.21\\nTABLE I: Energy consumption in Watt\\xc3\\x97hours (Wh) for Llama2-70B varying request lengths, frequency, and model parallelism\\nwith medium system load (2K tokens per second). Configurations that violate the SLO are shown as empty gray boxes, while\\nthe acceptable configurations are colored as a heat map according to their energy consumption, per row.\\nTensor Parallelism\\nTP2\\nTP4\\nTP8\\nGPU Frequency (GHz)\\n0.8\\n1.2\\n1.6\\n2.0\\n0.8\\n1.2\\n1.6\\n2.0\\n0.8\\n1.2\\n1.6\\n2.0\\nLow Load\\n3.41\\n3.75\\n3.44\\n2.93\\n3.71\\n3.73\\n4.49\\n3.76\\n4.52\\n4.64\\nMedium Load\\n4.23\\n3.91\\n4.08\\n5.34\\n4.39\\n4.56\\n5.44\\nHigh Load\\n4.22\\n4.13\\n5.86\\n5.24\\n5.42\\n6.62\\nTABLE II: Energy consumption in Wh for LLama2-70B medium-sized input and output (MM) requests varying frequency and\\nmodel parallelism under different system loads: low (650 TPS), medium (2K TPS) and high (4K TPS).\\nTensor Parallelism\\nTP2\\nTP4\\nTP8\\nGPU Frequency (GHz)\\n0.8\\n1.2\\n1.6\\n2.0\\n0.8\\n1.2\\n1.6\\n2.0\\n0.8\\n1.2\\n1.6\\n2.0\\nLlama2-13B [32]\\n1.05\\n0.99\\n1.14\\n1.24\\n1.52\\n1.27\\n1.58\\n1.65\\n2.61\\n2.35\\n2.74\\n3.45\\nMixtral-8x7B [39]\\n1.03\\n0.98\\n1.21\\n1.32\\n1.39\\n1.51\\n2.09\\n2.31\\n2.57\\n3.06\\n3.71\\n4.66\\nLlama2-70B [33]\\n4.23\\n3.91\\n4.08\\n5.34\\n4.39\\n4.56\\n5.44\\nLlama3-70B [34]\\n4.32\\n4.28\\n4.57\\n6.11\\n5.18\\n5.42\\n6.45\\nMixtral-8x22B [38]\\n3.83\\n3.23\\n3.65\\n4.03\\nFalcon-180B [66]\\n9.56\\n7.94\\n8.57\\n10.34\\nTABLE III: Energy consumption in Wh for medium-sized (MM) requests of different LLM architectures varying the frequency\\nand model parallelism with medium system load (2K TPS).\\nInput\\nOutput\\nTTFT SLO\\nTBT SLO\\nShort\\nS\\n<256\\n<100\\n250 ms\\n100 ms\\nMedium\\nM\\n<1024\\n<350\\n400 ms\\n100 ms\\nLong\\nL\\n\\xe2\\x89\\xa48192\\n\\xe2\\x89\\xa5350\\n2000 ms\\n100 ms\\nTABLE IV: Thresholds for classifying the requests based on\\ninput/output lengths and corresponding TTFT/TBT SLOs.\\nfrequencies compared to the rest. As an example, the least-\\nenergy configuration for SS requests is TP2 at 1.2 GHz. Con-\\nversely, LL requests can only run with TP8 without violating\\nthe SLO. With TP8, the least-energy configuration for LL\\nrequests is 1.6 GHz. Note that the lowest power configuration\\nthat meets SLOs (TP8 at 1.2 GHz), is not the energy-optimal\\none due to the increased execution time. Running all the\\nrequests together would require the system to run with the\\nmost constrained SLO configuration, in this case, as per the LL\\nconfiguration. This would make the system energy inefficient.\\nTo exploit this heterogeneity for energy-efficiency, the sys-\\ntem would need to separate requests based on their input/out-\\nput lengths, and process different request types with different\\nserver configurations. However, on request arrival, the input\\nlength is known, but, due to the auto-regressive LLM nature,\\nthe output length is unknown. Thus, the system needs to\\npredict the output length. DynamoLLM will rely on prior\\nwork that efficiently performs such operation with relatively\\nhigh precision [19], [55], [79], and will have a mechanism to\\nmitigate the impact of occasional mis-predictions.\\nRequest loads In addition to the request length, the incoming\\nload of the LLM inference server drives the compute require-\\nments. During periods of low load, the system has a larger\\nSLO slack to exploit and can run the requests at low-frequency\\nconfigurations to save energy. Conversely, during periods of\\nhigh load, the system does not have enough SLO slack, and\\nneeds to run at high-frequency configurations.\\nTable II shows the energy consumed when running Llama2-\\n70B medium-sized input and output (MM) requests while\\nvarying the number of processed prompt tokens per second\\n(TPS). The system can run low load with any TP at almost\\nany frequency. Among all feasible configurations, the lowest-\\nenergy configuration is TP4 with 1.2 GHz. TP8 requires more\\nGPUs to operate in parallel and, thus, consumes more energy.\\nTP2 uses fewer GPUs but increases the execution time and\\nforces individual GPUs to operate at high frequency to meet\\nSLOs, leading to high energy. Conversely, under high load,\\n3\\n', b'Mon\\nTue\\nWed\\nThu\\nFri\\nSat\\nSun\\nCoding\\n0\\n25\\n50\\n75\\n100\\nDistribution [%]\\nSS\\nSM\\nSL\\nMS\\nMM\\nML\\nLS\\nLM\\nLL\\nMon\\nTue\\nWed\\nThu\\nFri\\nSat\\nSun\\nConversation\\n0\\n25\\n50\\n75\\n100\\nFig. 1: Distribution of requests based on input and output\\nlengths categorized into three groups: short, medium, and long.\\nthe system cannot operate on TP2 and requires TP4 or TP8.\\nThe lowest energy configuration is TP4 with 2 GHz. Overall,\\nto minimize the energy consumption while operating under\\nperformance constraints, we need to consider the incoming\\nload to set the correct parallelism and GPU frequency.\\nRequested model The diversity of the compute properties\\nof an LLM directly translates into its energy profile. Ta-\\nble III shows the energy consumption of different LLMs\\nwhen running medium-sized requests at medium system load.\\nSmaller models, such as Llama2-13B and Mixtral-8x7B, can\\nrun with any TP (even with a single GPU); their lowest-energy\\nconfiguration is TP2 at 1.2 GHz. Mixtral-8x22B and Falcon-\\n180B are much larger and can only run with TP8. Their lowest-\\nenergy configuration is TP8 at 1.2 GHz.\\nCompute-bound models with large number of parameters\\nare more sensitive to the GPU frequency and model paral-\\nlelism. Hence, they often need to operate at high-frequency\\nand high-energy modes. Sparse models with relatively smaller\\nnumbers of parameters tolerate lower frequencies and lower\\nmodel parallelism. Hence, they meet the performance require-\\nments even with lower-performance modes.\\nService SLO Different services often use the same model\\nwith different SLO requirements [57]. As indicated before,\\nwe assume an SLO such that the P99 tail latency is within 5\\xc3\\x97\\nof the execution time of a request on an unloaded system [30].\\nHowever, some services have more relaxed SLOs, at 10\\xc3\\x97 or\\neven 20\\xc3\\x97 of a single request execution [10], [37]. For different\\nSLO requirements, the system may need different energy-\\noptimal configurations. For example, Table I shows that, with\\nstrict SLO (5\\xc3\\x97), short-input long-output sized LLama2-70B\\nrequests at medium load have the optimal configuration at TP4\\nand 1.2GHz. However, if with loose SLO (10\\xc3\\x97), the requests\\nmay even operate with TP2 at 1.6GHz.\\nInsight #1 LLM workloads are highly heterogeneous in their\\nenergy-performance profiles. To achieve the optimal energy\\nunder performance SLOs, different requests (sizes, models and\\nSLOs) need to be processed separately and differently.\\nB. Dynamic LLM Inference Workloads\\nChanging request-length distribution We measure the distri-\\nbution of request types for Coding and Conversation services.\\nFigure 1 shows the distribution of requests for each workload\\nover a week. The distribution differs across services. Conver-\\nsation has typically longer outputs and shorter inputs, while\\nMon\\nTue\\nWed\\nThu\\nFri\\nSat\\nSun\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nNormalized TPS\\nCoding\\nConversation\\nFig. 2: Load over a week for Coding and Conversation LLM\\ninference workloads.\\nCoding shows the opposite trend. However, both services have\\na significant fraction of each request type, and importantly, the\\npopularity of request types changes over time.\\nAs observed earlier, different request types require different\\nenergy-optimal configurations. Thus, the system needs to split\\nits resources into per request-type pools, configure pools\\nindividually, and dynamically adapt the pools\\xe2\\x80\\x99 configurations\\nbased on the current request distribution. However, if the\\nsystem classifies the requests into too few classes, it will not be\\nable to fine-tune the system for best energy. On the other hand,\\ntoo many classes may lead to fragmentation and negatively\\nimpact energy efficiency. Thus, the system has to find the\\nright number of resource pools. In DynamoLLM, we will use\\nhistorical data to set the number of pools such that requests\\nwith distinct SLO requirements (TTFT or TBT bound) and\\ncompute properties (compute or memory bound) have separate\\npools. Moreover, as the load of a given request type reduces,\\nDynamoLLM will avoid fragmentation by merging the pool\\nwith the next available pool that serves longer requests.\\nRequest load fluctuations LLM inference workloads, as user-\\nfacing applications, exhibit a typical diurnal pattern with peaks\\nduring working hours and valleys at night and weekends.\\nFigure 2 shows the load in tokens per second of the two\\nworkloads over a week. The load is normalized to the peak\\nof the individual workloads. The Coding trace shows a clear\\ndiurnal pattern, with peaks every day, lower load at night, and\\nmuch lower load during weekends. Conversation shows a less\\nextreme, but still significant, diurnal pattern.\\nThe peak load of Conversation is 1.7\\xc3\\x97 and 3.3\\xc3\\x97 higher than\\nits average and valley loads, respectively. The peak load of\\nCoding is 2.8\\xc3\\x97 and 34.6\\xc3\\x97 higher than its average and valley\\nloads, respectively. This large slack indicates that the LLM\\ninference servers can frequently operate in a less performant\\nbut energy-optimized configuration without violating the SLO.\\nOnce the load starts building up, the server needs to switch to\\na more performant mode of operation.\\nLLM service SLO and model diversity Finally, different\\nservices may time-share the same LLM model instance [14].\\nThey may have different SLOs, requiring the configuration\\nto be adapted based on the current service-user. On the other\\nhand, the same service may concurrently use multiple different\\nmodels [11]. This requires different execution plans for the\\noptimal energy consumption of the individual queries. Thus,\\nit is not trivial for service providers to operate in an energy-\\noptimal setting while meeting the performance SLOs.\\n4\\n', b'Overhead source\\nTime\\nCreate a new H100 VM [36]\\n\\xe2\\x88\\xbc1-2 min\\nInitialize distributed multi-GPU environment\\n\\xe2\\x88\\xbc2 min\\nDownload model weights (Llama2-70B [67])\\n\\xe2\\x88\\xbc3 min\\nSet up the engine configuration\\n\\xe2\\x88\\xbc18 sec\\nInstall weights and KV cache on GPUs\\n\\xe2\\x88\\xbc15 sec\\nTotal\\n\\xe2\\x88\\xbc6-8 min\\nTABLE V: Measured overheads of creating a new 8\\xc3\\x97H100\\ninstance of an LLM inference server VM.\\nInsight #2 LLM workloads are highly dynamic and, thus, an\\nenergy-optimal configuration can quickly become sub-optimal.\\nHowever, the complexity of a large search space requires an\\nautomatic and user-transparent configuration selection.\\nC. Reconfiguration Overheads\\nTo capture the fast changes in LLM inference workloads, we\\nneed to quickly transition between configurations. However,\\nthere are overheads to change (1) number of inference server\\ninstances, (2) model parallelism, and (3) GPU frequency.\\nChanging instance number To adjust to fluctuating load,\\nit is cost-beneficial to dynamically adjust the number of\\nLLM instances to serve the requests (i.e., scale in and out).\\nHowever, the overheads of adding a new inference server are\\ntoo large to be tolerable on the critical path of inference\\nloads. Table V shows the breakdown of the overheads to:\\n(1) instantiate a new GPU VM in the cloud (such as H100\\nVM [36]), (2) initialize the distributed multi-GPU environment\\n(e.g., Ray, MPI), (3) download the model weights, (4) setup\\nthe inference engine, and (5) install the weights and key-\\nvalue cache on the GPUs. In total, these overheads can take\\neven a few minutes. Hence, the conventional LLM inference\\nenvironments typically provision the static number of instances\\nto handle their peak load resulting in heavy underutilization.\\nIn DynamoLLM, we will propose techniques to efficiently\\nscale the number of instances (with the current load) while\\nminimizing most of the scale-out overheads.\\nChanging model parallelism To modify the model paral-\\nlelism of an LLM inference server, we need to perform two\\noperations. First, we need to re-shard the model weights and\\ntransfer them to the memory of the right GPUs. Second, the\\ninference engine needs to synchronize the involved GPUs.\\nCurrent systems stop the engine, unload the weights from\\nGPUs, load the weights from the host to the new set of GPUs,\\nand re-start the engine from the scratch. This adds intolerable\\noverheads (around 1-2 minutes) if performed on the critical\\npath. In DynamoLLM, we will show how to minimize the re-\\nsharding overheads by smartly mapping the logical to physical\\nGPUs, exploiting inter-GPU direct NVLink connections and\\nmoving the weights between GPUs in the background.\\nChanging GPU frequency Setting the GPU frequency (e.g.,\\nvia nvidia-smi [46]) incurs non-negligible overheads. It\\ninvolves invoking the OS, communicating with the GPU\\ndriver via system calls, and performing hardware interactions\\nvia firmware. On average, setting the GPU frequency takes\\nSS\\nSM\\nSL\\nMS\\nMM\\nML\\nLS\\nLM\\nLL\\nAvg\\n0\\n2\\n4\\n6\\n8\\n10\\nT-put [RPS]\\nConstFreq\\nSwitchFreq\\nFig. 3: Throughput for different request types with constant\\nfrequency (1980MHz) and with re-setting the frequency (to\\n1980MHz) on every iteration in the background.\\naround 50-80ms. In comparison, one decode iteration of the\\nLLM inference process takes 20-30ms. Consequently, the time\\nspent adjusting the GPU frequency can significantly impact\\nthe overall performance, potentially doubling the latency of\\nindividual inference steps. Figure 3 shows the throughput for\\ndifferent request types when constantly running at the highest\\nfrequency (1980 MHz) and when re-setting the frequency (to\\n1980 MHz) in the background on every LLM inference itera-\\ntion. Due to the software overheads, the throughput of LLM\\ninference system significantly drops. Therefore, optimizing\\nor minimizing frequency changes during LLM inference is\\ncrucial for maintaining efficient and responsive performance.\\nInsight #3 Transitioning between LLM server configurations\\nincurs significant overheads. For energy-efficiency, such over-\\nheads need to be minimized and considered when computing\\nthe energy/performance trade-offs.\\nIV. DYNAMOLLM: AN ENERGY MANAGEMENT\\nFRAMEWORK FOR LLM INFERENCE CLUSTERS\\nWe use the insights to design DynamoLLM, the first energy\\nmanagement framework for LLM inference environments. Dy-\\nnamoLLM seamlessly integrates with existing inference plat-\\nforms, enabling LLM workloads to operate energy-efficiently\\nand cost-effectively while meeting their performance SLOs.\\nDynamoLLM has four key principles. First, it is energy-\\noptimized and SLO-aware, leveraging model profiles to au-\\ntomatically select the most energy-efficient configuration for\\nspecific LLMs and inference workloads within their SLO\\nrequirements. Second, DynamoLLM fine-tunes configurations\\nfor heterogeneous LLM workloads by dividing cluster re-\\nsources into instance pools tailored to specific request types.\\nThird, DynamoLLM accommodates fluctuating LLM infer-\\nence loads by dynamically reconfiguring the chosen organiza-\\ntion. Finally, to ensure frequent and smooth reconfiguration,\\nDynamoLLM minimizes reconfiguration overheads.\\nArchitecture Figure 4 shows the DynamoLLM architecture.\\nThe system is organized hierarchically at the cluster, pool,\\nand instance levels. At each level, the controllers tune their\\nassigned configuration knob, and communicate their decisions\\nwith the controllers from the upper and lower levels. The\\ncontrollers use energy-performance models generated in the\\nprofiling phase to determine the number of instances, model\\nparallelization, and GPU frequency for an energy-optimized\\noperation given the current system state. (1) Cluster Manager\\nreceives inference requests, predicts their type, and forwards\\nthem to the appropriate instance pool. Additionally, it peri-\\n5\\n', b'Instance \\nManager\\nModel \\nWeights\\nCluster \\nStorage\\nInference \\nRequests\\nSelect GPU\\nFrequency\\nPerf-Energy \\nProfile\\nInstance \\nManager\\nInstance \\nManager\\nInstance Pool (SS)\\nModel \\nProfile\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nInstance Pool (MM)\\nInstance Pool (LL)\\nSelect Instance \\nCount\\nOutput-Length\\nPrediction\\nThroughput \\nProfile\\nLoad\\nPrediction\\nCluster Manager\\nPerf-Energy \\nProfile\\nPool Manager\\nSelect Model \\nParallelization\\nPerf-Energy \\nProfile\\nPool Manager\\nPerf-Energy \\nProfile\\nPool Manager\\nInstance Manager\\nFig. 4: DynamoLLM architecture: a hierarchy of controllers with cluster resources split into per request-type pools.\\nodically re-evaluates how many pools and how many model\\ninstances per pool are needed based on the system load. (2)\\nPool Manager schedules the requests to model instances in a\\nmanner that minimizes per-pool energy consumption. It also\\nperiodically checks if its instances need to be re-sharded into\\na more energy-efficient parallelization. (3) Instance Manager\\nschedules the requests to the inference engine and periodically\\nchecks if the instance\\xe2\\x80\\x99s GPU frequency needs to be adjusted.\\nA. Configuring Instances for Energy-Efficiency\\nGenerating LLM profiles When deploying their service to\\nDynamoLLM, users specify the LLM used by the service and\\nthe expected performance SLOs. Then, the system character-\\nizes the model and generates its energy-performance profile.\\nDynamoLLM profiles the model by running loads of different\\nrequest lengths at different model parallelisms (TP2, TP4 and\\nTP8) and GPU frequencies (800\\xe2\\x80\\x931980MHz, with a step of\\n200MHz). The system profiles a few load levels, up to the\\nmaximum throughput, and then extrapolates the behavior for\\nthe loads in between the measured ones. The profiling result is\\na function that takes the load, request length, model parallelism\\nand GPU frequency as inputs and outputs the expected energy\\nconsumption and TTFT/TBT latencies.\\nAs many services may use the same model, DynamoLLM\\ncan reuse the profiles across services, minimizing the profiling\\noverheads. Such profiles are stored in a global DynamoLLM\\nrepository, and then cached in a cluster-local storage when a\\ngiven service is deployed in the cluster.\\nSelecting the energy-optimized configuration Given the\\ncurrent load and available resources, DynamoLLM uses the\\ngenerated profiles to minimize energy consumption while\\nstaying within performance constraints. The system formulates\\nthis task as an optimization problem for the mixed integer\\nlinear programming (MILP) solver. The solver needs to output\\nhow many instances of each tensor parallelism (N T P2, N T P4\\nand N T P8) are needed, at which frequency they should run\\n(f T P2, f T P4 and f T P8), and which load should be assigned\\nto each instance (LT P2, LT P4 and LT P8). We assume that all\\ninstances of a given parallelism run at the same frequency and\\nreceive fair-share amount of work.\\nThe optimization target of the solver is to minimize the\\ntotal energy consumption (E), while the constraints are: 1)\\nthe total number of GPUs used by all instance types does\\nnot exceed the assigned number of GPUs (N); 2) the load\\nassigned to individual instances sums up to the total expected\\nload (L); and 3) the expected performance of all instances with\\nthe assigned load is within the requirements (SLO). Functions\\nEnergyT Pi,fi(LT Pi) and PerformanceT Pi,fi(LT Pi) output\\nthe expected energy and performance, respectively, when run-\\nning the load LT Pi with TPi parallelism at fi GPU frequency.\\nThen, the optimization task can be formulated as:\\nmin\\n X\\ni\\n(NT Pi \\xc3\\x97 EnergyT Pi,fi(LT Pi))\\n!\\n\\xe2\\x88\\x80i \\xe2\\x88\\x88{2, 4, 8}\\ns.t.\\nX\\ni\\ni \\xc3\\x97 NT Pi \\xe2\\x89\\xa4N\\nX\\ni\\n(NT Pi \\xc3\\x97 LT Pi) \\xe2\\x89\\xa5L\\nPerformanceT Pi,fi(LT Pi) \\xe2\\x89\\xa4SLO\\n(1)\\nThis approach guarantees the energy optimal configuration.\\nHowever, it introduces non-negligible overheads (i.e., \\xe2\\x88\\xbc100s\\nof ms) due to the large search-space for the solver. Hence, it\\ncannot be used to select the correct system configuration at\\nfine-grain intervals (e.g., every few seconds). Next we show\\nhow to break the task into a hierarchy of subtasks and use an\\napproximation heuristic to reduce the computation complexity.\\nB. Hierarchical Control for Dynamic Load\\nDynamoLLM simplifies computations by assigning spe-\\ncific optimization tasks to individual controllers. Instead of\\nsearching for a globally optimal configuration, controllers\\nset locally optimal values for individual knobs under the\\nconstraints imposed by the upper-level controllers and under\\nthe assumption that the lower-level controllers operate at the\\nhighest performance configuration. This allows the controllers\\nto operate at varying time scales\\xe2\\x80\\x93from minutes for node ad-\\njustments down to seconds for frequency tuning. The different\\nscales are needed as each operational change involves distinct\\noverheads and energy-efficiency impacts.\\n6\\n', b'Scale-out/in At every epoch (e.g., 30 minutes), the cluster\\nmanager computes the minimal number of nodes per pool that\\ncan support the load of a given request type. The manager uses\\na load predictor to forecast the incoming load, PL, for each\\nrequest type based on its historic data (e.g., via lightweight\\nload templates [62]). Moreover, the manager assumes that all\\ninstances will run at the highest-performance configuration\\n(i.e., TP8 at 1980 MHz). Then, if the predicted peak of a\\ngiven request type is PL, and the maximum load that a node\\ncan support for this request type is ML, the manager assigns\\n\\x06 P L\\nML\\n\\x07\\nnodes to that pool. By consolidating the work into a\\nsmall number of nodes, the system tries to minimize the cost\\nfor the user and the idle energy of lightly-loaded GPUs.\\nHandling fragmentation: Allocating resources to handle the\\npeak loads can cause resource underutilization. If overprovi-\\nsioning accumulates across pools, the energy efficiency drops.\\nTo prevent such cases, DynamoLLM assigns one instance less\\nto a given instance pool and moves the leftover load to the\\ninstance pool of the next larger request type. The cluster\\nmanager uses this information to forward a fraction of the\\nload for a given request type to the appropriate larger instance\\npool during the next scheduling epoch (e.g., 30 minutes). In\\nthis way, only the instance pool for the largest requests can be\\noverprovisioned minimizing the cluster-level fragmentation.\\nShard-up/down At every epoch (e.g., every 5 minutes), the\\npool manager decides how to split the assigned N GPUs from\\nthe cluster manager into correct model parallelism (how many\\ninstances to create in the pool) and tensor parallelism (how\\nmany GPUs to use for each instance). The pool manager uses\\na simplified version of Equation (1) assuming that all instances\\nrun at the highest GPU frequency (i.e., 1980 MHz). Thus,\\nthe goal is to minimize the energy, while operating with the\\nfixed number of GPUs running at the highest frequency, and\\ncontrolling only the parallelism knob.\\nAccounting for the overheads: DynamoLLM stores the tran-\\nsitioning overheads (scale-out/in, shard-up/down) in an Over-\\nhead Table. This table is integrated with the controllers, so that\\nwhen they calculate the energy benefits of new configurations,\\nthey can take into account the costs of reconfiguration. The\\ncontrollers evaluate whether the energy savings gained from\\nre-configuring justify the associated overheads and downtime.\\nReducing downtime: The reconfiguration cannot occur si-\\nmultaneously on all instances due to the risk of long downtime.\\nInstead, DynamoLLM employs a staggered reconfiguration\\napproach, where a subset of instances is reconfigured (e.g.,\\nre-sharded) at a time. This ensures that while some instances\\nare undergoing reconfiguration, others remain operational to\\nhandle ongoing workloads. The system first reconfigures the\\ninstances that have higher potential energy savings.\\nScale-up/down Finally, at every epoch (e.g., 5s), the instance\\nmanager fine tunes the GPU frequency for further energy\\nsavings given the assigned model parallelism. The instance\\nmanager uses the performance profile to first filter-out fre-\\nquencies that violate the SLO under the current load. Then,\\nit uses the energy profile to pick an acceptable frequency that\\nminimizes the energy consumption.\\nGPU0\\nGPU1\\nGPU2\\nGPU3\\nGPU4\\nGPU5\\nGPU6\\nGPU7\\nTP2\\nTP4\\nW0\\nW1\\nW4\\nW5\\nW2\\nW3\\nW6\\nW7\\nW0\\nW1\\nW2\\nW3\\nW4\\nW5\\nW6\\nW7\\nTP8\\nW0\\nW4\\nW2\\nW6\\nW1\\nW5\\nW3\\nW7\\nFig. 5: Example of re-sharding a TP4 model to TP2/TP8.\\nC. Reduced Overheads for Smooth Reconfiguration\\nTo enable frequent reconfiguration, DynamoLLM proposes\\na set of techniques to minimize the overheads of (1) scaling-\\nin/out the number of LLM inference servers, (2) sharding-\\nup/down the parallelism of a given instance, and (3) scaling-\\nup/down the GPU frequency of a given instance.\\nScaling in/out inference servers DynamoLLM reduces the\\noverheads of creating a new server instance by implementing\\nseveral strategies. First, it keeps the model weights cached\\nlocally within the cluster (shown in Figure 4) avoiding the need\\nto fetch them from a global repository. Second, it starts VMs\\nfrom a snapshot with the entire state of the inference engine\\nalready initialized, reducing the boot-up time. This snapshot\\nincludes pre-loaded libraries, GPU drivers and inference en-\\ngine configurations. Third, it proactively creates new VMs in\\nthe background, outside of the critical path of active workload\\nhandling. Specifically, DynamoLLM predicts the peak load for\\nthe next scheduling epoch and starts the extra VMs before the\\nepoch starts. By having these VMs ready to go, DynamoLLM\\ncan seamlessly offload a fraction of the load to new instances\\nwithout any noticeable latency impact.\\nSharding up/down an instance To reduce the re-sharding\\noverheads, DynamoLLM optimizes the distribution of weights\\nacross GPUs. We propose two techniques to minimize the\\ndata transfers and latency of individual transfers. First, the\\nsystem develops a graph matching algorithm that maximizes\\nthe amount of weights that remain stationary in their current\\nGPUs. The algorithm takes current weight distribution and\\ndesired tensor parallelism as inputs, and outputs the source\\nand destination GPUs and fraction of weights to be transferred\\nbetween each source-destination pair. Specifically, the algo-\\nrithm constructs a bipartite graph where nodes represent GPUs\\nin the current and next configurations. Edges between nodes\\nrepresent potential transfers, weighted by the amount of data\\nto be transferred. Then, it applies a maximum weight matching\\nalgorithm to find the optimal transfer plan that minimizes the\\ntotal weight of the edges (i.e., minimizes the amount of data\\ntransferred). Second, to reduce the transfer latency, the system\\nuses inter-GPU direct transfers via NVLink, allowing them\\nto send fractions of their weights in parallel to other GPUs\\nwithout any host intervention.\\n7\\n', b'Figure 5 shows an example of re-sharding from TP4 to\\nTP2 and TP8. Consider first the case when going from a\\nlower to a higher-level parallelism (TP4\\xe2\\x86\\x92TP8). In the initial\\nstate (TP4), GPUs 0-3 hold a quarter of the model weights\\neach. In the final state (TP8), all GPUs need to hold an\\neight of the model weight. Thus, the first four GPUs already\\nhave their required state, and they only need to send half of\\ntheir weights to the remaining four GPUs. The four transfers\\n(e.g., GPU0\\xe2\\x86\\x92GPU4, GPU1\\xe2\\x86\\x92GPU5,...) happen in parallel.\\nTherefore, the re-sharding requires the time to send 1/8 of the\\nmodel weights via NVLink (around 50ms in our setup).\\nConsider now the case when going from a higher to a lower\\nparallelism (TP4\\xe2\\x86\\x92TP2). In the final state (TP2), two GPUs\\nneed to hold a half of the weights each. As each GPU initially\\nholds a quarter of the weights, we merge the weights from\\ntwo GPUs to a single one: GPU1 sends weights W2/W3 to\\nGPU0, and GPU3 sends weights W6/W7 to GPU2. As these\\ntwo transfers happen in parallel, the total re-sharding time is\\nthe time to send 1/4 of the model weights (around 100ms\\nin our setup). Table VI shows the re-sharding overheads with\\ndifferent source/destination pairs with our optimized approach.\\nSome configurations quickly switch to other configurations\\n(transition time 0), other changes incur larger overheads (tran-\\nsition time 4T, where T is the time to send 1/8 of the weights).\\nMoreover, on some transitions, the instance continues serv-\\ning the requests with the same throughput. This is the case\\nwhen scaling from a smaller to a larger tensor parallelism\\n(e.g., TP4\\xe2\\x86\\x92TP8). The old instance sends a fraction of the\\nweights to the new instance, without increasing its memory\\nfootprint. During other transitions, the current instance needs\\nto operate under lower throughput. This is the case when the\\ninstance scales from a larger to a smaller tensor parallelism\\n(e.g., TP8\\xe2\\x86\\x92TP4). Some GPUs used by the old instance accept\\nextra weights, reducing their memory capacity to serve new\\nrequests. In general, whenever the GPU memory required to\\nhold model weights increases, the throughput decreases due\\nto the lower memory capacity for the incoming requests.\\nFinally, after the weights are sent to the correct memories,\\nthe inference engine needs to synchronize the GPUs that run\\nthe new instance. State-of-the-art engines, such as vLLM [26],\\nperform this operation in a few 100s of milliseconds to a few\\nseconds. During this period the instance cannot receive any\\nload, causing noticeable downtime. To reduce the downtime,\\nDynamoLLM allows the old instance to process the requests\\nwhile the new instance is going through the synchronization\\nprocess. Only when the new instance comes online, the old\\ninstance is removed. This is possible only when the sum of\\nthe memory from the old and new instances is below the\\nGPU\\xe2\\x80\\x99s memory capacity. When the sum exceeds the memory\\ncapacity (e.g., TP2 and TP4 with 70B parameters), the old\\ninstance needs to be shutdown before the new instance is\\ncreated. Overall, different transitions incur different overheads\\nand instance downtime requiring a fraction of the load to\\nbe shifted to another instance. DynamoLLM minimizes the\\noverheads, and considers their impact on the overall efficiency\\nwhen deciding whether to re-configure an instance.\\nSrc/Dst\\nTP2\\n4TP2\\nTP4\\nTP2+TP4\\n2TP4\\nTP8\\nTP2\\n0\\n4T\\n2T\\n2T\\n2T\\nT\\n4TP2\\n0\\n0\\n0\\n0\\n0\\n0\\nTP4\\n2T\\n2T\\n0\\n2T\\n2T\\nT\\nTP2+TP4\\n0\\n2T\\n0\\n0\\nT\\nT\\n2TP4\\nT\\nT\\n0\\nT\\n0\\n0\\nTP8\\nT\\nT\\nT\\nT\\nT\\n0\\nTABLE VI: Overhead of transferring model weights on a re-\\nsharding. T is the time unit to move 1/8 of the model (e.g., with\\n300GB/s NVLink bandwidth on NVIDIA DGX H100 [44] and\\nLlama2-70B model [67], T = 50ms).\\nScaling up/down the frequency The overheads of changing\\nthe GPU frequency are minimized by keeping the NVIDIA\\nSystem Management Interface (nvidia-smi) monitor pro-\\ngram directly loaded into memory. This eliminates the need\\nto reload the program every time a frequency adjustment is\\nrequired. Moreover, by running the controller in privileged\\nmode, DynamoLLM avoids the overhead associated with OS-\\nuser interactions, allowing for rapid frequency adjustments.\\nD. Predictive Scheduling for Request Heterogeneity\\nTo map the heterogeneity of requests to the heterogeneous\\ninstance pools, the cluster controller in DynamoLLM uses an\\noutput-length predictor to anticipate the request type and steer\\nrequests to the correct instance pool. The predictor acts as a\\nproxy model that takes input prompt and classifies the output\\nas short, medium or long. Based on the predicted output length\\nand known input length, the cluster manager forwards the\\nrequest to the pool manager being in charge for a given request\\ntype. If the instance pool is currently overloaded, the cluster\\nmanager forwards the request to the next available pool for\\na larger request type. Once the request arrives to the correct\\npool, the pool manager needs to pick an instance from the\\npool. Specifically, the manager uses the generated models from\\nthe profiling step to predict energy and response times of each\\ninstance after potentially adding a new request to that instance.\\nThen, it chooses the instance that minimizes total energy while\\nstaying within per-instance throughput determined by the SLO.\\nHandling mis-predictions If the system over-estimates a\\nrequest length, the request gets routed to a higher-performance\\npool. Hence, it runs with sub-optimal energy, but its latency\\nremains unaffected. Conversely, if a request length is under-\\nestimated, the request is placed to a lower-performance pool,\\npotentially missing its SLOs. Similarly, load mis-predictions\\ncan result in insufficient resources for a given pool during\\nrequest bursts. In both cases, due to some mis-predictions, the\\nsystem needs to react to the created emergency event.\\nWhen an instance manager detects that its queue is building\\nup, indicating that the rate of request processing is lower than\\nthe rate of request arrival, it triggers an emergency event.\\nFirst, the instance manager tries to re-order the requests in its\\nqueue and prioritizes those requests that are about to miss their\\ndeadline. Second, if some requests will miss their deadlines\\neven after the reordering, the instance manager ramps up the\\nfrequency of its GPUs to the maximum value. Third, if the\\n8\\n', b'0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\nEnergy [kWh]\\nDynamoLLM\\nScaleFreq\\nScaleShard\\nScaleInst\\nMultiPool\\nSinglePool\\nSS\\nSM\\nSL\\nMS\\nMM\\nML\\nLS\\nLM\\nLL\\nFig. 6: Energy consumption with the six evaluated systems\\nrunning open-source Llama2-70B model [67] with 1-hour\\nopen-source production traces [50].\\nbacklog persists or worsens, the instance manager re-steers\\nsome requests that have not started their execution. A subset\\nof requests is moved to another instance within the pool via the\\npool manager. Finally, if all the attempts are insufficient, the\\ninstance manager resorts to more drastic measures. One such\\nmeasure involves squashing requests that have been waiting for\\nprocessing beyond a specified threshold. This action signals\\nusers to retry their requests, allowing the frontend system to\\nredirect these requests to alternative instance pools or retry\\nthem later when system load has stabilized.\\nE. DynamoLLM Implementation\\nWe build DynamoLLM on top of vLLM [26], a state-\\nof-the-art LLM inference platform. However, DynamoLLM\\xe2\\x80\\x99s\\nmodularity allows it to be integrated with other platforms, e.g.,\\nTensorRT-LLM [47], without modifications. We implement\\ncontrollers as lightweight gRPC servers with low memory and\\ncompute requirements. Cluster and pool managers are hosted\\nin a dedicated VM for robust management. Instance managers\\nare co-located with the LLM inference engine instances for\\nlow communication overheads. For output length prediction,\\nwe leverage a BERT-based proxy model [55], which provides\\naccurate and efficient classification of incoming requests. For\\nload prediction, we use a template-based approach that uses\\nhistorical data to model load patterns over a week [62]. The\\npool manager employs Python\\xe2\\x80\\x99s PuLP library [53] for solving\\nMILP. DynamoLLM models energy and performance using\\nthe interp1d function from the SciPy [54] Python library.\\nV. EVALUATION\\nA. Evaluation Setup\\nWe run our experiments on servers with 8 H100 GPUs [44].\\nWe show the results for Llama2-70B [67], but other models\\n(i.e., Mixtral [38], Falcon [66], BLOOM [59]) follow the\\nsame trends. We set the load using production-level traces:\\n1 hour open-source traces [50] and 1-day and 1-week traces\\nfor Coding and Conversation from our fleet. We compare Dy-\\nnamoLLM to five systems. SinglePool (a state-of-the-practice\\nbaseline) schedules all the requests to the common pool of\\ninstances running with TP8 at the highest GPU frequency.\\nMultiPool separates LLM instances in multiple per-request-\\ntype pools. ScaleInst, ScaleShard, and ScaleFreq additionally\\nP50\\nP90\\nP99\\n0.0\\n0.2\\n0.4\\n0.6\\nTTFT [ms]\\nP50\\nP90\\nP99\\n0\\n20\\n40\\n60\\nTBT [ms]\\nSinglePool\\nMultiPool\\nScaleInst\\nScaleShard\\nScaleFreq\\nDynamoLLM\\nFig. 7: Summary of the latencies for each of the systems\\nrunning open-source Llama2-70B model [67] with 1-hour\\nopen-source production traces [50].\\nP50\\nP90\\nP99\\n0\\n20\\n40\\n60\\nCluster Power [kW]\\nP50\\nP90\\nP99\\n0\\n200\\n400\\n600\\n800\\nGPU Power [W]\\nSinglePool\\nMultiPool\\nScaleInst\\nScaleShard\\nScaleFreq\\nDynamoLLM\\nFig. 8: Summary of the power for each of the systems running\\nopen-source Llama2-70B model [67] with 1-hour open-source\\nproduction traces [50].\\nscale the number of instances in the pool, model parallelism,\\nor GPU frequency according to the current load, respectively.\\nB. Cluster-Level Experiments\\nWe first evaluate the system on a cluster of GPU servers\\nusing the 1h open source production traces for the Conver-\\nsation service [50]. We provision the baselines with 12 H100\\nservers to handle the peak load, while DynamoLLM scales the\\nnumber of servers according to the current load.\\nEnergy Figure 6 shows the energy consumption of the\\ncluster for the experiment. MultiPool increases the energy\\nconsumption by 20% over SinglePool, because it allocates\\na larger number of resources while always operating at\\nthe highest-performance configuration. Meanwhile, ScaleInst,\\nScaleShard, ScaleFreq and DynamoLLM reduce the energy\\nconsumption by 4.1%, 7%, 19% and 35%, respectively. Scale-\\nInst/Shard/Freq reduce the energy by configuring one knob\\nbut leave substantial space for further savings. Finally, Dy-\\nnamoLLM synchronously scales multiple knobs to achieve the\\nlowest energy consumption. We further breakdown the total\\nenergy per request type. Figure 6 shows that longer requests\\n(e.g., LL) and highly-popular requests (e.g., ML) consume dis-\\nproportionally more energy than the other types.\\nLatency Figure 7 shows the TTFT/TBT latencies for each sys-\\ntem. By separating request types into different resource pools,\\nMultiPool removes the head-of-line blocking effect and re-\\nduces the latencies over SinglePool. Similarly, ScaleShard and\\nScaleFreq, and DynamoLLM reduce the tail latency. However,\\nthese systems slightly increase the P50 latency by operating in\\nlower-performance modes when there is available SLO slack.\\nOn the other hand, ScaleInst increases the tail latency due\\nto the large overheads of creating a new inference server on\\nthe critical path of users\\xe2\\x80\\x99 load. Overall, DynamoLLM reduces\\n9\\n', b'0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\nTime [min]\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nAvg Freq. [GHz]\\nTotal\\nSL\\nLL\\nFig. 9: GPU Frequency over one hour for DynamoLLM\\nrunning open-source Llama2-70B model [67] with 1-hour\\nopen-source production traces [50].\\n0\\n10\\n20\\n30\\n40\\n50\\n0\\n20\\n40\\n60\\n80\\n100\\nNum. GPUs\\nTotal\\nTP2\\nTP4\\nTP8\\n0\\n10\\n20\\n30\\n40\\n50\\n0\\n2\\n4\\n6\\n8\\nSL Pool\\n0\\n10\\n20\\n30\\n40\\n50\\nTime [min]\\n0\\n10\\n20\\n30\\nNum. GPUs\\nML Pool\\n0\\n10\\n20\\n30\\n40\\n50\\nTime [min]\\n0\\n10\\n20\\n30\\n40\\nLL Pool\\n35\\n45\\n55\\n65\\n75\\n85\\n0\\n1\\n2\\n3\\n4\\nkTPS\\n0\\n4\\n8\\n12\\n0\\n10\\n20\\n30\\n40\\nkTPS\\nFig. 10: Number of GPUs used for each sharding configuration\\n(TP2, TP4 or TP8) over time running open-source Llama2-70B\\nmodel [67] with 1-hour open-source production traces [50].\\nthe P99 TTFT and TBT latencies by 5.3% and 11.1% over\\nSinglePool, respectively, while it increases the P50 TTFT and\\nTBT latencies by 11.4% and 7.6%, respectively.\\nPower Figure 8 shows the power consumption across the\\nsystems for the cluster (right figure) and on average per-\\nGPU (left figure). Due to operating in energy-efficient modes,\\nDynamoLLM effectively reduces both cluster and per-GPU\\npower. DynamoLLM reduces the P50 and P99 power con-\\nsumption over the baseline by 43% and 9%, respectively.\\nFrequency changes Figure 9 shows the average GPU fre-\\nquency over time for (1) the whole cluster, (2) the pool\\nserving short requests, and (3) the pool serving long requests.\\nAverage frequency is always significantly lower than the\\nmaximum allowed frequency (1980 MHz) that is used by\\nthe baseline. DynamoLLM effectively accommodates different\\nrequest types by operating their pools at different frequencies.\\nSharding changes Figure 10 shows the number of GPUs\\nused for different model parallelisms (TP2, TP4 and TP8)\\nfor the whole cluster and for the individual pools (SL, ML\\nand LL). The figure also shows the load over time that a\\ngiven pool experiences. Different pools operate with different\\nmodel parallelisms and DynamoLLM efficiently changes the\\nparallelism as the load changes.\\nC. Sensitivity Studies\\nSensitivity to predictor accuracy We analyze how the ac-\\ncuracy of the prediction models affects the overall system\\nEnergy\\n0\\n10\\n20\\n30\\n40\\nEnergy [kWh]\\nPerformance\\n0.0\\n0.2\\n0.4\\n0.6\\nTTFT [s]\\nSinglePool\\nDyn-100%\\nDyn-90%\\nDyn-80%\\nDyn-60%\\nDyn-50%\\nFig. 11: Energy and performance with different accuracy\\nrunning open-source Llama2-70B model [67] with 1-hour\\nopen-source production traces [50].\\nLow Load\\nMedium Load\\nHigh Load\\n0\\n5\\n10\\n15\\n20\\n25\\nEnergy [kWh]\\nSinglePool\\nMultiPool\\nScaleInst\\nScaleShard\\nScaleFreq\\nDynamoLLM\\nFig. 12: Energy with different levels of load running open-\\nsource Llama2-70B model [67].\\nefficiency. We introduce bounded errors for the output length\\nmisclassification and measure the energy consumption with\\nmedium load. Figure 11 shows that the impact of the predictor\\naccuracy is modest for both energy and performance. Com-\\npared to an environment with no error, an environment with\\nan 40% error increases the energy consumption by 13% and\\nTTFT by 7.3%. The reason for robustness to prediction errors\\nis that DynamoLLM can promptly detect mis-predictions and\\nre-configures the knobs accordingly.\\nSensitivity to load We evaluate DynamoLLM with different\\nsystem loads. We generate Low, Medium, and High loads with\\na Poisson distribution for request inter-arrival times. Figure 12\\nshows the energy consumption of the five evaluated systems\\nwith different load levels. With Low, Medium, and High load,\\nDynamoLLM reduces the energy of SinglePool baseline by\\n51%, 40%, and 23.4%, respectively. As the load increases, the\\nenergy savings of DynamoLLM reduce, because the system\\nmore frequently needs to operate at higher frequencies with\\nhigher levels of model parallelism.\\nSensitivity to number of pools Figure 13 shows the energy\\nconsumption and performance (TTFT) of DynamoLLM with\\ndifferent number of request pools. Recall that our chosen\\ndesign has 9 pools. By adding too many pools (12 or 16),\\nthe system gets fragmented, and the idle energy of GPUs\\nresults in the overall energy increase. Reducing the number\\nof pools (2 or 4) prevents the system from fine tuning\\nthe frequency and the model parallelism for specific request\\ntypes. The performance improves by adding moderately more\\npools because it helps remove head of the line blocking and\\nintroduces more resources for execution.\\nD. Long Cluster-Level Experiments\\nWe run longer experiments by running the 1-day traces for\\nthe Conversation service. The trace covers all invocations for\\n10\\n', b'Energy\\n0\\n10\\n20\\n30\\n40\\nEnergy [kWh]\\nPerformance\\n0.0\\n0.2\\n0.4\\n0.6\\nTTFT [s]\\n2Pool\\n4Pool\\n6Pool\\n9Pool\\n12Pool\\n16Pool\\nFig. 13: Energy and performance with different number of\\npools (or request types) running open-source Llama2-70B\\nmodel [67] with 1-hour open-source production traces [50].\\nConversation\\nCoding\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nNorm. Energy\\nSinglePool\\nMultiPool\\nScaleInst\\nScaleShard\\nScaleFreq\\nDynamoLLM\\nFig. 14: Normalized energy consumption for the six evaluated\\nsystems with the week-long production traces running open-\\nsource Llama2-70B model [67].\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\nTime [h]\\n0\\n1\\n2\\n3\\n4\\n5\\nEnergy [kWh]\\nBaseline\\nDynamoLLM\\nFig. 15: Energy consumption of SinglePool and DynamoLLM\\nwith 1-day production traces running open-source Llama2-70B\\nmodel [67].\\nMon\\nTue\\nWed\\nThu\\nFri\\nSat\\nSun\\nTime\\n0\\n10\\n20\\n30\\n40\\n50\\nCO2 Emissions [kg/h]\\nBaseline\\nDynamoLLM\\nFig. 16: Carbon emissions over time for the week-long con-\\nversation traces with DynamoLLM and SinglePool baseline\\nrunning open-source Llama2-70B model [67].\\na subset of the service\\xe2\\x80\\x99s instances during a typical work day.\\nWe run the experiment for 24-hours on 11 H100 servers with\\nSinglePool and scale the number of instances based on the load\\nin DynamoLLM. Figure 15 shows the energy consumption\\nover 5-minute intervals for the two systems. DynamoLLM\\nreduces the energy consumption over the baseline during peak\\nhours (when dynamic power dominates), and during the low\\nutilization times (when idle power dominates). Over the whole\\nday, DynamoLLM reduces the energy consumption by 42%.\\nE. Large-Scale Simulations\\nTo generalize our insights to large-scale, we develop a\\ndiscrete-time simulator that simulates the energy consumption\\nof different systems using production traces. Figure 14 shows\\nthe normalized energy consumption for the five evaluated\\nsystems using 1-week traces for Conversation and Coding\\nservices. DynamoLLM significantly reduces the energy con-\\nsumption for both types of services. DynamoLLM operates\\nin higher energy-efficient modes for the Conversation service\\ndue to its typically shorter input lengths (ML dominant request\\ntype). On the other hand, the Coding service has deep valleys\\nduring the night and weekends. Thus, DynamoLLM exploits\\nthe periods of low load to save energy. DynamoLLM reduces\\nthe energy consumption over the baseline by 47% and 56%\\nfor the Conversation and Coding services, respectively.\\nF. Cost and Carbon Emission\\nUser cost DynamoLLM reduces the operational cost for users\\nby minimizing the number of GPUs and optimizing their\\nenergy efficiency. The number of GPU servers for the week-\\nlong experiments reduces from 40 to 24.6 on average (38.5%\\ncost reduction). Using the current GPU VM pricing [8], this\\nsaves $1362.7/hour. By reducing the energy consumption,\\nDynamoLLM reduces the associated energy costs by up to\\n56%. As energy costs [28] are currently substantially lower\\nthan GPU costs, this translates to only $4.4/hour savings.\\nCarbon emissions The energy consumption translates into the\\namount of operational CO2 emissions. We use the traces of\\ncarbon intensity [2] for a week-long period from multiple grids\\nand map the carbon intensity to the energy consumption over\\ntime for the SinglePool baseline and DynamoLLM. Figure 16\\nshows the operational carbon emissions over time for the\\ntwo systems for CAISO [1]. SinglePool and DynamoLLM\\nconsume 5t and 3.1t/week of CO2. These substantial savings\\n(38%) make a step towards sustainable LLM environments.\\nVI. RELATED WORK\\nCluster resource and power management A rich body of\\nwork seeks to improve resource efficiency under the SLO\\nconstraints through resource management for a wide range\\nof latency sensitive workloads, such as microservices [76]\\nand DL workloads, through effective resource sharing [6],\\n[43], [51], dynamic allocation [71], and hardware reconfigura-\\ntion [24]. Others focus on approaches that enable safe power\\nmanagement and oversubscription [16], [29], [49] leveraging\\nworkload characteristics [25], [75] and system state [62].\\nEnergy-efficient workloads Prior works focused on energy-\\nefficiency for CPU workloads [15], [31], [43], [61], and\\nresearchers started exploring unique energy properties of GPU\\nworkloads [58], [65], [74]. Recent schemes build on top and\\nmanage energy and power consumption for DNN inference\\nand training [69], [70], [72] through frequency scaling [20],\\n[23], [40], [41], [64], [77], [82], autoscaling [22], and re-\\nsource partitioning and mapping [18], [64]. We show that\\nimproving energy efficiency for LLM inference necessitates\\na comprehensive view of all available knobs. DynamoLLM is\\nholistic framework that dynamically reconfigures all the knobs\\nconsidering the diversity and dynamism of requests and loads.\\n11\\n', b'Efficient LLM inference serving Recent works propose\\napproaches to improve LLM inference efficiency through het-\\nerogeneous resources [4] and platforms [35], memory and key-\\nvalue cache management [9], [26], and node- and cluster-level\\nscheduling [3], [30], [42], [48], [50], [73], [81]. While these\\nstudies focus on improving throughput or latency, we show\\nthat optimizing energy efficiency for LLM inference exhibits\\ndistinct trade-offs between performance, energy consumption,\\nand overheads and thus requires a comprehensive framework.\\nVII. CONCLUSION\\nWe present DynamoLLM, the first energy-management\\nframework for LLM inference clusters. DynamoLLM exploits\\nheterogeneity in inference compute properties and fluctuations\\nin inference workloads to save energy. The system automat-\\nically and dynamically configures the energy-optimal organi-\\nzation of the cluster (number of instances, model parallelism\\nand GPU frequency) while performing under performance\\nguarantees. DynamoLLM reduces energy, carbon emissions\\nand cost to the customer by 53%, 38% and 61%, respectively.\\nREFERENCES\\n[1] \\xe2\\x80\\x9cCAISO\\ncarbon\\nemission,\\xe2\\x80\\x9d\\nhttps://www.caiso.com/todays-\\noutlook/emissions, 2024.\\n[2] \\xe2\\x80\\x9cWattTime,\\xe2\\x80\\x9d https://watttime.org, 2024.\\n[3] A. Agrawal, A. Panwar, J. Mohan, N. Kwatra, B. S. Gulavani, and\\nR. Ramjee, \\xe2\\x80\\x9cSARATHI: Efficient LLM Inference by Piggybacking\\nDecodes with Chunked Prefills,\\xe2\\x80\\x9d 2023.\\n[4] K. Alizadeh, I. Mirzadeh, D. Belenko, K. Khatamifard, M. Cho, C. C. D.\\nMundo, M. Rastegari, and M. Farajtabar, \\xe2\\x80\\x9cLLM in a flash: Efficient\\nLarge Language Model Inference with Limited Memory,\\xe2\\x80\\x9d 2024.\\n[5] J. Bowne, \\xe2\\x80\\x9cUsing large language models in learning and teaching,\\xe2\\x80\\x9d\\nhttps://biomedicalsciences.unimelb.edu.au/study/dlh/assets/documents/\\nlarge-language-models-in-education/llms-in-education, 2024.\\n[6] S. Chen, C. Delimitrou, and J. F. Mart\\xc2\\xb4\\xc4\\xb1nez, \\xe2\\x80\\x9cPARTIES: QoS-Aware\\nResource Partitioning for Multiple Interactive Services,\\xe2\\x80\\x9d in Proceedings\\nof the Twenty-Fourth International Conference on Architectural Support\\nfor Programming Languages and Operating Systems (ASPLOS \\xe2\\x80\\x9919),\\n2019.\\n[7] S. Chen, A. Jin, C. Delimitrou, and J. F. Mart\\xc2\\xb4\\xc4\\xb1nez, \\xe2\\x80\\x9cReTail: Opting for\\nLearning Simplicity to Enable QoS-Aware Power Management in the\\nCloud,\\xe2\\x80\\x9d in Proceedings of the IEEE International Symposium on High-\\nPerformance Computer Architecture (HPCA \\xe2\\x80\\x9922), 2022.\\n[8] Cloud Price, \\xe2\\x80\\x9cStandard ND96is H100 v5,\\xe2\\x80\\x9d https://cloudprice.net/vm/\\nStandard ND96is H100 v5, 2024.\\n[9] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R\\xc2\\xb4e, \\xe2\\x80\\x9cFlashAttention: Fast\\nand Memory-Efficient Exact Attention with IO-Awareness,\\xe2\\x80\\x9d 2022.\\n[10] C. Delimitrou and C. Kozyrakis, \\xe2\\x80\\x9cAmdahl\\xe2\\x80\\x99s Law for Tail Latency,\\xe2\\x80\\x9d\\nCommun. ACM, vol. 61, no. 8, jul 2018.\\n[11] D. Ding, A. Mallick, C. Wang, R. Sim, S. Mukherjee, V. Ruehle,\\nL. V. S. Lakshmanan, and A. Awadallah, \\xe2\\x80\\x9cHybrid LLM: Cost-Efficient\\nand Quality-Aware Query Routing,\\xe2\\x80\\x9d in Proceedings of the International\\nConference on Learning Representations (ICLR \\xe2\\x80\\x9924), 2024.\\n[12] A. Faiz, S. Kaneda, R. Wang, R. Osi, P. Sharma, F. Chen, and L. Jiang,\\n\\xe2\\x80\\x9cLLMCarbon: Modeling the end-to-end Carbon Footprint of Large\\nLanguage Models,\\xe2\\x80\\x9d arXiv preprint arXiv:2309.14393, 2024.\\n[13] GitHub, \\xe2\\x80\\x9cThe world\\xe2\\x80\\x99s most widely adopted ai developer tool,\\xe2\\x80\\x9d https:\\n//github.com/features/copilot, 2024.\\n[14] M. Halpern, B. Boroujerdian, T. Mummert, E. Duesterwald, and\\nV. Janapa Reddi, \\xe2\\x80\\x9cOne Size Does Not Fit All: Quantifying and Exposing\\nthe Accuracy-Latency Trade-Off in Machine Learning Cloud Service\\nAPIs via Tolerance Tiers,\\xe2\\x80\\x9d in Proceedings of the IEEE International\\nSymposium on Performance Analysis of Systems and Software (ISPASS\\n\\xe2\\x80\\x9919), 2019.\\n[15] M. E. Haque, Y. He, S. Elnikety, T. D. Nguyen, R. Bianchini, and\\nK. S. McKinley, \\xe2\\x80\\x9cExploiting Heterogeneity for Tail Latency and Energy\\nEfficiency,\\xe2\\x80\\x9d in Proceedings of the 50th Annual IEEE/ACM International\\nSymposium on Microarchitecture (MICRO \\xe2\\x80\\x9917), 2017.\\n[16] C.-H. Hsu, Q. Deng, J. Mars, and L. Tang, \\xe2\\x80\\x9cSmoothOperator: Reducing\\nPower Fragmentation and Improving Power Utilization in Large-Scale\\nDatacenters,\\xe2\\x80\\x9d in Proceedings of the 23rd International Conference\\non Architectural Support for Programming Languages and Operating\\nSystems (ASPLOS \\xe2\\x80\\x9918), 2018.\\n[17] C.-H. Hsu, Y. Zhang, M. A. Laurenzano, D. Meisner, T. Wenisch,\\nJ. Mars, L. Tang, and R. G. Dreslinski, \\xe2\\x80\\x9cAdrenaline: Pinpointing and\\nreining in tail queries with quick voltage boosting,\\xe2\\x80\\x9d in Proceedings of\\nthe IEEE 21st International Symposium on High Performance Computer\\nArchitecture (HPCA \\xe2\\x80\\x9915), 2015.\\n[18] A. Jahanshahi, M. Rezvani, and D. Wong, \\xe2\\x80\\x9cWattWiser: Power and\\nResource-Efficient Scheduling for Multi-Model Multi-GPU Inference\\nServers,\\xe2\\x80\\x9d in Proceedings of the 14th International Green and Sustainable\\nComputing Conference (IGSC \\xe2\\x80\\x9923), 2023.\\n[19] Y. Jin, C.-F. Wu, D. Brooks, and G.-Y. Wei, \\xe2\\x80\\x9c S3: Increasing GPU\\nutilization during generative inference for higher throughput,\\xe2\\x80\\x9d in Pro-\\nceedings of the Advances in Neural Information Processing Systems\\n(NeurIPS \\xe2\\x80\\x9923), 2023.\\n[20] A. K. Kakolyris, D. Masouros, S. Xydis, and D. Soudris, \\xe2\\x80\\x9cSLO-\\naware GPU DVFS for Energy-efficient LLM Inference Serving,\\xe2\\x80\\x9d IEEE\\nComputer Architecture Letters, 2024.\\n[21] H. Kasture, D. B. Bartolini, N. Beckmann, and D. Sanchez, \\xe2\\x80\\x9cRubik:\\nFast analytical power management for latency-critical systems,\\xe2\\x80\\x9d in\\nProceedings of the 48th Annual IEEE/ACM International Symposium\\non Microarchitecture (MICRO \\xe2\\x80\\x9915), 2015.\\n[22] Y. G. Kim and C.-J. Wu, \\xe2\\x80\\x9cAutoScale: Energy Efficiency Optimization\\nfor Stochastic Edge Inference Using Reinforcement Learning,\\xe2\\x80\\x9d in Pro-\\nceedings of the 53rd Annual IEEE/ACM International Symposium on\\nMicroarchitecture (MICRO \\xe2\\x80\\x9920), 2020.\\n[23] T. Komoda, S. Hayashi, T. Nakada, S. Miwa, and H. Nakamura, \\xe2\\x80\\x9cPower\\ncapping of cpu-gpu heterogeneous systems through coordinating dvfs\\nand task mapping,\\xe2\\x80\\x9d in Proceedings of the IEEE 31st International\\nConference on Computer Design (ICCD \\xe2\\x80\\x9913), 2013.\\n[24] N. Kulkarni, G. Gonzalez-Pumariega, A. Khurana, C. A. Shoemaker,\\nC. Delimitrou, and D. H. Albonesi, \\xe2\\x80\\x9cCuttleSys: Data-Driven Resource\\nManagement for Interactive Services on Reconfigurable Multicores,\\xe2\\x80\\x9d in\\nProceedings of the 53rd Annual IEEE/ACM International Symposium on\\nMicroarchitecture (MICRO \\xe2\\x80\\x9920), 2020.\\n[25] A. G. Kumbhare, R. Azimi, I. Manousakis, A. Bonde, F. Frujeri,\\nN. Mahalingam, P. A. Misra, S. A. Javadi, B. Schroeder, M. Fontoura,\\nand R. Bianchini, \\xe2\\x80\\x9cPrediction-Based Power Oversubscription in Cloud\\nPlatforms,\\xe2\\x80\\x9d in Proceedings of the USENIX Annual Technical Conference\\n(USENIX ATC \\xe2\\x80\\x9921), 2021.\\n[26] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E.\\nGonzalez, H. Zhang, and I. Stoica, \\xe2\\x80\\x9cEfficient Memory Management for\\nLarge Language Model Serving with PagedAttention,\\xe2\\x80\\x9d in Proceedings\\nof the 29th Symposium on Operating Systems Principles (SOSP \\xe2\\x80\\x9923),\\n2023.\\n[27] M. Lammertyn, \\xe2\\x80\\x9c60+ ChatGPT Statistics And Facts You Need to Know\\nin 2024,\\xe2\\x80\\x9d https://blog.invgate.com/chatgpt-statistics, 2024.\\n[28] LCG\\nConsulting,\\n\\xe2\\x80\\x9cEnergy\\nOnline:\\nERCOT\\nReal\\nTime\\nPrice,\\xe2\\x80\\x9d\\nhttps://energyonline.com/Data/GenericData.aspx?DataId=4&ERCOT\\nReal-time Price, 2024.\\n[29] S. Li, X. Wang, X. Zhang, V. Kontorinis, S. Kodakara, D. Lo,\\nand P. Ranganathan, \\xe2\\x80\\x9cThunderbolt: Throughput-Optimized, Quality-of-\\nService-Aware Power Capping at Scale,\\xe2\\x80\\x9d in Proceedings of the 14th\\nUSENIX Symposium on Operating Systems Design and Implementation\\n(OSDI \\xe2\\x80\\x9920), 2020.\\n[30] Z. Li, L. Zheng, Y. Zhong, V. Liu, Y. Sheng, X. Jin, Y. Huang,\\nZ. Chen, H. Zhang, J. E. Gonzalez, and I. Stoica, \\xe2\\x80\\x9cAlpaServe: Statistical\\nMultiplexing with Model Parallelism for Deep Learning Serving,\\xe2\\x80\\x9d in\\nProceedings of the 17th USENIX Symposium on Operating Systems\\nDesign and Implementation (OSDI \\xe2\\x80\\x9923), 2023.\\n[31] D. Lo, L. Cheng, R. Govindaraju, L. A. Barroso, and C. Kozyrakis,\\n\\xe2\\x80\\x9cTowards energy proportionality for large-scale latency-critical work-\\nloads,\\xe2\\x80\\x9d in Proceedings of the ACM/IEEE 41st International Symposium\\non Computer Architecture (ISCA \\xe2\\x80\\x9914), 2014.\\n[32] Meta, \\xe2\\x80\\x9cLlama2-13B,\\xe2\\x80\\x9d https://huggingface.co/meta-llama/Llama-2-13b,\\n2024.\\n[33] Meta, \\xe2\\x80\\x9cLlama2-70B,\\xe2\\x80\\x9d https://huggingface.co/meta-llama/Llama-2-70b,\\n2024.\\n[34] Meta, \\xe2\\x80\\x9cLlama3-70B,\\xe2\\x80\\x9d https://huggingface.co/meta-llama/Meta-Llama-3-\\n70B-Instruct, 2024.\\n12\\n', b'[35] X. Miao, C. Shi, J. Duan, X. Xi, D. Lin, B. Cui, and Z. Jia, \\xe2\\x80\\x9cSpotServe:\\nServing Generative Large Language Models on Preemptible Instances,\\xe2\\x80\\x9d\\nin Proceedings of the 29th ACM International Conference on Archi-\\ntectural Support for Programming Languages and Operating Systems\\n(ASPLOS \\xe2\\x80\\x9924), 2024.\\n[36] Microsoft Azure, \\xe2\\x80\\x9cND H100 v5-series,\\xe2\\x80\\x9d https://learn.microsoft.com/en-\\nus/azure/virtual-machines/nd-h100-v5-series, 2024.\\n[37] A. Mirhosseini and T. Wenisch, \\xe2\\x80\\x9c\\xc2\\xb5Steal: A Theory-Backed Framework\\nfor Preemptive Work and Resource Stealing in Mixed-Criticality Mi-\\ncroservices,\\xe2\\x80\\x9d in Proceedings of the ACM International Conference on\\nSupercomputing (ICS \\xe2\\x80\\x9921), 2021.\\n[38] Mistral AI, \\xe2\\x80\\x9cThe Mixtral-8x22B Large Language Model,\\xe2\\x80\\x9d https://\\nhuggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1, 2024.\\n[39] Mistral AI, \\xe2\\x80\\x9cThe Mixtral-8x7B Large Language Model,\\xe2\\x80\\x9d https://\\nhuggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1, 2024.\\n[40] S. M. Nabavinejad, S. Reda, and M. Ebrahimi, \\xe2\\x80\\x9cBatchSizer: Power-\\nPerformance Trade-off for DNN Inference,\\xe2\\x80\\x9d in Proceedings of the 26th\\nAsia and South Pacific Design Automation Conference (ASP-DAC \\xe2\\x80\\x9921),\\n2021.\\n[41] S. M. Nabavinejad, S. Reda, and M. Ebrahimi, \\xe2\\x80\\x9cCoordinated batching\\nand dvfs for dnn inference on gpu accelerators,\\xe2\\x80\\x9d IEEE Transactions on\\nParallel and Distributed Systems, vol. 33, no. 10, 2022.\\n[42] K. K. W. Ng, H. M. Demoulin, and V. Liu, \\xe2\\x80\\x9cPaella: Low-latency Model\\nServing with Software-defined GPU Scheduling,\\xe2\\x80\\x9d in Proceedings of the\\n29th Symposium on Operating Systems Principles (SOSP \\xe2\\x80\\x9923), 2023.\\n[43] R. Nishtala, V. Petrucci, P. Carpenter, and M. Sjalander, \\xe2\\x80\\x9cTwig: Multi-\\nAgent Task Management for Colocated Latency-Critical Cloud Ser-\\nvices,\\xe2\\x80\\x9d in Proceedings of the IEEE International Symposium on High\\nPerformance Computer Architecture (HPCA \\xe2\\x80\\x9920), 2020.\\n[44] NVIDIA, \\xe2\\x80\\x9cDGX H100: AI for Enterprise,\\xe2\\x80\\x9d https://www.nvidia.com/en-\\nus/data-center/dgx-h100/, 2024.\\n[45] NVIDIA, \\xe2\\x80\\x9cNVLink and NVLink Switch,\\xe2\\x80\\x9d https://www.nvidia.com/en-\\nus/data-center/nvlink/, 2024.\\n[46] NVIDIA, \\xe2\\x80\\x9cSystem Management Interface SMI,\\xe2\\x80\\x9d https://developer.nvidia.\\ncom/system-management-interface, 2024.\\n[47] NVIDIA, \\xe2\\x80\\x9cTensorRT-LLM\\xe2\\x80\\x99s Documentation,\\xe2\\x80\\x9d https://nvidia.github.io/\\nTensorRT-LLM/, 2024.\\n[48] H. Oh, K. Kim, J. Kim, S. Kim, J. Lee, D.-s. Chang, and J. Seo, \\xe2\\x80\\x9cEx-\\neGPT: Constraint-Aware Resource Scheduling for LLM Inference,\\xe2\\x80\\x9d in\\nProceedings of the 29th ACM International Conference on Architectural\\nSupport for Programming Languages and Operating Systems (ASPLOS\\n\\xe2\\x80\\x9924), 2024.\\n[49] P. Patel, E. Choukse, C. Zhang, I. Goiri, B. Warrier, N. Mahalingam,\\nand R. Bianchini, \\xe2\\x80\\x9cCharacterizing Power Management Opportunities for\\nLLMs in the Cloud,\\xe2\\x80\\x9d in Proceedings of the 29th ACM International\\nConference on Architectural Support for Programming Languages and\\nOperating Systems (ASPLOS \\xe2\\x80\\x9924), 2024.\\n[50] P. Patel, E. Choukse, C. Zhang, A. Shah, I. Goiri, S. Maleki, and\\nR. Bianchini, \\xe2\\x80\\x9cSplitwise: Efficient generative LLM inference using phase\\nsplitting,\\xe2\\x80\\x9d in Proceedings of the 51st Annual International Symposium\\non Computer Architecture (ISCA \\xe2\\x80\\x9924), 2024.\\n[51] T. Patel and D. Tiwari, \\xe2\\x80\\x9cCLITE: Efficient and QoS-Aware Co-Location\\nof Multiple Latency-Critical Jobs for Warehouse Scale Computers,\\xe2\\x80\\x9d in\\nProceedings of the IEEE International Symposium on High Performance\\nComputer Architecture (HPCA \\xe2\\x80\\x9920), 2020.\\n[52] C. Peng, X. Yang, A. Chen, K. Smith, N. PourNejatian, A. Costa,\\nC. Martin, M. Flores, Y. Zhang, T. Magoc, G. Lipori, M. Duane,\\nN. Ospina, M. Ahmed, W. Hogan, E. Shenkman, Y. Guo, J. Bian, and\\nY. Wu, \\xe2\\x80\\x9cA study of generative large language model for medical research\\nand healthcare,\\xe2\\x80\\x9d npj Digital Medicine, 2023.\\n[53] Python PuLP, \\xe2\\x80\\x9cOptimization with PuLP,\\xe2\\x80\\x9d https://coin-or.github.io/pulp/,\\n2024.\\n[54] Python SciPy, \\xe2\\x80\\x9cSciPy Library,\\xe2\\x80\\x9d https://scipy.org/, 2024.\\n[55] H. Qiu, W. Mao, A. Patke, S. Cui, S. Jha, C. Wang, H. Franke,\\nZ. T. Kalbarczyk, T. Bas\\xc2\\xb8ar, and R. K. Iyer, \\xe2\\x80\\x9cEfficient Interactive LLM\\nServing with Proxy Model-based Sequence Length Prediction,\\xe2\\x80\\x9d in The\\n5th International Workshop on Cloud Intelligence / AIOps at ASPLOS\\n2024, 2024.\\n[56] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\\n\\xe2\\x80\\x9cLanguage models are unsupervised multitask learners,\\xe2\\x80\\x9d OpenAI blog,\\nvol. 1, no. 8, p. 9, 2019.\\n[57] F. Romero, Q. Li, N. J. Yadwadkar, and C. Kozyrakis, \\xe2\\x80\\x9cINFaaS: Au-\\ntomated Model-less Inference Serving,\\xe2\\x80\\x9d in Proceedings of the USENIX\\nAnnual Technical Conference (USENIX ATC \\xe2\\x80\\x9921), 2021.\\n[58] S. Samsi, D. Zhao, J. McDonald, B. Li, A. Michaleas, M. Jones,\\nW. Bergeron, J. Kepner, D. Tiwari, and V. Gadepally, \\xe2\\x80\\x9cFrom words\\nto watts: Benchmarking the energy costs of large language model infer-\\nence,\\xe2\\x80\\x9d in 2023 IEEE High Performance Extreme Computing Conference\\n(HPEC).\\nIEEE, 2023, pp. 1\\xe2\\x80\\x939.\\n[59] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili\\xc2\\xb4c, D. Hesslow,\\nR. Castagn\\xc2\\xb4e, A. S. Luccioni, F. Yvon, M. Gall\\xc2\\xb4e et al., \\xe2\\x80\\x9cBLOOM:\\nA 176b-parameter open-access multilingual language model,\\xe2\\x80\\x9d arXiv\\npreprint arXiv:2211.05100, 2022.\\n[60] J. Stojkovic, E. Choukse, C. Zhang, I. Goiri, and J. Torrellas, \\xe2\\x80\\x9cTowards\\nGreener LLMs: Bringing Energy-Efficiency to the Forefront of LLM\\nInference,\\xe2\\x80\\x9d arXiv e-prints, p. arXiv:2403.20306, Mar. 2024.\\n[61] J. Stojkovic, N. Iliakopoulou, T. Xu, H. Franke, and J. Torrellas,\\n\\xe2\\x80\\x9cEcoFaaS: Rethinking the Design of Serverless Environments for Energy\\nEfficiency,\\xe2\\x80\\x9d in Proceedings of the 51st Annual International Symposium\\non Computer Architecture (ISCA \\xe2\\x80\\x9924), 2024.\\n[62] J. Stojkovic, P. Misra, I. Goiri, S. Whitlock, E. Choukse, M. Das,\\nC. Bansal, J. Lee, Z. Sun, H. Qiu, R. Zimmermann, S. Samal, B. Warrier,\\nA. Raniwala, and R. Bianchini, \\xe2\\x80\\x9cSmartOClock: Workload- and Risk-\\nAware Overclocking in the Cloud,\\xe2\\x80\\x9d in Proceedings of the 51st Annual\\nInternational Symposium on Computer Architecture (ISCA \\xe2\\x80\\x9924), 2024.\\n[63] K.\\nTalamadupula,\\n\\xe2\\x80\\x9cA\\nGuide\\nto\\nLLM\\nInference\\nPerformance\\nMonitoring,\\xe2\\x80\\x9d https://symbl.ai/developers/blog/a-guide-to-llm-inference-\\nperformance-monitoring/, 2024.\\n[64] T. Tambe, C. Hooper, L. Pentecost, T. Jia, E.-Y. Yang, M. Donato,\\nV. Sanh, P. Whatmough, A. M. Rush, D. Brooks, and G.-Y. Wei, \\xe2\\x80\\x9cEdge-\\nBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-\\nTask NLP Inference,\\xe2\\x80\\x9d in Proceedings of the 54th Annual IEEE/ACM\\nInternational Symposium on Microarchitecture (MICRO \\xe2\\x80\\x9921), 2021.\\n[65] Z. Tang, Y. Wang, Q. Wang, and X. Chu, \\xe2\\x80\\x9cThe Impact of GPU DVFS\\non the Energy and Performance of Deep Learning: an Empirical Study,\\xe2\\x80\\x9d\\nin Proceedings of the Tenth ACM International Conference on Future\\nEnergy Systems (e-Energy \\xe2\\x80\\x9919), 2019.\\n[66] Technology Innovation Institute (TII), \\xe2\\x80\\x9cFalcon-180B,\\xe2\\x80\\x9d 2024.\\n[67] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \\xe2\\x80\\x9cLlama\\n2: Open foundation and fine-tuned chat models,\\xe2\\x80\\x9d arXiv preprint\\narXiv:2307.09288, 2023.\\n[68] T. Varshney, \\xe2\\x80\\x9cBuild an llm-powered data agent for data analysis,\\xe2\\x80\\x9d\\nhttps://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-\\ndata-analysis/, 2024.\\n[69] C. Wan, M. Santriaji, E. Rogers, H. Hoffmann, M. Maire, and S. Lu,\\n\\xe2\\x80\\x9cALERT: Accurate learning for energy and timeliness,\\xe2\\x80\\x9d in Proceedings\\nof the USENIX Annual Technical Conference (USENIX ATC \\xe2\\x80\\x9920), 2020.\\n[70] F. Wang, W. Zhang, S. Lai, M. Hao, and Z. Wang, \\xe2\\x80\\x9cDynamic GPU\\nEnergy Optimization for Machine Learning Training Workloads,\\xe2\\x80\\x9d IEEE\\nTransactions on Parallel and Distributed Systems, 2022.\\n[71] W. Xiao, S. Ren, Y. Li, Y. Zhang, P. Hou, Z. Li, Y. Feng, W. Lin, and\\nY. Jia, \\xe2\\x80\\x9cAntMan: Dynamic scaling on GPU clusters for deep learning,\\xe2\\x80\\x9d\\nin Proceedings of the 14th USENIX Symposium on Operating Systems\\nDesign and Implementation (OSDI \\xe2\\x80\\x9920), 2020.\\n[72] J. You, J.-W. Chung, and M. Chowdhury, \\xe2\\x80\\x9cZeus: Understanding and\\noptimizing GPU energy consumption of DNN training,\\xe2\\x80\\x9d in Proceedings\\nof the 20th USENIX Symposium on Networked Systems Design and\\nImplementation (NSDI \\xe2\\x80\\x9923), 2023.\\n[73] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, \\xe2\\x80\\x9cOrca: A\\nDistributed Serving System for Transformer-Based Generative Models,\\xe2\\x80\\x9d\\nin Proceedings of the 16th USENIX Symposium on Operating Systems\\nDesign and Implementation (OSDI \\xe2\\x80\\x9922), 2022.\\n[74] J. Yu, J. Kim, and E. Seo, \\xe2\\x80\\x9cKnow Your Enemy To Save Cloud Energy:\\nEnergy-Performance Characterization of Machine Learning Serving,\\xe2\\x80\\x9d in\\n2023 IEEE International Symposium on High-Performance Computer\\nArchitecture (HPCA), 2023.\\n[75] C. Zhang, A. G. Kumbhare, I. Manousakis, D. Zhang, P. A. Misra,\\nR. Assis, K. Woolcock, N. Mahalingam, B. Warrier, D. Gauthier,\\nL. Kunnath, S. Solomon, O. Morales, M. Fontoura, and R. Bianchini,\\n\\xe2\\x80\\x9cFlex: High-Availability Datacenters with Zero Reserved Power,\\xe2\\x80\\x9d in\\nProceedings of the 48th Annual International Symposium on Computer\\nArchitecture (ISCA \\xe2\\x80\\x9921), 2021.\\n[76] Y. Zhang, W. Hua, Z. Zhou, G. E. Suh, and C. Delimitrou, \\xe2\\x80\\x9cSinan: ML-\\nBased and QoS-Aware Resource Management for Cloud Microservices,\\xe2\\x80\\x9d\\nin Proceedings of the 26th ACM International Conference on Archi-\\ntectural Support for Programming Languages and Operating Systems\\n(ASPLOS \\xe2\\x80\\x9921), 2021.\\n13\\n', b'[77] Y. Zhang, Q. Wang, Z. Lin, P. Xu, and B. Wang, \\xe2\\x80\\x9cImproving gpu energy\\nefficiency through an application-transparent frequency scaling policy\\nwith performance assurance,\\xe2\\x80\\x9d in Proceedings of the Nineteenth European\\nConference on Computer Systems (EuroSys \\xe2\\x80\\x9924), 2024.\\n[78] Y. Z. Zhao, D. W. Wu, and J. Wang, \\xe2\\x80\\x9cALISA: Accelerating Large Lan-\\nguage Model Inference via Sparsity-Aware KV Caching,\\xe2\\x80\\x9d in Proceedings\\nof the 51st Annual International Symposium on Computer Architecture\\n(ISCA \\xe2\\x80\\x9924), 2024.\\n[79] Z. Zheng, X. Ren, F. Xue, Y. Luo, X. Jiang, and Y. You, \\xe2\\x80\\x9cResponse\\nLength Perception and Sequence Scheduling: An LLM-Empowered\\nLLM Inference Pipeline,\\xe2\\x80\\x9d in Proceedings of the Advances in Neural\\nInformation Processing Systems (NeurIPS \\xe2\\x80\\x9923), 2023.\\n[80] L. Zhou, L. N. Bhuyan, and K. K. Ramakrishnan, \\xe2\\x80\\x9cGemini: Learning\\nto Manage CPU Power for Latency-Critical Search Engines,\\xe2\\x80\\x9d in Pro-\\nceedings of the 53rd Annual IEEE/ACM International Symposium on\\nMicroarchitecture (MICRO \\xe2\\x80\\x9920), 2020.\\n[81] Z. Zhou, X. Wei, J. Zhang, and G. Sun, \\xe2\\x80\\x9cPetS: A unified framework for\\nParameter-Efficient transformers serving,\\xe2\\x80\\x9d in Proceedings of the USENIX\\nAnnual Technical Conference (USENIX ATC \\xe2\\x80\\x9922), 2022.\\n[82] P. Zou, A. Li, K. Barker, and R. Ge, \\xe2\\x80\\x9cIndicator-Directed Dynamic Power\\nManagement for Iterative Workloads on GPU-Accelerated Systems,\\xe2\\x80\\x9d in\\nProceedings of the 20th IEEE/ACM International Symposium on Cluster,\\nCloud and Internet Computing (CCGRID \\xe2\\x80\\x9920), 2020.\\n14\\n']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Text from paper2.pdf:\n",
            "[b'Offline Energy-Optimal LLM Serving: Workload-Based Energy\\nModels for LLM Inference on Heterogeneous Systems\\nGrant Wilkins\\ngfw27@cam.ac.uk\\nUniversity of Cambridge\\nCambridge, UK\\nSrinivasan Keshav\\nsk818@cam.ac.uk\\nUniversity of Cambridge\\nCambridge, UK\\nRichard Mortier\\nrmm1002@cam.ac.uk\\nUniversity of Cambridge\\nCambridge, UK\\nABSTRACT\\nThe rapid adoption of large language models (LLMs) has led to\\nsignificant advances in natural language processing and text gener-\\nation. However, the energy consumed through LLM model infer-\\nence remains a major challenge for sustainable AI deployment. To\\naddress this problem, we model the workload-dependent energy\\nconsumption and runtime of LLM inference tasks on heterogeneous\\nGPU-CPU systems. By conducting an extensive characterization\\nstudy of several state-of-the-art LLMs and analyzing their energy\\nand runtime behavior across different magnitudes of input prompts\\nand output text, we develop accurate (\\xf0\\x9d\\x91\\x852 > 0.96) energy and run-\\ntime models for each LLM. We employ these models to explore\\nan offline, energy-optimal LLM workload scheduling framework.\\nThrough a case study, we demonstrate the advantages of energy\\nand accuracy aware scheduling compared to existing best practices.\\nCCS CONCEPTS\\n\\xe2\\x80\\xa2 Computer systems organization \\xe2\\x86\\x92Heterogeneous (hybrid)\\nsystems; \\xe2\\x80\\xa2 Hardware \\xe2\\x86\\x92Impact on the environment.\\nKEYWORDS\\nSustainable computing, Heterogeneous computing, Large Language\\nModels, Artificial Intelligence\\nACM Reference Format:\\nGrant Wilkins, Srinivasan Keshav, and Richard Mortier. 2024. Offline Energy-\\nOptimal LLM Serving: Workload-Based Energy Models for LLM Inference on\\nHeterogeneous Systems. In The 3rd ACM HotCarbon Workshop on Sustainable\\nComputer System (HotCarbon \\xe2\\x80\\x9924), 9 July 2024, Santa Cruz, CA. ACM, New\\nYork, NY, USA, 7 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\\n1\\nINTRODUCTION\\nRapid advancements in large language models (LLMs) have rev-\\nolutionized natural language processing, enabling AI systems to\\nachieve human-level performance on a wide range of language\\ntasks [3, 26, 40]. However, the computational resources and energy\\nconsumption associated with deploying these models present signif-\\nicant challenges to not only energy systems but also sustainability\\ngoals [20, 21, 29]. As LLMs become increasingly integrated into\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nHotCarbon\\xe2\\x80\\x9924, 9 July 2024, Santa Cruz, CA\\n\\xc2\\xa9 2024 Copyright held by the owner/author(s).\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\\nreal-world applications, optimizing their energy efficiency during\\ninference is crucial for sustainable AI development [45].\\nInference, the process of using a trained model to make predic-\\ntions on new data, is a critical phase in LLM deployment as it is the\\npoint at which AI capabilities become accessible to users. Unlike the\\none-time training process, inference is an ongoing, real-time pro-\\ncess that directly impacts end-user experience. Inference in LLMs\\ncan be computationally expensive due to model size [45] and quality\\nof service/latency expectations [42]. Scaling LLMs up across large\\ndata centers is challenging due to power [27] and communication\\noverheads [28].\\nThe energy intensity of LLM inference can be substantial even\\nwhen compared to training [4]. Decarbonizing the energy sources\\nfor data centers can be challenging due to both sporadic demand and\\nregional inefficiencies in adopting renewables. Higher energy con-\\nsumption of an application approximately correlates with greater\\ncarbon intensity [32]. It is thus crucial to find energy-efficient meth-\\nods to mitigate the environmental costs of LLM inference [1, 18].\\nTo address this issue, we propose a workload-based model of\\nenergy consumption for LLM inference to let system operators\\nnavigate the trade-off between accuracy and energy usage.\\nOur contributions are as follows:\\n(1) We characterize the energy consumption and runtime be-\\nhavior of several state-of-the-art LLMs on a heterogeneous\\nGPU-CPU system (\\xc2\\xa74).\\n(2) We develop workload-based energy and runtime models\\nthat accurately capture the relationship between the number\\nof input and output tokens and the energy and runtime\\ncharacteristics of each LLM (\\xc2\\xa75).\\n(3) We demonstrate the effectiveness of our approach through a\\ncase study, showcasing a tunable trade-off between energy\\nand accuracy (\\xc2\\xa76).\\nOur profiling framework and datasets are openly available.1\\n2\\nRELATED WORK\\n2.1\\nEnergy Consumption in AI Systems\\nRecent reports have found that the computation required by state-\\nof-the-art AI systems entail massive energy consumption and car-\\nbon emissions [4, 22, 29, 35, 45]. The energy intensity of AI systems\\ncan be broadly split between the energy required for training and\\nthat required for inference after models are deployed [10]. Training\\ncomplex models on massive datasets is an energy-intensive process,\\nwith estimates finding that training GPT-3 required 1,287 megawatt-\\nhours of energy [29]. Even with this huge amount of energy, a year\\nof inference by an LLM on cloud infrastructure can consume over\\n1https://github.com/grantwilkins/energy-inference.git\\narXiv:2407.04014v1  [cs.DC]  4 Jul 2024\\n', b'HotCarbon\\xe2\\x80\\x9924, 9 July 2024, Santa Cruz, CA\\nGrant Wilkins, Srinivasan Keshav, and Richard Mortier\\n25\\xc3\\x97 more energy than training that same model [4]. Some of these\\nissues and emissions of course depend on the deployment scale and\\nhardware efficiency [35], however, the trend remains that energy\\nconsumption in inference is a large issue. Optimizing software and\\nhardware specifically for AI workloads is thus essential [1].\\nDesislavov et al. [5] provide an examination of trends in AI in-\\nference energy consumption, arguing that while performance has\\nincreased dramatically, energy consumption has not escalated at\\nthe same pace, thanks to hardware optimizations and algorithmic\\ninnovations. Chien et al. [4] discuss larger trends in LLM inference\\nenergy consumption and do not focus on device-level energy mod-\\neling benefits. Samsi et al. [35] explore the energy consumption of\\nMeta\\xe2\\x80\\x99s Llama LLMs for different batch sizes and numbers of GPUs,\\nshowing the potential energy reductions obtainable by tuning these\\nparameters. Stojcovik et al. [37] discuss the impacts of GPU fre-\\nquency scaling on the energy efficiency of serving LLMs; however,\\nat this point, this work is only a characterization and not an applied\\nanalysis.\\nOur work extends these studies with a thorough CPU+GPU\\nenergy measurements across multiple model families and sizes,\\nproducing one of the most comprehensive datasets of its kind.\\n2.2\\nEnergy-Aware Data Center Scheduling\\nA large body of work that focuses on energy-aware scheduling [6,\\n13, 19, 24, 34, 38], but none of these focus on the unique challenge\\nof developing workload-aware models for LLM inference towards\\nthis goal. Hu et al. [12] analyze deep learning workloads in GPU\\ndata centers, offering insights into energy conservation strategies\\nthrough workload scheduling. This research aligns with our ob-\\njectives by confirming the critical role of scheduling in reducing\\nenergy footprints.\\nLi et al. [17] introduce Clover, which promises to minimize car-\\nbon emissions for serving AI inference. Unlike our study, this work\\ndoes not explicitly consider LLMs or a per-model function to capture\\nenergy and runtime, instead focusing on carbon-emission patterns\\nfor a data center.\\nGu et al. [8] presents PowerFlow, a tool that uses clock-frequency\\ndata from GPUs to minimize energy consumption as a scheduling\\ndecision. However, their study does not consider LLMs and is not\\nnecessarily workload-aware.\\nPatel et al. introduce POLCA [27], which can provide a way to\\nautomatically power-cap based on existing workload traces. Li et\\nal. [18] focuses on delivering a geographic load balancing perspec-\\ntive for AI inference, optimizing environmental equity. However,\\ntheir model considers large-scale workload traces, not device-level\\nenergy and runtime data.\\nOur work aims to fill the niche with energy-aware LLM inference\\nscheduling using measurements from state-of-the-art open-source\\nLLMs leading to an applied analysis using offline optimization. The\\nresults of our findings can be used by system operators to accurately\\npredict and schedule based on the amount of energy and runtime\\nfor inference.\\n3\\nMETHODS\\nFor our LLM inference engine we use Hugging Face\\xe2\\x80\\x99s Accelerate [9].\\nThis library uses all available GPUs, and divides a model among\\nthe available GPUs in a tensor parallel fashion to minimize inter-\\nmediate communication and maximize the distributed capabilities\\nfor computation across the devices. We disable KV-caching [41] to\\nensure that our measurements are consistent between runs and do\\nnot require a warm-start phase.\\n3.1\\nLLM Selection\\nWe study several open-source LLMs, summarized in Table 1. By\\nprofiling different LLMs we are able to explore the effects of diverse\\narchitectures and parameter values on runtime, energy consump-\\ntion, and accuracy. For each model, we conduct a series of standard-\\nized text generation prompts to evaluate their energy consumption\\nduring inference.\\nNumerous benchmarks have sought to quantify LLM accuracy,\\ne.g., the MMLU [11] and HellaSwag [46]. To avoid the inadequacies\\nintroduced by individual tests for accuracy [23], we use the Hugging\\nFace Leaderboard\\xe2\\x80\\x99s [2] average accuracy, denoted \\xf0\\x9d\\x90\\xb4\\xf0\\x9d\\x90\\xbe, that averages\\nthe performance of a model, \\xf0\\x9d\\x90\\xbe, on a large repository of datasets\\nand tests.\\nTable 1: LLM Energy Consumption and Runtime\\nLLM (# Params)\\nvRAM Size (GB)\\n# A100s\\n\\xf0\\x9d\\x91\\xa8\\xf0\\x9d\\x91\\xb2(%) [2]\\nFalcon (7B)\\n14.48\\n1\\n44.17\\nFalcon (40B)\\n83.66\\n3\\n58.07\\nLlama-2 (7B)\\n13.48\\n1\\n50.97\\nLlama-2 (13B)\\n26.03\\n1\\n55.69\\nLlama-2 (70B)\\n137.98\\n4\\n64.52\\nMistral (7B)\\n15.00\\n1\\n60.97\\nMixtral (8x7B)\\n93.37\\n3\\n68.47\\n3.2\\nEnergy Profiling of Our Cluster\\nWe perform all experiments the Swing cluster at Argonne National\\nLab using a single node with 8\\xc3\\x97Nvidia A100 (40GB) GPUs, 2\\xc3\\x97AMD\\nEpyc 7742 64-core processors, and 1TB of DDR4 RAM. We use only\\nthe minimum number of GPUs as shown in Table 1. We profile\\nthe system\\xe2\\x80\\x99s energy consumption during inference using tools that\\ncapture Nvidia GPU energy and AMD CPU power while timing the\\noperation. Our methods utilize the known relationship that \\xf0\\x9d\\x90\\xb8= \\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\xa1\\nwhere \\xf0\\x9d\\x90\\xb8represents energy, \\xf0\\x9d\\x91\\x83is average power, and \\xf0\\x9d\\x91\\xa1is runtime.\\n3.2.1\\nNVIDIA GPUs. We use PyJoules [31], a Python-based en-\\nergy measurement library, to quantify the energy consumption\\nassociated with inference on NVIDIA GPUs. PyJoules provides an\\ninterface to NVML [25], providing a software-defined energy usage\\nassessment for targeted NVIDIA devices. This tool offers GPUs\\nreal-time energy consumption for a given tracked process.\\n3.2.2\\nAMD CPUs. We adopt a different strategy for AMD CPUs\\ndue to the absence of a Python API. Instead, we utilize AMD\\xf0\\x9d\\x9c\\x87Prof\\xe2\\x80\\x99s\\ntimechart feature, which provides detailed power draw metrics\\nfor every core on the chip at fine-grained intervals. By polling\\nAMD\\xf0\\x9d\\x9c\\x87Prof at 100ms intervals, we can capture the power draw of\\neach physical CPU core throughout the model inference process.\\nTo ensure we accurately attribute the energy consumption to our\\ninference task, we monitor the CPU core residency through psutil.\\n', b'Offline Energy-Optimal LLM Serving: Workload-Based Energy Models for LLM Inference on Heterogeneous Systems\\nHotCarbon\\xe2\\x80\\x9924, 9 July 2024, Santa Cruz, CA\\nThis information allows us to identify and record the specific cores\\nactively engaged in the inference process at each time step. The total\\nenergy consumption for the inference task is then calculated by\\nsumming the power usage across all active cores and summing over\\nthe product of the power usage and time of inference, as follows:\\n\\xf0\\x9d\\x90\\xb8\\xf0\\x9d\\x91\\x87\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x91\\x99,\\xf0\\x9d\\x90\\xb6\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x88=\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\n\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x92\\n \\xe2\\x88\\x91\\xef\\xb8\\x81\\n\\xf0\\x9d\\x91\\x96\\n\\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x92,\\xf0\\x9d\\x91\\x96\\xce\\x94\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x96\\n!\\nwhere \\xf0\\x9d\\x91\\x83\\xf0\\x9d\\x91\\x90\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x91\\x92,\\xf0\\x9d\\x91\\x96represents the power draw of an individual core at\\neach time step, \\xf0\\x9d\\x91\\x96, with \\xce\\x94\\xf0\\x9d\\x91\\xa1\\xf0\\x9d\\x91\\x96being the time step size.\\n4\\nPROBLEM FORMULATION\\nThe purpose of developing workload-based models of LLM infer-\\nence is to create a framework that allows a data center operator\\nto navigate the trade-off between model accuracy and energy con-\\nsumption. To do so, we formalize an optimization problem below.\\nConsider a data center that hosts K = {1, . . . , \\xf0\\x9d\\x90\\xbe} distinct LLM\\nmodels. Assume that a fraction \\xf0\\x9d\\x9b\\xbe\\xf0\\x9d\\x90\\xbeof the inference workload is\\nassigned to each model \\xf0\\x9d\\x90\\xbe, where\\xf0\\x9d\\x9b\\xbe\\xf0\\x9d\\x90\\xbe\\xe2\\x88\\x88[0, 1], \\xe2\\x88\\x80\\xf0\\x9d\\x90\\xbeand \\xc3\\x8d\\n\\xf0\\x9d\\x90\\xbe\\xe2\\x88\\x88K \\xf0\\x9d\\x9b\\xbe\\xf0\\x9d\\x90\\xbe= 1.\\nWe denote a query \\xf0\\x9d\\x91\\x9eby its count of input and output tokens, \\xf0\\x9d\\x91\\x9e=\\n(\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1). A workload with\\xf0\\x9d\\x91\\x9aqueries is then a multiset\\xf0\\x9d\\x91\\x84\\xe2\\x88\\x88\\x00N2\\x01\\xf0\\x9d\\x91\\x9a.\\nAs our goal is to perform scheduling of each query, we must create\\na disjoint partition of our set \\xf0\\x9d\\x91\\x84. We say that each \\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x90\\xbe\\xe2\\x88\\x88\\x00N2\\x01\\xf0\\x9d\\x91\\x9a\\xf0\\x9d\\x90\\xbe\\nhas \\xf0\\x9d\\x91\\x9a\\xf0\\x9d\\x90\\xbeprompts and is composed of a set of lengths of input and\\noutput tokens \\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x90\\xbe=\\n\\x08\\n(\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b,1,\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1,1), . . . , (\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x91\\x96,\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1,\\xf0\\x9d\\x91\\x96)\\n\\t\\n.\\nSince this is an offline setting we assume we have perfect knowl-\\nedge of our system, including the number of output tokens that a\\ngiven input prompt will produce. In reality, this is not known ab\\ninitio though work by Zheng et al. [47] has shown that the number\\nof output tokens can be reasonably well estimated by analyzing\\npast input-output pairs.\\nFor optimization purposes, we must define a function based on\\nthe constant \\xf0\\x9d\\x90\\xb4\\xf0\\x9d\\x90\\xbefrom Table 1. We propose \\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x90\\xbe: N2 \\xe2\\x86\\x92[0, \\xe2\\x88\\x9e), a\\nmonotonically increasing function based on the number of input\\nand output tokens that a model \\xf0\\x9d\\x90\\xbeingests and produces. Therefore,\\nfor a model \\xf0\\x9d\\x90\\xbeprocessing tokens (\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1) we have\\n\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x90\\xbe(\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1) = \\xf0\\x9d\\x90\\xb4\\xf0\\x9d\\x90\\xbe\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b+ \\xf0\\x9d\\x90\\xb4\\xf0\\x9d\\x90\\xbe\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1.\\n(1)\\nAs we will later derive, we denote a model for energy consumption\\nfor a given number of input and output tokens as \\xf0\\x9d\\x91\\x92\\xf0\\x9d\\x90\\xbe(\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x91\\x96,\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1,\\xf0\\x9d\\x91\\x96) :\\nN2 \\xe2\\x86\\x92[0, \\xe2\\x88\\x9e).\\nBoth of these functions have a normalized counterpart c\\n\\xf0\\x9d\\x91\\x92\\xf0\\x9d\\x90\\xbe, c\\n\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x90\\xbe:\\nN2 \\xe2\\x86\\x92[0, 1] that scales the cost associated with these values [0, 1]\\nto make these different metrics comparable. We normalize by di-\\nviding by the largest known value of energy and accuracy prior to\\noptimization.\\nFinally, let \\xf0\\x9d\\x9c\\x81\\xe2\\x88\\x88[0, 1] denote a tuning parameter that lets a data\\ncenter operator trade off energy for accuracy. Let |\\xf0\\x9d\\x91\\x84| represent the\\ntotal number of queries in our workload, and |\\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x90\\xbe| represent the\\ntotal number of queries each model \\xf0\\x9d\\x90\\xbeprocesses.\\nWe now formulate our workload assignment problem as:\\nmin\\n\\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x90\\xbe\\xe2\\x88\\x88\\xf0\\x9d\\x91\\x84\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\n\\xf0\\x9d\\x90\\xbe\\xe2\\x88\\x88K\\n\\xe2\\x88\\x91\\xef\\xb8\\x81\\n(\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1)\\xe2\\x88\\x88\\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x90\\xbe\\n\\xf0\\x9d\\x9c\\x81c\\n\\xf0\\x9d\\x91\\x92\\xf0\\x9d\\x90\\xbe(\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1) \\xe2\\x88\\x92(1 \\xe2\\x88\\x92\\xf0\\x9d\\x9c\\x81)c\\n\\xf0\\x9d\\x91\\x8e\\xf0\\x9d\\x90\\xbe(\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1)\\n(2)\\ns.t., 0 < |\\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x90\\xbe|\\n|\\xf0\\x9d\\x91\\x84| < 1\\n(3)\\n\\xf0\\x9d\\x91\\x84=\\n\\xc3\\x98\\n\\xf0\\x9d\\x90\\xbe\\xe2\\x88\\x88K\\n\\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x90\\xbe\\n(4)\\n\\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x90\\xbc\\xe2\\x88\\xa9\\xf0\\x9d\\x91\\x84\\xf0\\x9d\\x90\\xbd= \\xe2\\x88\\x85, \\xf0\\x9d\\x90\\xbc\\xe2\\x89\\xa0\\xf0\\x9d\\x90\\xbd, \\xe2\\x88\\x80\\xf0\\x9d\\x90\\xbc, \\xf0\\x9d\\x90\\xbd\\xe2\\x88\\x88K,\\n(5)\\nwhere Equations 4 and 5 define the partition coverage of the work-\\nload, and Equation 3 ensures we give each LLM some queries. In\\nour implementation, we dynamically normalize our energy and\\naccuracy measures across all the queries to allow us to adjust the\\nscale of costs across different models and query combinations. This\\nproblem is computationally intensive to solve as it is an example of\\na general assignment problem which are known to be NP-hard [7].\\n5\\nLLM INFERENCE PERFORMANCE\\nAll hardware information we state in Section 3.2. We use Ubuntu\\n20.04 with Python 3.12.0, PyTorch v2.0.1, Torchvision v0.15.2, Numpy\\nv1.26.0, Hugging Face v0.20.2, and Accelerate v0.26.1.\\n5.1\\nExperimental Strategy\\nWe conduct an experimental campaign to evaluate the performance\\nof differing workloads across various models. We systematically\\nvaried the number of input and output tokens to measure their\\neffects on runtime and energy consumption under two main ex-\\nperimental conditions. In each experiment we do not allow for\\nkey-value caches [41] to be re-used to ensure our measurements\\nare standard between iterations. We fix the batch size at 32.\\n5.1.1\\nVary Input Tokens. For the first experimental condition, we\\nexecuted inference requests with increasing the number of input\\ntokens, ranging from 8 to 2048 tokens, while maintaining a fixed\\noutput token size of 32. This setup allowed us to isolate the impact\\nof input size on the system\\xe2\\x80\\x99s performance and energy efficiency.\\n5.1.2\\nVary Output Tokens. In the second set of experiments, we\\nvaried the output token limit from 8 to 4096 tokens, keeping the\\nnumber of input tokens constant at 32. This approach helped us\\nunderstand how increasing output demands affect the runtime and\\nenergy consumption of the systems tested.\\n5.1.3\\nRandomization and Stopping Criteria. Each experiment was\\nconducted in a randomized order to mitigate any potential bias\\nintroduced by the sequence of tests. Also, we repeated trials until\\neither of two conditions was met: (i) the measured runtime was\\nwithin 0.5 seconds of the actual mean runtime with 95% confidence;\\nand (ii) a maximum of 25 trials were conducted for each setting if\\nthe first condition could not be met.\\n5.2\\nInput Token Effects\\nFigure 1 presents the impact of varying numbers of input tokens on\\nthe runtime, throughput, and energy per token for various LLMs.\\nThe results depict a clear trend: as the number of input tokens\\nincreases, the runtime tends to increase, while the throughput\\n', b'HotCarbon\\xe2\\x80\\x9924, 9 July 2024, Santa Cruz, CA\\nGrant Wilkins, Srinivasan Keshav, and Richard Mortier\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n210\\n211\\nNumber of Input Tokens\\n100\\n101\\n102\\nRuntime (s)\\n(a) Runtime\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n210\\n211\\nNumber of Input Tokens\\n101\\n102\\n103\\nThroughput (tokens/s)\\n(b) Throughput\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n210\\n211\\nNumber of Input Tokens\\n100\\n101\\n102\\nEnergy per Token (J/tokens)\\nFalcon (7B)\\nFalcon (40B)\\nLlama-2 (7B)\\nLlama-2 (13B)\\nLlama-2 (70B)\\nMistral (7B)\\nMixtral (8x7B)\\n(c) Energy per Token\\nFigure 1: Model performance against number of input tokens. Low variance renders error bars invisible.\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n210\\n211\\n212\\nNumber of Output Tokens\\n10\\xe2\\x88\\x921\\n100\\n101\\n102\\n103\\n104\\nRuntime (s)\\n(a) Runtime\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n210\\n211\\n212\\nNumber of Output Tokens\\n10\\xe2\\x88\\x921\\n100\\n101\\n102\\n103\\nThroughput (tokens/s)\\n(b) Throughput\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n210\\n211\\n212\\nNumber of Output Tokens\\n10\\xe2\\x88\\x921\\n100\\n101\\n102\\n103\\nEnergy per Token (J/tokens)\\nFalcon (7B)\\nFalcon (40B)\\nLlama-2 (7B)\\nLlama-2 (13B)\\nLlama-2 (70B)\\nMistral (7B)\\nMixtral (8x7B)\\n(c) Energy per Token\\nFigure 2: Model performance against number of output tokens. Low variance renders error bars invisible.\\nplateaus, in accordance with a roofline model [44]. Specifically, the\\nruntime increase is most pronounced for larger models like Llama-\\n2 (70B) and Falcon (40B), likely due to the higher computational\\nburden these models sustain as they process more extensive input\\nsequences. The energy consumption per token demonstrates similar\\ntrends, with smaller models exhibiting lower energy per token\\ncompared to larger models.\\nAn outlier to all of these cases is Mixtral (8x7B), which has a\\nhigher throughput and energy efficiency compared to other large\\nmodels at larger token input sizes. This LLM\\xe2\\x80\\x99s sparse mixture-\\nof-experts architecture (SMoE) [14, 33] allows it to activate just\\n12B parameters on average by selecting two expert sub-models.\\nThis classification phase comes with an added runtime and energy\\noverhead, however, on larger prompts it regains its performance\\ncapabilities. Therefore, for SMoE one gets the accuracy advantages\\nof a large model for less energy and lower runtime than its denser\\ncounterparts.\\n5.3\\nOutput Token Effects\\nFigure 2 illustrates how changes in the number of output tokens\\naffect runtime, throughput, and energy consumption per token\\nacross different LLMs. Notably, the runtime exhibits a steep in-\\ncrease with larger output token sizes, which is consistent across all\\nmodels but is especially significant for the high-parameter models\\nsuch as Falcon (40B) and Llama-2 (70B). Throughput, decreases as\\nthe number of output tokens increases. This inverse relationship\\nhighlights the additional time required to generate each additional\\ntoken, which involves more extensive interaction between model\\nlayers and successive passes through the LLM to generate each\\ntoken [41]. Energy per token also increases with the number of out-\\nput tokens and number of parameters. This increase is particularly\\nsharp in higher-parameter models like Falcon (40B).\\nAgain, Mixtral (8x7B) demonstrates greater energy efficiency\\ncompared to its large parameter counterparts. Even in cases of high\\noutput token generation, an SMoE architecture can yield improve-\\nments in energy efficiency.\\n6\\nWORKLOAD-BASED MODEL FITTING\\nFrom the experimental results in Section 5 we can see that each LLM\\nhas a unique runtime and energy consumption characteristic that\\nis a function of the given workload. In this section we develop and\\napply these models to optimizing energy and runtime of serving\\nLLMs.\\n', b'Offline Energy-Optimal LLM Serving: Workload-Based Energy Models for LLM Inference on Heterogeneous Systems\\nHotCarbon\\xe2\\x80\\x9924, 9 July 2024, Santa Cruz, CA\\n6.1\\nIndependence of Input and Output Tokens\\nFrom observing our results, we explored whether the number of\\ninput and output tokens are independent in their effect on the\\nenergy consumption and runtime. The following table presents the\\nANOVA results for assessing the effects of the number of input\\ntokens, the number of output tokens, and their interaction on the\\ntotal energy consumption and runtime for LLM inference. To collect\\nthis data we perform a grid search from 8 to 2048, in increments of\\npowers of two, for the space of input and output tokens to eliminate\\nthe bias of holding the input or output size constant. This analysis\\nincludes data aggregated across all models in Table 1.\\nTable 2: ANOVA Results for LLM Energy Consumption and\\nRuntime\\nMetric\\nVariable\\nSum of Squares\\nF-statistic\\n\\xf0\\x9d\\x91\\x9d-value\\nEnergy (J)\\nInput Tokens\\n5.17 \\xc3\\x97 1010\\n15.86\\n3.79 \\xc3\\x97 10\\xe2\\x88\\x9217\\nOutput Tokens\\n4.13 \\xc3\\x97 1011\\n126.63\\n1.22 \\xc3\\x97 10\\xe2\\x88\\x9265\\nInteraction\\n1.18 \\xc3\\x97 1011\\n4.53\\n4.67 \\xc3\\x97 10\\xe2\\x88\\x9215\\nRuntime (s)\\nInput Tokens\\n3.43 \\xc3\\x97 105\\n12.97\\n2.34 \\xc3\\x97 10\\xe2\\x88\\x9214\\nOutput Tokens\\n2.78 \\xc3\\x97 106\\n104.98\\n4.56 \\xc3\\x97 10\\xe2\\x88\\x9260\\nInteraction\\n8.21 \\xc3\\x97 105\\n3.88\\n1.92 \\xc3\\x97 10\\xe2\\x88\\x9212\\nThe number of input tokens and number of output tokens both\\nindividually have a substantial impact on energy consumption and\\nruntime, with output tokens having a larger effect size as indicated\\nby the higher \\xf0\\x9d\\x90\\xb9statistic. Also, the interaction term shows that the\\ninput and output tokens depend on each other while impacting en-\\nergy consumption and runtime. The high \\xf0\\x9d\\x90\\xb9-statistics and extremely\\nlow \\xf0\\x9d\\x91\\x9d-values for these effects confirm their significance. Therefore,\\nwe conclude that there is dependence between the number of input\\nand output tokens for energy consumption and runtime.\\n6.2\\nModeling Energy and Runtime\\nWe use the results in Table 2 to guide the creation of models to\\npredict the energy consumption and runtime of LLMs for use in\\noptimization problems such as those discussed in Section 4.\\nFor accurate models based on the number of input and output\\ntokens there needs to be an interaction term that combines them. We\\ntherefore propose a model to describe the total energy consumption\\nfor a model \\xf0\\x9d\\x90\\xbeas a function of input and output tokens, \\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9band \\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1,\\nrespectively:\\n\\xf0\\x9d\\x91\\x92\\xf0\\x9d\\x90\\xbe(\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1) = \\xf0\\x9d\\x9b\\xbc\\xf0\\x9d\\x90\\xbe,0\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b+ \\xf0\\x9d\\x9b\\xbc\\xf0\\x9d\\x90\\xbe,1\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1+ \\xf0\\x9d\\x9b\\xbc\\xf0\\x9d\\x90\\xbe,2\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1,\\n(6)\\nwhere \\xf0\\x9d\\x9b\\xbc\\xf0\\x9d\\x90\\xbe,0, \\xf0\\x9d\\x9b\\xbc\\xf0\\x9d\\x90\\xbe,1, \\xf0\\x9d\\x9b\\xbc\\xf0\\x9d\\x90\\xbe,2 are parameters determined through ordinary\\nleast squares (OLS) regression for each model and system combina-\\ntion.\\nSimilarly, we propose the following model to describe the total\\nruntime for a model \\xf0\\x9d\\x90\\xbeas a function of input and output tokens, \\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\nand \\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1, respectively:\\n\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x90\\xbe(\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b,\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1) = \\xf0\\x9d\\x9b\\xbd\\xf0\\x9d\\x90\\xbe,0\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b+ \\xf0\\x9d\\x9b\\xbd\\xf0\\x9d\\x90\\xbe,1\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1+ \\xf0\\x9d\\x9b\\xbd\\xf0\\x9d\\x90\\xbe,2\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x96\\xf0\\x9d\\x91\\x9b\\xf0\\x9d\\x9c\\x8f\\xf0\\x9d\\x91\\x9c\\xf0\\x9d\\x91\\xa2\\xf0\\x9d\\x91\\xa1,\\n(7)\\nwhere \\xf0\\x9d\\x9b\\xbd\\xf0\\x9d\\x90\\xbe,0, \\xf0\\x9d\\x9b\\xbd\\xf0\\x9d\\x90\\xbe,1, \\xf0\\x9d\\x9b\\xbd\\xf0\\x9d\\x90\\xbe,2 are also unique to each model \\xf0\\x9d\\x90\\xbe.\\nUsing the statsmodel (v0.14.2) Python package and its OLS API,\\nwe can determine the values of \\xf0\\x9d\\x9b\\xbc\\xf0\\x9d\\x90\\xbe,\\xf0\\x9d\\x91\\x96and \\xf0\\x9d\\x9b\\xbd\\xf0\\x9d\\x90\\xbe,\\xf0\\x9d\\x91\\x97that best fit Equa-\\ntions 6 and 7 for each LLM, \\xf0\\x9d\\x90\\xbe. A summary of the quality of these\\nfits are included in Table 3. As we can see, this model has high\\nexplainability for the effect of input and output tokens on energy\\nand runtime for inference of these different LLMs.\\nTable 3: Summary of OLS Regression Results Across Models\\nLLM (# Params)\\nEnergy Model (\\xf0\\x9d\\x91\\x92\\xf0\\x9d\\x90\\xbe)\\nRuntime Model (\\xf0\\x9d\\x91\\x9f\\xf0\\x9d\\x90\\xbe)\\n\\xf0\\x9d\\x91\\xb92\\nF-statistic\\n\\xf0\\x9d\\x91\\x9d-value\\n\\xf0\\x9d\\x91\\xb92\\nF-statistic\\n\\xf0\\x9d\\x91\\x9d-value\\nFalcon (7B)\\n0.964\\n681.2\\n2.53e-55\\n0.962\\n651.1\\n1.35e-54\\nFalcon (40B)\\n0.972\\n904.5\\n1.78e-60\\n0.976\\n1073.0\\n2.74e-63\\nLlama-2 (7B)\\n0.973\\n942.3\\n3.76e-61\\n0.972\\n1032.0\\n1.19e-62\\nLlama-2 (13B)\\n0.972\\n887.8\\n3.60e-60\\n0.972\\n907.0\\n1.60e-60\\nLlama-2 (70B)\\n0.976\\n1022.0\\n6.66e-62\\n0.980\\n1230.0\\n6.23e-65\\nMistral (7B)\\n0.975\\n997.0\\n1.70e-61\\n0.976\\n1039.0\\n3.62e-62\\nMixtral (8x7B)\\n0.980\\n1238.0\\n4.97e-65\\n0.992\\n3139.0\\n2.23e-80\\n6.3\\nApplying Our Models to Workload Routing\\nWe can now use our runtime and energy consumption models to\\nsolve the workload-aware routing problem outlined in Section 4.\\nUsing PuLP (v.2.8.0), a Python package designed for solving opti-\\nmization problems like that we formulate in Equation 2, we can\\nencode a workload of input and output tokens with a set of binary\\nvariables that indicate which model will process that pair of tokens.\\nThen, we convert the given constraints in Equations 3\\xe2\\x80\\x935 using this\\nformat and effectively route our workload to different models.\\nAs we show in Table 1 and Figures 1 and 2, an LLM with a larger\\nparameter count has greater accuracy but also greater runtime\\nand energy consumption for each input and output token. It is\\nreasonable to host differently sized models to allow us to serve\\ninference requests more runtime and energy efficiently with a trade-\\noff of slightly lower accuracy.\\nFor this example, we consider a data center serving the three\\nLlama-2 models of 7B, 13B, and 70B parameters. Assume that our\\nset K = {1, 2, 3} enumerates those models, respectively. A tunable\\nparameter that affects our optimization problem is the data center\\npartition \\xf0\\x9d\\x9b\\xbe\\xf0\\x9d\\x91\\x96. In our evaluation, we choose \\xf0\\x9d\\x9b\\xbe1 = 0.05,\\xf0\\x9d\\x9b\\xbe2 = 0.2, and\\n\\xf0\\x9d\\x9b\\xbe3 = 0.75.\\nWith this, we can use the model for energy consumption of\\neach LLM, \\xf0\\x9d\\x90\\xbe, in Equation 6 and our function to capture accuracy\\nfrom Equation 1 to calculate the costs associated with each query\\nand model as shown in Equation 2. For our sample workload, we\\nuse a subset of 500 queries from the Alpaca dataset [39], as it is a\\ncollection of 52002 queries with answers from GPT-4 [26].\\nFigure 3 shows the trade-offs in energy consumption, runtime,\\nand accuracy by varying the operational parameter \\xf0\\x9d\\x9c\\x81while routing\\nqueries to different models. We represent as constants (straight\\nlines) methods that do not use \\xf0\\x9d\\x9c\\x81, preferring to pick either a single\\nLLM or to use a simple query-independent mechanism to route a\\nquery to an LLM. The remaining non-constant line represents the\\ntrade-off our offline scheduler makes as it adjusts to changes in \\xf0\\x9d\\x9c\\x81.\\nIn Figure 3(a), we see that energy consumption is high when \\xf0\\x9d\\x9c\\x81is\\nlow because the system prioritizes accuracy over energy efficiency.\\nHigher \\xf0\\x9d\\x9c\\x81values lead to more energy-efficient routing decisions,\\nsacrificing accuracy for energy savings. Similarly, Figure 3(b) shows\\nthat the mean runtime per query decreases with increasing \\xf0\\x9d\\x9c\\x81. A\\nlow \\xf0\\x9d\\x9c\\x81value results in longer runtimes as the system routes queries\\nto models that provide higher accuracy but are less efficient in time\\nand energy. Conversely, higher \\xf0\\x9d\\x9c\\x81values result in shorter runtimes,\\n', b'HotCarbon\\xe2\\x80\\x9924, 9 July 2024, Santa Cruz, CA\\nGrant Wilkins, Srinivasan Keshav, and Richard Mortier\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n\\xed\\x9c\\x81\\n0\\n2\\n4\\n6\\n8\\n10\\nTotal Energy (kWh)\\n(a) Energy Consumption\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n\\xed\\x9c\\x81\\n0\\n25\\n50\\n75\\n100\\n125\\nMean Runtime (s)\\n(b) Mean Runtime\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n\\xed\\x9c\\x81\\n50.0\\n52.5\\n55.0\\n57.5\\n60.0\\n62.5\\n65.0\\nAccuracy (%)\\nLlama-2 (7B)\\nLlama-2 (13B)\\nLlama-2 (70B)\\nRound Robin\\nRandom\\nO\\xef\\xac\\x80line\\n(c) Accuracy\\nFigure 3: Behavior under offline simulation as \\xf0\\x9d\\x9c\\x81varies. Round-robin and Random query assignment are indistinguishable.\\nas the system favors more energy and time-efficient models over\\nthe most accurate ones. Figure 3(c) demonstrates the accuracy-cost\\ntrade-off, with small increases in accuracy requiring significant\\nincreases in runtime and energy consumption.\\nOur solution allows data center operators use \\xf0\\x9d\\x9c\\x81to navigate the\\ntrade-off space by, e.g., providing higher accuracy when energy\\nprices are lower, or delivering lower latency and lower energy\\nresponses during times of peak load albeit with slightly reduced\\naccuracy. This flexibility is important for adapting to different op-\\nerational scenarios.\\n7\\nCONCLUSIONS\\nIn this paper, we have examined the significant energy expenditure\\nof LLM inference. We show that modeling and optimizing the en-\\nergy consumption of LLM inference for a system is straightforward.\\nWe also showed that SMoE LLMs exhibit very promising energy\\nefficiency characteristics. Through our models of energy and run-\\ntime we contribute to the ongoing efforts towards sustainable AI\\nby providing a tunable optimization framework that allows for\\nsystem operators to trade-off energy and accuracy. We confirm our\\nhypothesis that there is potential for energy optimization using\\nmodels of energy and accuracy.\\nOf course, as many others have done [4, 10, 16, 20, 21, 36] we\\nhave used energy consumption as a proxy for carbon footprint. As\\npointed out by Kannan and Kremer [15], improving carbon effi-\\nciency and energy efficiency are distinct goals, yet they are related\\nand energy metrics can assist in understanding the magnitude of\\nemissions for a given application [1]. Our measurements of energy\\nconsumption are also based on a single node in an HPC setting and\\nso we cannot capture the runtime and energy overheads introduced\\nby faults, networking, and communications that would pertain at\\ndata center scale. We also disabled key-value caching [30] to estab-\\nlish a performance baseline; future work should explore the impact\\nof this and other optimizations. Finally, our workload-models are\\nspecific primarily to an NVIDIA A100 (40GB), as pointed out in\\nother studies there are large variations for the same inference task\\nacross hardware [35, 43].\\nWe hope that our energy models can be used in real-time systems\\nto reduce energy consumption dynamically. By integrating these\\nmodels into online scheduling algorithms, data centers can make\\nenergy-aware decisions based on the current workload and system\\nstate. This real-time optimization approach has the potential to\\nsignificantly improve the energy efficiency of LLM inference in\\nproduction environments. Similarly, including externalities like\\nenergy pricing and availability of sustainable energy into our model\\nwould bring systems closer to meeting sustainability goals.\\nACKNOWLEDGMENTS\\nWe gratefully acknowledge the computing resources provided on\\nSwing, a high-performance computing cluster operated by the Labo-\\nratory Computing Resource Center at Argonne National Laboratory.\\nDuring this work GW was supported by a Churchill Scholarship.\\nWe would like to thank the reviewers for their valuable feedback\\nto help improve our work.\\nREFERENCES\\n[1] Thomas Anderson, Adam Belay, Mosharaf Chowdhury, Asaf Cidon, and Irene\\nZhang. 2023. Treehouse: A Case For Carbon-Aware Datacenter Software. SIGEN-\\nERGY Energy Inform. Rev. 3, 3 (oct 2023), 64\\xe2\\x80\\x9370. https://doi.org/10.1145/3630614.\\n3630626\\n[2] Edward Beeching, Cl\\xc3\\xa9mentine Fourrier, Nathan Habib, Sheon Han, Nathan Lam-\\nbert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023.\\nOpen LLM Leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_\\nllm_leaderboard.\\n[3] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, and et al. 2022. On the Oppor-\\ntunities and Risks of Foundation Models. arXiv:2108.07258 [cs.LG]\\n[4] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and\\nRajini Wijayawardana. 2023. Reducing the Carbon Impact of Generative AI\\nInference (Today and in 2035). In Proceedings of the 2nd Workshop on Sustainable\\nComputer Systems (Boston, MA, USA) (HotCarbon \\xe2\\x80\\x9923). Association for Computing\\nMachinery, New York, NY, USA, Article 11, 7 pages.\\nhttps://doi.org/10.1145/\\n3604930.3605705\\n[5] Radosvet Desislavov, Fernando Mart\\xc3\\xadnez-Plumed, and Jos\\xc3\\xa9 Hern\\xc3\\xa1ndez-Orallo.\\n2023. Trends in AI inference energy consumption: Beyond the performance-vs-\\nparameter laws of deep learning. Sustainable Computing: Informatics and Systems\\n38 (2023), 100857. https://doi.org/10.1016/j.suscom.2023.100857\\n[6] Kaijie Fan, Marco D\\xe2\\x80\\x99Antonio, Lorenzo Carpentieri, Biagio Cosenza, Federico\\nFicarelli, and Daniele Cesarini. 2023. SYnergy: Fine-grained Energy-Efficient\\nHeterogeneous Computing for Scalable Energy Saving. In Proceedings of the\\nInternational Conference for High Performance Computing, Networking, Storage\\nand Analysis. 1\\xe2\\x80\\x9313.\\n[7] Marshall L Fisher, Ramchandran Jaikumar, and Luk N Van Wassenhove. 1986. A\\nmultiplier adjustment method for the generalized assignment problem. Manage-\\nment science 32, 9 (1986), 1095\\xe2\\x80\\x931103.\\n[8] Diandian Gu, Xintong Xie, Gang Huang, Xin Jin, and Xuanzhe Liu. 2023. Energy-\\nEfficient GPU Clusters Scheduling for Deep Learning. arXiv:2304.06381 [cs.DC]\\n[9] Sylvain Gugger, Lysandre Debut, Thomas Wolf, et al. 2022. Accelerate: Training\\nand inference at scale made simple, efficient and adaptable. https://github.com/\\nhuggingface/accelerate.\\n', b'Offline Energy-Optimal LLM Serving: Workload-Based Energy Models for LLM Inference on Heterogeneous Systems\\nHotCarbon\\xe2\\x80\\x9924, 9 July 2024, Santa Cruz, CA\\n[10] Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and\\nJoelle Pineau. 2020. Towards the Systematic Reporting of the Energy and Carbon\\nFootprints of Machine Learning. J. Mach. Learn. Res. 21, 1, Article 248 (jan 2020),\\n43 pages.\\n[11] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,\\nDawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Lan-\\nguage Understanding. In International Conference on Learning Representations.\\nhttps://openreview.net/forum?id=d7KBjmI3GmQ\\n[12] Qinghao Hu, Peng Sun, Shengen Yan, Yonggang Wen, and Tianwei Zhang. 2021.\\nCharacterization and prediction of deep learning workloads in large-scale GPU\\ndatacenters. In Proceedings of the International Conference for High Performance\\nComputing, Networking, Storage and Analysis (SC \\xe2\\x80\\x9921). Association for Computing\\nMachinery, New York, NY, USA, Article 104, 15 pages. https://doi.org/10.1145/\\n3458817.3476223\\n[13] Hongpeng Huo, Chongchong Sheng, Xinming Hu, and Baifeng Wu. 2012. An en-\\nergy efficient task scheduling scheme for heterogeneous GPU-enhanced clusters.\\nIn 2012 International Conference on Systems and Informatics (ICSAI2012). IEEE,\\n623\\xe2\\x80\\x93627.\\n[14] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche\\nSavary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou\\nHanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,\\nL\\xc3\\xa9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep\\nSubramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\\xc3\\xa9ophile Gervet,\\nThibaut Lavril, Thomas Wang, Timoth\\xc3\\xa9e Lacroix, and William El Sayed. 2024.\\nMixtral of Experts. arXiv:2401.04088 [cs.LG]\\n[15] Sudarsun Kannan and Ulrich Kremer. 2023.\\nTowards Application Centric\\nCarbon Emission Management. In Proceedings of the 2nd Workshop on Sus-\\ntainable Computer Systems (Boston, MA, USA) (HotCarbon \\xe2\\x80\\x9923). Association\\nfor Computing Machinery, New York, NY, USA, Article 5, 7 pages.\\nhttps:\\n//doi.org/10.1145/3604930.3605725\\n[16] Baolin Li, Yankai Jiang, Vijay Gadepally, and Devesh Tiwari. 2024. Toward Sus-\\ntainable GenAI using Generation Directives for Carbon-Friendly Large Language\\nModel Inference. arXiv:2403.12900 [cs.DC] https://arxiv.org/abs/2403.12900\\n[17] Baolin Li, Siddharth Samsi, Vijay Gadepally, and Devesh Tiwari. 2023. Clover:\\nToward Sustainable AI with Carbon-Aware Machine Learning Inference Service.\\nIn Proceedings of the International Conference for High Performance Computing,\\nNetworking, Storage and Analysis (SC \\xe2\\x80\\x9923). Association for Computing Machinery,\\nNew York, NY, USA, Article 20, 15 pages. https://doi.org/10.1145/3581784.3607034\\n[18] Pengfei Li, Jianyi Yang, Adam Wierman, and Shaolei Ren. 2024. Towards En-\\nvironmentally Equitable AI via Geographical Load Balancing. In Proceedings of\\nthe 15th ACM International Conference on Future and Sustainable Energy Systems\\n(Singapore, Singapore) (e-Energy \\xe2\\x80\\x9924). Association for Computing Machinery,\\nNew York, NY, USA, 291\\xe2\\x80\\x93307. https://doi.org/10.1145/3632775.3661938\\n[19] Qianlin Liang, Walid A Hanafy, Ahmed Ali-Eldin, and Prashant Shenoy. 2023.\\nModel-driven cluster resource management for ai workloads in edge clouds. ACM\\nTransactions on Autonomous and Adaptive Systems 18, 1 (2023), 1\\xe2\\x80\\x9326.\\n[20] Liuzixuan Lin and Andrew A Chien. 2023. Adapting Datacenter Capacity for\\nGreener Datacenters and Grid. In Proceedings of the 14th ACM International\\nConference on Future Energy Systems (Orlando, FL, USA) (e-Energy \\xe2\\x80\\x9923). As-\\nsociation for Computing Machinery, New York, NY, USA, 200\\xe2\\x80\\x93213.\\nhttps:\\n//doi.org/10.1145/3575813.3595197\\n[21] Liuzixuan Lin, Rajini Wijayawardana, Varsha Rao, Hai Nguyen, Emmanuel Wedan\\nGNIBGA, and Andrew A. Chien. 2024. Exploding AI Power Use: an Opportunity\\nto Rethink Grid Planning and Management. In Proceedings of the 15th ACM\\nInternational Conference on Future and Sustainable Energy Systems (Singapore,\\nSingapore) (e-Energy \\xe2\\x80\\x9924). Association for Computing Machinery, New York, NY,\\nUSA, 434\\xe2\\x80\\x93441. https://doi.org/10.1145/3632775.3661959\\n[22] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2023. Es-\\ntimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model.\\nJournal of Machine Learning Research 24, 253 (2023), 1\\xe2\\x80\\x9315. http://jmlr.org/papers/\\nv24/23-0069.html\\n[23] Timothy R. McIntosh, Teo Susnjak, Tong Liu, Paul Watters, and Malka N. Halga-\\nmuge. 2024. Inadequacies of Large Language Model Benchmarks in the Era of\\nGenerative Artificial Intelligence. arXiv:2402.09880 [cs.AI]\\n[24] Xinxin Mei, Xiaowen Chu, Hai Liu, Yiu-Wing Leung, and Zongpeng Li. 2017.\\nEnergy efficient real-time task scheduling on CPU-GPU hybrid clusters. In IEEE\\nINFOCOM 2017-IEEE Conference on Computer Communications. IEEE, 1\\xe2\\x80\\x939.\\n[25] NVIDIA. Accessed 2024. NVIDIA-NVML. https://docs.nvidia.com/deploy/nvml-\\napi/index.html. Available online.\\n[26] OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, et al. 2023. GPT-4\\nTechnical Report. arXiv:2303.08774 [cs.CL]\\n[27] Pratyush Patel, Esha Choukse, Chaojie Zhang, \\xc3\\x8d\\xc3\\xb1igo Goiri, Brijesh Warrier,\\nNithish Mahalingam, and Ricardo Bianchini. 2024. Characterizing Power Man-\\nagement Opportunities for LLMs in the Cloud. In Proceedings of the 29th ACM\\nInternational Conference on Architectural Support for Programming Languages and\\nOperating Systems, Volume 3 (ASPLOS \\xe2\\x80\\x9924). Association for Computing Machinery,\\nNew York, NY, USA, 207\\xe2\\x80\\x93222. https://doi.org/10.1145/3620666.3651329\\n[28] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, \\xc3\\x8d\\xc3\\xb1igo Goiri,\\nSaeed Maleki, and Ricardo Bianchini. 2024.\\nSplitwise: Efficient generative\\nLLM inference using phase splitting. In ISCA. https://www.microsoft.com/en-\\nus/research/publication/splitwise-efficient-generative-llm-inference-using-\\nphase-splitting/\\n[29] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,\\nDaniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon Emissions\\nand Large Neural Network Training. arXiv:2104.10350 [cs.LG]\\n[30] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Brad-\\nbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2023. Efficiently\\nscaling transformer inference. Proceedings of Machine Learning and Systems 5\\n(2023), 606\\xe2\\x80\\x93624.\\n[31] PowerAPI. 2024. PyJoules: Python-based energy measurement library for various\\ndomains including NVIDIA GPUs. https://github.com/powerapi-ng/pyJoules.\\nAccessed: 2024-01-10.\\n[32] Ana Radovanovi\\xc4\\x87, Ross Koningstein, Ian Schneider, Bokan Chen, Alexandre\\nDuarte, Binz Roy, Diyue Xiao, Maya Haridasan, Patrick Hung, Nick Care, et al.\\n2022. Carbon-aware computing for datacenters. IEEE Transactions on Power\\nSystems 38, 2 (2022), 1270\\xe2\\x80\\x931280.\\n[33] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani\\nAminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. 2022. DeepSpeed-\\nMoE: Advancing Mixture-of-Experts Inference and Training to Power Next-\\nGeneration AI Scale. In Proceedings of the 39th International Conference on Machine\\nLearning (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaud-\\nhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato\\n(Eds.). PMLR, 18332\\xe2\\x80\\x9318346. https://proceedings.mlr.press/v162/rajbhandari22a.\\nhtml\\n[34] Lavanya Ramapantulu, Bogdan Marius Tudor, Dumitrel Loghin, Trang Vu, and\\nYong Meng Teo. 2014. Modeling the energy efficiency of heterogeneous clusters.\\nIn 2014 43rd International Conference on Parallel Processing. IEEE, 321\\xe2\\x80\\x93330.\\n[35] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas,\\nMichael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay\\nGadepally. 2023. From Words to Watts: Benchmarking the Energy Costs of Large\\nLanguage Model Inference. In 2023 IEEE High Performance Extreme Computing\\nConference (HPEC). 1\\xe2\\x80\\x939. https://doi.org/10.1109/HPEC58863.2023.10363447\\n[36] Satveer and Mahendra Singh Aswal. 2016. A comparative study of resource\\nallocation strategies for a green cloud. In 2016 2nd International Conference on\\nNext Generation Computing Technologies (NGCT). 621\\xe2\\x80\\x93625. https://doi.org/10.\\n1109/NGCT.2016.7877487\\n[37] Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo Goiri, and Josep Torrellas.\\n2024. Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of\\nLLM Inference. arXiv:2403.20306 [cs.AI]\\n[38] Xiaoyong Tang and Zhuojun Fu. 2020. CPU\\xe2\\x80\\x93GPU utilization aware energy-\\nefficient scheduling algorithm on heterogeneous computing systems. IEEE Access\\n8 (2020), 58948\\xe2\\x80\\x9358958.\\n[39] R. Taori, I. Gulrajani, T. Zhang, and et al. 2024. Stanford alpaca: An instruction\\nfollowing llama model. https://github.com/tatsu-lab/stanford_alpaca. Accessed:\\n2024-01-15.\\n[40] Google Gemini Team. 2024. Gemini: A Family of Highly Capable Multimodal\\nModels. arXiv:2312.11805 [cs.CL]\\n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. 2017. Attention is all you\\nneed. In Proceedings of the 31st International Conference on Neural Information\\nProcessing Systems (Long Beach, California, USA) (NIPS\\xe2\\x80\\x9917). Curran Associates\\nInc., Red Hook, NY, USA, 6000\\xe2\\x80\\x936010.\\n[42] Yuxin Wang, Yuhan Chen, Zeyu Li, Zhenheng Tang, Rui Guo, Xin Wang, Qiang\\nWang, Amelie Chi Zhou, and Xiaowen Chu. 2024. Towards Efficient and Reliable\\nLLM Serving: A Real-World Workload Study. arXiv:2401.17644 [cs.DC]\\n[43] Grant Wilkins, Srinivasan Keshav, and Richard Mortier. 2024. Hybrid Heteroge-\\nneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads.\\nIn Proceedings of the 15th ACM International Conference on Future and Sustainable\\nEnergy Systems (e-Energy \\xe2\\x80\\x9924). Association for Computing Machinery, New York,\\nNY, USA, 506\\xe2\\x80\\x93513. https://doi.org/10.1145/3632775.3662830\\n[44] Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: an\\ninsightful visual performance model for multicore architectures. Commun. ACM\\n52, 4 (apr 2009), 65\\xe2\\x80\\x9376. https://doi.org/10.1145/1498765.1498785\\n[45] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, and et al. 2022. Sustainable\\nai: Environmental implications, challenges and opportunities. Proceedings of\\nMachine Learning and Systems 4 (2022), 795\\xe2\\x80\\x93813.\\n[46] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.\\nHellaSwag: Can a Machine Really Finish Your Sentence? arXiv:1905.07830 [cs.CL]\\n[47] Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang\\nYou. 2023. Response Length Perception and Sequence Scheduling: An LLM-\\nEmpowered LLM Inference Pipeline. In Thirty-seventh Conference on Neural In-\\nformation Processing Systems. https://openreview.net/forum?id=eW233GDOpm\\n']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import fitz\n",
        "\n",
        "\n",
        "#TODO:  Function to extract text from a PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    #open an example pdf\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text_data = []\n",
        "    # Extract text from the first page\n",
        "    for page in doc: # iterate the document pages\n",
        "        text = page.get_text().encode(\"utf8\") # get plain text (is in UTF-8)\n",
        "        text_data.append(text)\n",
        "    return text_data\n",
        "\n",
        "# Extract text from all uploaded PDF files\n",
        "pdf_texts = {}\n",
        "# your code here...\n",
        "folder = \"/content/polio_llm/lab06/\"\n",
        "for filename in os.listdir(folder):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(folder, filename)\n",
        "        pdf_texts[filename] = extract_text_from_pdf(pdf_path)\n",
        "#Display the text from all the PDF files\n",
        "for pdf_file, text in pdf_texts.items():\n",
        "    print(f\"Text from {pdf_file}:\")\n",
        "    print(text)\n",
        "    print(\"-\"*200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGxPl1Pfu3nQ"
      },
      "source": [
        "### Creating an index of vectors to represent the documents\n",
        "\n",
        "To perform efficient searches, we need to convert our text data into numerical vectors. To do so, we will use the first step of the BERT transformer.\n",
        "\n",
        "Since our full pdf files are very long to be fed as input into BERT, we perform a step in which we create a structure where we associate a document number to its abstract, and in a separate dictionary we associate a document number to its full text.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def get_abstract_from_pdf_text(pdf_pages_text):\n",
        "    full_text = \"\".join([page.decode('utf-8', errors='ignore') for page in pdf_pages_text])\n",
        "\n",
        "    # Try to find \"Abstract\" followed by content until \"I. INTRODUCTION\", \"Index Terms\", or a new section heading\n",
        "    match = re.search(r'Abstract\\W*(.*?)(?=(?:Index Terms|I\\. INTRODUCTION|II\\. SYSTEM MODEL|\\n\\n[A-Z]+\\. ))', full_text, re.DOTALL | re.IGNORECASE)\n",
        "    if match:\n",
        "        abstract = match.group(1).strip()\n",
        "        # Clean up common PDF extraction artifacts\n",
        "        abstract = re.sub(r'[\\s\\W]*\\n[\\s\\W]*', ' ', abstract) # Replace multiple newlines/spaces with single space\n",
        "        abstract = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', abstract) # Join hyphenated words broken by newline\n",
        "        return abstract\n",
        "\n",
        "    # Fallback if the above pattern doesn't work (e.g., shorter papers, different formatting)\n",
        "    # Try to find the first significant block of text after 'Abstract' or 'Summary'\n",
        "    match_fallback = re.search(r'(?:Abstract|Summary)\\W*(.*?)(?:\\n\\n|\\Z)', full_text, re.DOTALL | re.IGNORECASE)\n",
        "    if match_fallback:\n",
        "        abstract = match_fallback.group(1).strip()\n",
        "        abstract = re.sub(r'[\\s\\W]*\\n[\\s\\W]*', ' ', abstract)\n",
        "        abstract = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', abstract)\n",
        "        return abstract\n",
        "\n",
        "    return \"Abstract not found.\"\n",
        "\n",
        "extracted_abstracts = {}\n",
        "# pdf_texts is available from a previous cell in the notebook\n",
        "for filename, pages_text_list in pdf_texts.items():\n",
        "    extracted_abstracts[filename] = get_abstract_from_pdf_text(pages_text_list)\n",
        "\n",
        "# Print the extracted abstracts for verification\n",
        "abstracts_dict = {}\n",
        "for filename, abstract in extracted_abstracts.items():\n",
        "    print(f\"--- Abstract from {filename} ---\")\n",
        "    print(abstract)\n",
        "    print(\"\\n\")\n",
        "    abstracts_dict[filename] = abstract\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pblKtg23Xp9",
        "outputId": "de6ad152-1669-4298-b731-727478b87902"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Abstract from paper6.pdf ---\n",
            "Large language model (LLM) has recently been considered a promising technique for many fields. This work explores LLM-based wireless network optimization via in-context learning. To showcase the potential of LLM technologies, we consider the base station (BS) power control as a case study a fundamental but crucial technique that is widely investigated in wireless networks. Different from existing machine learning ML) methods, our proposed in-context learning algorithm relies on LLM’s inference capabilities. It avoids the complexity of tedious model training and hyper-parameter fine-tuning, which is a well-known bottleneck of many ML algorithms. Specifically, the proposed algorithm first describes the target task via formatted natural language, and then designs the in-context learning framework and demonstration examples. After that, it considers two cases, namely discrete-state and continuous-state problems and proposes state-based and ranking-based methods to select appropriate examples for these two cases, respectively. Finally, the simulations demonstrate that the proposed algorithm can achieve comparable performance as conventional deep reinforcement learning (DRL) techniques without dedicated model training or fine-tuning. Such an efficient and low-complexity approach has great potential for future wireless network optimization.\n",
            "\n",
            "\n",
            "--- Abstract from paper9.pdf ---\n",
            "This paper introduces a method for personaliz ing energy optimization using large language models (LLMs combined with an optimization solver. This approach, termed human-guided optimization autoformalism, translates natural language speciﬁcations into optimization problems, enabling LLMs to handle various user-speciﬁc energy-related tasks. It allows for nuanced understanding and nonlinear reasoning tailored to individual preferences. The research covers common energy sector tasks like electric vehicle charging, HVAC control and long-term planning for renewable energy installations. This novel strategy represents a signiﬁcant advancement in context based optimization using LLMs, facilitating sustainable energy practices customized to individual needs.\n",
            "\n",
            "\n",
            "--- Abstract from example2.pdf ---\n",
            "Abstract not found.\n",
            "\n",
            "\n",
            "--- Abstract from paper0.pdf ---\n",
            "Large Language Models (LLMs) are undergoing a period of rapid updates and changes, with state of-the-art (SOTA) model frequently being replaced. When applying LLMs to a specific scientific field, it’s challenging to acquire unique domain knowledge while keeping the model itself advanced To address this challenge, a sophisticated large language model system named as Xiwu has been developed, allowing you switch between the most advanced foundation models and quickly teach the model domain knowledge. In this work, we will report on the best practices for applying LLMs in the field of high-energy physics (HEP), including: a seed fission technology is proposed and some data collection and cleaning tools are developed to quickly obtain domain AI-Ready dataset; a just-in-time learning system is implemented based on the vector store technology; an on-the-fly fine-tuning system has been developed to facilitate rapid training under a specified foundation model The results show that Xiwu can smoothly switch between foundation models such as LLaMA, Vicuna ChatGLM and Grok-1. The trained Xiwu model is significantly outperformed the benchmark model on the HEP knowledge Q&A and code generation. This strategy significantly enhances the potential for growth of our model’s performance, with the hope of surpassing GPT-4 as it evolves with the development of open-source models. This work provides a customized LLM for the field of HEP while also offering references for applying LLM to other fields, the corresponding codes are available on Github https://github.comzhang/zhengde0225/Xiwu Keywords LLM · Deep Learning · Artificial Intelligence · Particle Physics · HEP Corresponding Author: qfz@ihep.ac.cn Corresponding Author: yuancz@ihep.ac.cn arXiv:2404.08001v1  [hep-ph]  8 Apr 2024 Running Title for Header Figure 1: The hallucination of GPT-4 when answering domain questions 1 Introduction Large Language Models The Large Language Models (LLMs) such as GPT-4 [1] and LLaMA[2, 3] have exhibited capabilities beyond expectations in terms of general intent understanding, robust continuous dialogue, intelligent interaction correction, and adequate logical reasoning, actively propelling the shifts towards an intelligent paradigm in scientific research 2 Running Title for Header High Energy Physics High Energy Physics (HEP) is a crucial branch of physics that delves into the characteristics and interactions of the most basic elements of matter. It serves as an essential field for unraveling the universe’s underlying principles and laws. Creating Large Language Models (LLMs) for High Energy Physics (HEP) can play a pivotal role in streamlining research activities, including literature review, coding, data management, analysis of physics, interpretation of results, and manuscript composition. This advancement can free researchers from mundane tasks that demand less creativity, thereby amplifying the efficiency and productivity of scientific investigations Hallucinations However, the LLMs can suffer from hallucinations, i.e. “produce content that is nonsensical or untruthful in relation to certain sources” [4, 5], which is particularly harmful in scientific fields. As shown in Figure 1, even the most powerful LLM (GPT-4) can struggle with domain-specific questions or generating scientific code of HEP. The capability of general LLMs to precisely tackle specialized topics or develop code for focused areas may be constrained Way to Reduce Hallucinations There are two methods can enhance the model’s ability to handle domain-specific tasks: one is further training the model, including both secondary pre-training and fine-tuning, and the other is Retrieval Augmented Generation (RAG). The foundation models are often referred to as L0 (Level 0) models, such as LLaMA and GPT-3. Models fine-tuned for specific tasks based on the foundation model are called L1 models, such as LLaMA-Chat Vicuna [6] and GPT-3.5 [7], which is further adjusted for conversational tasks. Models that are further fine-tuned with domain-specific knowledge can be termed as L2 models HEP·Xiwu LLM Here, we introduce the Xiwu (/SeI\"wu:/), a L2 large language model that exhibits fewer hallu cinations in HEP field. The most significant features of this model are its basis flexibility and learnability. Basis flexible means that the foundational model can be changed, becoming increasingly powerful with the upgrade of open-source models. Learnable implies that once the foundational model is updated, it can quickly be taught domain knowledge. Specifically, a just-in-time learning system based on a vector store has been implemented, allowing for instant knowledge updates to the Xiwu at a low cost Why Consider Flexible Training a foundation LLM is extremely costly. For a specific scientific field, on one hand training LLM from scratch is unaffordable; on the other hand, even if trained LLM from scratch, it might not keep up with the pace of development of open-source models. \"Using the strengths of others to improve oneself,\" we believe that having a flexible foundational model is the best strategy to maintain the most advanced performance of the model at the lowest cost Contributions The contributions are as follows 1. The seed fission technology is proposed and realized, which has proven its effectiveness in acquiring training Q&A data related to the field with only one seed. It can be widely applied in various fields 2. A basis flexible LLM system has been strategized and implemented, evolving from the initial LLaMA to now support upgrades to LLaMA2 and Vicuna, and beyond 3. An just-in-time learning system based on RAG is realized, capable of acquiring knowledge instantly 4. Xiwu, the first LLM specilized for high energy physics outperforms the foundation model in accuracy for domain-specific knowledge question answering, and exceeds GPT-4 in BOSS (BESIII Offline Software System) code generation 5. The WebUI application has been deployed into HepAI platform, the related codes is open-sourced on GitHub 2 Related Work Language Models The introduction of deep learning has greatly propelled the development of language models. Early applications of neural networks to language models, such as Feedforward Neural Networks[8], laid the groundwork The emergence of RNN[9, 10, 11] and LSTM[12] models advanced language model by effectively capturing temporal dependencies. However, their inability to process sequences in parallel limited their efficiency. In 2017, the introduction of the Transformer model[13], with its self-attention mechanism, overcame these limitations, significantly boosting the handling of long-distance dependencies and computational efficiency, propelling the development of language models into a new era. Building on this architecture, BERT[14] and OpenAI’s GPT series[5, 4] were developed for substantial advancements in NLP tasks through pre-training and fine-tuning. Subsequent works [15, 16, 17, 18] further refined and enhanced these methodologies, leading to continuous performance improvements. In 2020, The release of GPT-3[19 a significantly larger laguage model, underscored the effectiveness of few-shot and zero-shot learning, prompting a 3 Running Title for Header significant trend toward model scaling. By 2022, the launch of ChatGPT[7] marked milestones in NLP, showcasing the potential of Large Language Models (LLMs) as efficient AI communicators. Moreover, the open-source model Vicuna[6], which is based on fine-tuning LLAMA[2], made a substantial contribution to the open-source community by achieving up to 90% of GPT-4’s[1] performance, sparking widespread research interest.[20, 21, 22, 23 LLMs in Specific-Domain Recent research has shown that BERT models, when fine-tuned for specific domains, can achieve significant performance improvements. This trend is exemplified by models such as BioBERT[24](targeted at the biomedical domain), SciBERT[25] (for scientific literature), and ClinicalBERT[26](for clinical data). However, the high costs associated with pre-training large-scale, domain-specific models like Galactica[27] and BloombergGPT[28 underscore the value of domain-specific fine-tuning as a more practical and cost-effective strategy. For instance Flan-PaLM[29] constructs a specialized large model for the clinical medicine domain by combining few-shot learning chain-of-thought (CoT) prompting strategies, self-consistency prompting, and instruction fine-tuning. WizardMath[30 fine-tunes LLAMA2[3] using the Reinforcement Learning from Evol-Instruct Feedback method to create a large model for mathematical reasoning. Meanwhile, SciGLM[31] develops a specialized large model for the scientific domain by fine-tuning with high-quality datasets built using a self-reflexive annotation framework. These studies showcase the potential and value of fine-tuning large language models for specific domains LLMs in HEP In the field of high-energy physics, ChATLAS has trained a domain-customized model using documents such as Twiki, ATLAS docs, and the E-group Archive, providing AI services to the collaboration group. AccGPT developed based on LLAMA, is used for accelerator auxiliary control at CERN. Based on ChatGPT, the ATLAS Open Data Higgs Analysis Guide can assist individuals interested in physics analysis who are not specialized in high-energy physics. LLMs are being experimented with in various types of task of HEP 3 Methodology 3.1 Overall Architecture Figure 2: The architecture of Xiwu large language model system The architecture of Xiwu LLM is shown in FIG. 2. It consists of four parts: data engine, large language model, memory based on external knowledge lib and intelligent agent with its interface Data Engine The data engine is primarily used to collect text data from eight fields related to HEP. The specific methods of data collection include seed fission technology, real concerns from ChatBot, knowledge from highly cited full-text literature, and a large number of literature abstracts. The data is cleaned and organized using LLM, and reviewed by professional staffs Xiwu LLM Based on the Level 0 Model LLaMA open-sourced by Meta, the Lmsys team has implemented a Level 1 Vicuna through secondary training. Currently, Xiwu is a Level 2 model trained on high-energy physics data based on Vicuna. With the upgrades of LLaMA and Vicucna, Xiwu’s current base model is Vicuna-1.5 (corresponding to LLaMA2), with parameter sizes of 7B and 13B. Plans are in place to adapt to even more advanced models 4 Running Title for Header Memory Module RAG has been widely used to mitigate hallucinations in LLMs, essentially enhancing the model’s capabilities through external knowledge library. Similarly, we vectorize HEP data using an Embedding model and store it in a vector database as the long-term memory for Xiwu. Additionally, the process of adding or modifying information in the vector database represents a low-cost, rapid, and just-in-time learning process Intelligent Agent The intelligent agent is structured around the Xiwu model, supplemented by an onboard knowl edge base. Utilizing our proposed HepAI Distributed Deployment Framework (HepAI-DDP), we’ve successfully implemented both an Application Programming Interface (API) and a ChatBot Web User Interface (WebUI). This advancement significantly simplifies the interaction process between the intelligent agent and its human counterparts be they scientists or developers Learning Loops There are two learning loops in the system. The first loop is a just-in-time learning system, wherein users can directly embed accurate information into the knowledge base via the WebUI or API. Xiwu then retrieves and synthesizes this information to provide answers to queries. The second loop involves stable, unchanging information being integrated into the HEP text datasets, which are subsequently used to fine-tune the weights of Xiwu model. The former loop facilitates the rapid assimilation of evolving information, while the latter progressively enables the model to \"understand\" knowledge 3.2 Datasets Figure 3: The data resources and acquisition methods. (a) Eight domains related to High Energy Physics that are of our concern; (b) Four methods employed to gather the dataset The FIG. 3 shows HEP text data from eight sub-domains and the data collection methods. As shown in FIG. 3 (a in order to ensure the diversity and breadth of the model, we extensively gathered Q&A pairs and literatures from eight HEP related domains, including particle physics, particle astrophysics, experimental physics, theoretical physics accelerator physics, synchrotron radiation, neutron science and computer science. Data of each field is collected using the methods shown in FIG. 3 (b) on the right. There are four methods we used to collect data, including seed fission technology, chat robot backend, full-text of highly-cited papers and abundant abstracts of the papers. Through those methods, totally 26k Q&A pairs for fine-tuning and 750M tokens for pre-training are collected and clearned up The details will be described below 3.2.1 Data from Seed Fission Technology Seed Fission The seed fission technology was proposed by us to quickly obtain related Q&A pairs in a specific domain. This technology allows a single seed such as Particle Physics, can fission into a multitude of diverse and in-depth Q&A pairs. By employing this technique, we successfully generated more than 1K Q&A pairs for each HEP-related domain as shown in FIG. 3 (a). After a thorough human review, we obtained about 8K high-quality data entries Fission Process As shown in FIG. 4, our approach involved designing three chat robots through prompt engineering Newbie, Expert and Checker. The Newbie asking questions with curiosity based on information. The Expert providing answers based on information and reliable sources as much as possible. The Checker is a topic supervisor that responsible for selecting topics and determining when to transition away from the current topic. The fission process is as follows 5 Running Title for Header Figure 4: The seed fission technology for getting diverse and in-depth data 1. The initial topic is considered a seed, such as \"Particle Physics,\" is given to the Newbie 2. The Newbie generates 2 to 10 random questions based on the input. A single input leading to multiple outputs is thus referred to as fission 3. These questions are then reduplicated and filtered by the Checker, selecting the most interesting ones, or deciding to exit the current topic to choose another candidate topic. The selected question is directed to the next robot, while the remaining questions go into a candidate question list 4. The Expert answers questions based on the knowledge base and search engine it is equipped with, and provides the source of the answers 5. The answer serves as input information for the Newbie, who then poses more questions Data Quality By using \"Particle Physics\" as the \"seed,\" 50 question-answer pairs and 2,822 candidate questions can be generated. Some samples are shown in FIG. 4, from just this seed alone, concepts related to high-energy physics such as fermions, spin, gravitational waves, and redshift can be derived, thus the data generated is diverse and in-depth Advantage The significant advantage of this technique is that it allows us to generate a large volume of relevant and diverse question-answer datasets with depth using just one topic as a guide Limitations Initially, a notable limitation is the tendency for both Newbie and Expert robots to become ensnared within similar topics. Subsequently, we introduced the Checker to oversee this process, effectively addressing the issue Another limitation is that the quality of the generated answers depends on the robot itself, and there is a possibility of generating incorrect or even harmful information, equipping the Expert with a trustworthy knowledge base, knowledge retrieval capabilities, and incorporating human verification can to some extent solve this problem 3.2.2 Data from Chat Robot HaiChat Sever We developed HaiChat [32] to provide generative model services for HEP researchers. The backend includes API for close models, local deployment of open-source models, and Xiwu model. High Energy Physics (HEP researchers utilize the HaiChat service in their daily work, generating real questions of interest and corresponding answers. This serves as an important data source for further improving the performance of the Xiwu model Data and Quality As of now, HaiChat has generated about 600,000 Q&A pairs, of which 8%, or 48,000, are related to high-energy physics. To ensure the quality and accuracy of the data, we employed a multi-step filtering and cleaning process. First, we utilized the ChatGPT model to automatically filter out irrelevant or inaccurate dialogues from the collected question-answer pairs. Then, we performed manual review and editing to further enhance the quality and usability of the data 3.2.3 Data from Highly Cited Papers Data Sources For the eight sub-domains mentioned above, We analyzed the citation data of papers over the past five years and used tools to download a carefully selected set of 20,000 papers 6 Running Title for Header HaiNougat PDF Parser Based on the Nougat [33] model, we trained a PDF parsing algorithm, HaiNougat [34 using a domain-specific dataset, which achieves more accurate parsing of formulas and tables of HEP Q&A Pairs Generation and Human Verification Based on the parsed data, we extracted the core contributions viewpoints, and results of the papers through prompt engineering, obtaining relevant question-and-answer pairs. To ensure quality, these were reviewed by humans Data Collected From those highly cited papers, about 8,000 Q&A pairs and 150M tokens were collected 3.2.4 Abstracts Data from arXiv Kaggle offers comprehensive coverage of research papers across various domains, but it does not provide the full text We downloaded approximately 2 million paper abstracts from the aforementioned eight related fields on Kaggle. The quality of these abstract data is very high, they can be directly used for the secondary pre-training of the Xiwu model 3.2.5 Final Dataset After data collection and cleaning, a AI-Ready dataset of HEP texts was ultimately formed that can be used for LLM training, which includes 750M tokens for secondary pre-training and 26k Q&A pairs for fine-tuning 3.3 Neural Networks 3.3.1 Model Basis Flexible Design Xiwu is positioned as a Level 2 LLM. With the goal of minimizing costs while maintaining advanced capabilities, it has been designed to be adaptable to foundational models of Level 0 or Level 1, including LLaMA, Vicuna, Chat-GLM, Grok-1 and more in the future. This adaptability is implemented in the code repository through an ’apis’ folder, where interfaces for any model have appropriate API adaptations. For more details, you can refer to the Github [35] repository Current Foundation Model The Vicuna [36] model was chosen as the foundational model after a comparative analysis, as its overall performance surpassed that of others like LLaMA-13B, LLaMA2-13B, Koala-13B, Oasst pythia-13B, Alpaca-13B, and ChatGLM-6B. Vicuna is a Level 1 model based on LLAMA, and currently, we are using Vicuna-1.5, which corresponds to LLAMA-2. In this model, the transformer architecture is employed. Compared to Recurrent Neural Networks (RNNs), the transformer model has proven to be superior in quality for many sequence-to sequence tasks while being more parallelizable Modeling of NLP Tasks The language modeling task is to assign a probability for the likelihood of a given word or a sequence of words) to follow a sequence of words. A sequence of tokens are passed to the embedding layer first followed by a positional encoding layer to account for the order of the word. Positional Encoding module injects some information about the relative or absolute position of the tokens in the sequence. The BPE (Byte-Pair Encoding) is used the size of vocabulary is 32000. The positional encoding, RoRE and Pre-LN (layer normalization) [37] is used The Pre RMS Norm is as follows RMS(x v u u t 1 N N X i=1 x2 i The SwishGLU is used as activation function, there are tree trainable weight matrix in SwishGLU, the formular of SwishGLU is SwishGLU = Swishβ(xW) ⊗xV The nheads is 32, the headdim is 128, the FFNdimension is 11008, the hdimension is 4096 3.3.2 Training Techniques During the training process, several advanced techniques are employed to enhance the performance and efficiency of the Xiwu model. These techniques, namely BF16 and TF32 mixed-precision training, FlashAttention, FSDP (Fully Sharded Data Parallelism), CPU-Offload, and Torchrun, played a crucial role in achieving improved training outcomes In this section, we provide a detailed explanation of these techniques and their effects 7 Running Title for Header Figure 5: The illustration algorithm components and training technologies Mixed-precision Training Mixed-precision training[38] involves performing computations using a combination of lower (BF16) and higher (TF32) precision formats. By utilizing BF16 for most of the computations and TF32 for certain critical operations, we were able to reduce memory usage, improve computational throughput, and accelerate the training process without sacrificing model accuracy FlashAttention FlashAttention [39] is a technique designed to enhance Flops (floating-point operations per second utilization during self-attention computations, a critical component in transformer-based models like Xiwu. It optimizes the memory access patterns and computation flow, resulting in improved computational efficiency and reduced training time FSDP Fully Sharded Data Parallelism (FSDP)[40] is a data parallelism technique that enables efficient parallel training across multiple devices by dividing the model’s parameters into shards and assigning each shard to a different device. By distributing the model’s parameters, FSDP reduces memory consumption per device, enables larger models to fit within device memory constraints, and facilitates seamless parallelization for faster and more scalable training CPU-Offload CPU-Offload[41, 42] is a strategy used to alleviate excessive GPU memory usage during training. By offloading certain computations to the CPU, we were able to free up GPU memory for more efficient memory utilization This approach allowed us to train Xiwu on GPUs without encountering memory limitations and improved the overall training performance Torchrun for Single-Node Multi-GPU Parallelism Torchrun is a utility that facilitates single-node multi-GPU parallelism in PyTorch[43], the framework used for developing Xiwu. By leveraging Torchrun, we achieved an effective distribution of computations across multiple GPUs within a single node, allowing for parallel training and accelerated model convergence Bitsandbytes Bitsandbytes[44] is an innovative software designed to address the substantial GPU memory require ments encountered by LLMs during inference. Employing vector quantization and mixed-precision decomposition Bitsandbytes substantially reduces memory consumption without compromising on performance. It enables the deploy ment of large-scale models, such as those with 175B parameters like OPT-175B/BLOOM[45], on consumer-grade GPU servers, presenting a highly efficient solution for managing LLMs 8 Running Title for Header Deepspeed The Deepspeed framework[46] is designed to accelerate deep learning training by utilizing mixed precision, gradient checkpointing, and offloading memory to the host CPU. It allows for larger models and batch sizes reducing memory requirements and improving training speed ZeRO Series Zero Redundancy Optimizer series techniques aim to reduce memory consumption during training by partitioning model weights and optimizer states across multiple devices or nodes. These techniques include ZeRO-2[47], ZeRO-3[47], and ZeRO-offload[48]. By leveraging memory optimization strategies such as activation checkpointing and optimizer state partitioning, ZeRO techniques enable training larger models that would otherwise exceed available memory limits. Adopting ZeRO techniques can potentially allow us to scale up Xiwu and achieve even better performance LoRA Low-Rank Adaptive technique[49] is a low-rank approximation technique that reduces the computational cost of large-scale language models. It achieves this by approximating the weight matrices with low-rank factors while preserving the model’s representational capacity. By leveraging the inherent redundancy in model parameters, LoRA significantly reduces both the memory and compute requirements, making training and inference more efficient These techniques collectively contributed to the enhanced performance and efficiency of Xiwu during the training process. By leveraging mixed-precision training, FlashAttention, FSDP, CPU-Offload, and Torchrun, we achieved improved memory utilization, accelerated training speed, and scalable parallelization. These advancements enable us to train Xiwu more effectively, ultimately leading to improved performance in high-energy physics question-answering tasks 3.3.3 Computing Power and Training Settings Computing Power We conducted full-scale training of the Xiwu-13B model using eight A100 GPUs, each with a memory capacity of 40 GB. Additionally, the Xiwu-7B model can be trained on two A100 GPUs with a memory capacity of 80 GB each Training Parameters The following parameters were carefully selected and tuned to optimize the training process and improve the performance of Xiwu Learning rate: 2e-5 Learning rate scheduler: cosine Weight decay: 0.0 Warmup ratio: 0.03 Per-device training batch size: 2 Gradient accumulation steps: 16 Learning Rate The learning rate determines the step size at which the model adjusts its weights during training A value of 2e-5 was chosen to strike a balance between ensuring stable convergence and preventing overshooting or getting stuck in sub-optimal local minima Learning Rate Scheduler The cosine scheduler gradually reduces the learning rate as training progresses. It helps the model to converge smoothly by reducing the learning rate towards the end of training. The cosine scheduler was selected due to its proven effectiveness in stabilizing training and improving the model’s generalization ability Weight Decay Weight decay is a regularization technique that adds a penalty to the loss function based on the magnitude of the model’s weights. A weight decay value of 0.0 indicates that no additional regularization is applied during training, allowing the model to fully utilize the available information in the training data Warmup Ratio The warmup ratio determines the proportion of training steps dedicated to gradually increasing the learning rate from zero to its initial value. A warm-up ratio of 0.03 ensures that the model’s learning rate ramps up slowly at the beginning of training, allowing the model to stabilize and avoid sudden, large weight updates Per-device Training Batch Size This parameter determines the number of samples processed in parallel on each device during training. A batch size of 2 per device strikes a balance between utilizing the available memory efficiently and maintaining reasonable training speed 9 Running Title for Header Gradient Accumulation Steps Gradient accumulation is a technique that helps simulate larger batch sizes by accumulating gradients over multiple smaller batches before performing weight updates. With 16 gradient accumulation steps, the model benefits from an effective utilization of computational resources while still obtaining accurate gradient estimates It is important to note that the presented configuration and training setup can be adjusted according to the available resources and specific requirements of different applications 3.4 Just-In-Time Learning System Limitations of Secondary Training Although secondary training of LLMs is effective, it is costly, inefficient, and training can lead to a degradation in model performance. For example, when dealing with unseen information, the model cannot effectively learn new information under the usual learning intensity (lower learning rates and fewer epochs). If the learning intensity is increased, the model can learn new information, but then issues such as performance degradation on other Q&A tasks and repetitive answer fragments may occur Just-In-Time Learning Just-In-Time Learning (JITL) is a learning system that utilizes a vector database for long-term memory enhancement to augment model capabilities, operating in a basic process equivalent to RAG. We call it JITL to emphasize its learning attributes. JITL is capable of rapid knowledge learning in seconds, specific knowledge updating and unloading. These features are not present in the traditional model training approach Figure 6: The just-in-time learning system Implementation of JITL The architecture of the Just-in-time Learning system is illustrated in FIG. 6, where the core concept is integrating an external knowledge base as the memory part of the model. New information is saved in the knowledge base, and after retrieval, comprehensive information is outputted. The complete information flow of the JITL system is as follows 1. When an expert user asks the model a specialized question related to HEP and receives a nonsensical answer the expert can suggest modifications or directly edit the answer, then teach the model via WebUI or API 2. The system categorizes and organizes the new information, storing it in a text database 3. The text data is processed by a pre-trained embedding model to generate feature vectors, which are then stored in a vector database. These feature vectors capture the semantic information of the text, and semantically similar expressions are close to each other in the feature space 10 Running Title for Header 4. When ordinary users ask the model questions, these go through a question classifier and select an appropriate model, while text data is converted into feature vectors by the embedding model. As different texts are separable in high-dimensional vector space, a simple similarity search can retrieve stored related texts 5. The related texts and query are processed through automatic prompt engineering and then fed into a suitable LLM such as Xiwu. Due to the intent understanding and contextual learning abilities of Xiwu, the final output is an accurate answer that also includes information sources Advantages Compared to training models, Just-In-Time learning systems only store vectors after embedding into the database, making the learning process extremely fast, achieving learning in seconds. Knowledge can be dynamically updated, which is particularly effective for rapidly changing information. Knowledge can also be specifically unloaded, a feature not achievable in traditional model training processes. Compared to conventional retrieval systems, Just-In-Time learning systems accomplish semantic-level rather than keyword-level retrieval, resulting in substantial performance improvements Limitations Just-In-Time learning systems have limitations. First, the additional vector database search process can be time-consuming, leading to higher latency compared to pure LLMs. One mitigation strategy is to use GPU acceleration to speed up the search process, another approach is to train the model with information that is always correct. The second limitation is that the model doesn’t truly \"understand\" the knowledge in the external database meaning there’s no improvement in reasoning ability. This mode is akin to open-book exams where students don’t have to understand the material but can look up information when needed 4 Evaluation 4.1 Evaluation between Models Evaluation Data To assess the performance of Xiwu in comparison to other models, we conducted a human evaluation using a set of 5,100 prompts. These prompts consisted of questions related to particle physics, astrophysics, synchrotron physics, and neutron science. Examples of these questions include: \"What quarks make up protons and neutrons What is general relativity?\", \"Why can’t we directly observe dark matter?\", and \"Why is particle therapy more expensive than X-ray treatment?\". The evaluation dimensions included accuracy, clarity, and fluency. Accuracy assessed whether the answers correctly addressed the questions and contained the necessary information. Clarity evaluated the clarity and comprehensibility of the answers, including the use of accessible language and terminology. Fluency assessed the overall fluency and coherence of the answers. Human scientists participated in the evaluation process and compared the answers generated by Xiwu with those from other models, namely Vicuna-13B and ChatGPT. The evaluation results were categorized into three outcomes: Xiwu wins, Xiwu fails, and Draw Comparison Results As shown in the FIG. 7, Xiwu-13B achieved a win or draw rate of 95% when compared to the baseline model Vicuna-13B. The performance of Xiwu-13B reached 65% of ChatGPT-175B, while Vicuna achieved less than 10%. These findings indicate that fine-tuning a large-scale model with domain-specific data, even on a model with 13 billion parameters, can significantly improve the accuracy of domain-specific question answering. Fine-tuning smaller models based on general-domain knowledge has the potential to achieve or even surpass the performance of larger models in specialized domains Figure 7: Human preference evaluation, Compare Xiwu-13B with Vicuna-13B and ChatGPT-175B Fine-tuning Cannot Learn New Knowledge Furthermore, we also tested the ability of Xiwu to incorporate internal knowledge that was not pre-trained. Examples of such knowledge included \"What is HepAI?\" (a high-energy physics artificial intelligence platform we are developing) and \"What is HaiGF?\" (an AI application interface framework we are 11 Running Title for Header developing). The results showed that a limited number of epochs were insufficient to guide Xiwu in answering these types of questions. Increasing the number of epochs not only failed to yield correct answers but also led to a decrease in the model’s performance on other questions 4.2 Absolute Evaluation Results In the absolute evaluation, human scientists assessed the content of each Xiwu answer, categorizing them as Excellent (Xiwu provided an outstanding response), Pass (Xiwu’s answer was acceptable), or Fail (Xiwu’s answer was incorrect). During the absolute evaluation, it was observed that occasionally the 13-billion-parameter Xiwu model produced answers that did not understand the human intent, resulting in responses that were unrelated to the questions Additionally, there were instances where the model repeated the same text. This behavior may be attributed to the lack of reinforcement learning from human feedback. Further investigation and incorporating reinforcement learning techniques are necessary to address these issues 5 Conclusion First LLM for HEP In this work, we developed Xiwu, the first large language model customized for high energy physics. Xiwu features a flexible foundational model and two distinct learning systems. The flexibility of the foundational model allows Xiwu to evolve alongside the development of open-source models. The learning system based on model training is traditional, enabling the model to effectively learn and \"understand\" HEP knowledge. The vector database-based just-in-time learning system enables rapid learning of new knowledge, dynamic knowledge updates, and unloading, all at a low cost Results Currently, Xiwu is an Level 2 LLM based on Vicuna. Xiwu-13B significantly outperforms the Vicuna-13B on the HEP domain Q&A test set, achieving about 65% of the performance of ChatGPT-175B. Furthermore, our just-in-time learning system enables multiple individuals to collectively teach Xiwu, demonstrating the potential for collaborative AI training. The seed fission technology we have developed has potential for wide application, capable of generating Q&A pair data in diverse fields Meaning This paper not only presents our findings but also provides the corresponding implementation code, which is readily available on GitHub [35]. We hope that our work will inspire further research and development in the application of large language models in specialized scientific fields Acknowledgments This work is Supported by the Informatization Plan of Chinese Academy of Science, Grant No. CAS-WX2022SF-0104 and \"From 0 to 1\" Original Innovation Project of IHEP, Grant No. E3545PU2. We would like to express our gratitude to Beijiang Liu, Yaquan Fang, Gang Li, Wuming Luo, Ye Yuan, Shengsen Sun, Yi Jiao and others who are not listed here for engaging in beneficial discussions or providing computing resources References 1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023 2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023 3] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models arXiv preprint arXiv:2307.09288, 2023 4] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019 5] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. OpenAI blog, 2018 12 Running Title for Header 6] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023 7] OpenAI. Introducing chatgpt, 2022 8] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language model. Advances in neural information processing systems, 13, 2000 9] Tomáš Mikolov et al. Statistical language models based on neural networks. Presentation at Google, Mountain View, 2nd April, 80(26), 2012 10] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In 2013 IEEE international conference on acoustics, speech and signal processing, pages 6645–6649 Ieee, 2013 11] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1700–1709, 2013 12] Alex Graves and Alex Graves. Long short-term memory. Supervised sequence labelling with recurrent neural networks, pages 37–45, 2012 13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017 14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018 15] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019 16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019 17] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020 18] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020 19] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020 20] Deepanway Ghosal, Yew Ken Chia, Navonil Majumder, and Soujanya Poria. Flacuna: Unleashing the problem solving power of vicuna using flan fine-tuning. arXiv preprint arXiv:2307.02053, 2023 21] Pritam Mukherjee, Benjamin Hou, Ricardo B Lanfredi, and Ronald M Summers. Feasibility of using the privacy-preserving large language model vicuna for labeling radiology reports. Radiology, 309(1):e231147, 2023 22] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023 23] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301, 2023 24] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 36(4):1234–1240, 2020 25] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676, 2019 26] Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and Matthew McDermott. Publicly available clinical bert embeddings. arXiv preprint arXiv:1904.03323, 2019 27] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022 13 Running Title for Header 28] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023 29] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13138, 2022 30] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023 31] Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, and Jie Tang. Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. arXiv preprint arXiv:2401.07950, 2024 32] Zhengde Zhang and Yiyu Zhang. Haichat: High energy physics generative artificial intelligence chat robot service https://chat.ihep.ac.cn, 2023. Accessed: 2024-04-07 33] Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418, 2023 34] Jianwen Luo and Zhengde Zhang. Hainougat: A academic document parser that preserves formulas and tables for high energy physics. https://ai.ihep.ac.cn/m/hai-nougat, 2024. Accessed: 2024-04-07 35] Zhengde Zhang, Yiyu Zhang, Haodong Yao, Jianwen Luo, Rui Zhao, Bo Huang, Jiaomeng Zhao, Yipu Liao Ke Li, Lina Zhao, Jun Cao, Fazhi Qi, and Changzheng Yuan. Xiwu: A basis flexible and learnable llm for high energy physics. https://github.com/zhangzhengde0225/Xiwu, 2024 36] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023 37] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in Neural Information Processing Systems, 34:19822–19835, 2021 38] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Gins burg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017 39] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022 40] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023 41] Bharadwaj Pudipeddi, Maral Mesmakhosroshahi, Jinwen Xi, and Sujeeth Bharadwaj. Training large neural networks with constant memory using a new execution algorithm. arXiv preprint arXiv:2002.05645, 2020 42] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W Keckler. vdnn: Virtualized deep neural networks for scalable, memory-efficient neural network design. In 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 1–13. IEEE, 2016 43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, volume 32, pages 8026–8037. Curran Associates, Inc., 2019 44] Dettmers Tim, Belkada Mike, Lewis adn Younes, and Zettlemoyer Luke. bitsandbytes: Highly optimized bit and byte level operations for deep learning. https://github.com/TimDettmers/bitsandbytes, 2023 45] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman Castagné Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022 46] Microsoft. Deepspeed: A deep learning optimization library. https://github.com/microsoft/DeepSpeed 2023. Accessed: 2023-10-01 47] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. arXiv preprint arXiv:1910.02054, 2020 14 Running Title for Header 48] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. {Zero-offload}: Democratizing {billion-scale} model training. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pages 551–564, 2021 49] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021 15\n",
            "\n",
            "\n",
            "--- Abstract from paper3.pdf ---\n",
            "The growing demand for eﬃcient and scalable AI solutions has driven research into optimizing the performance and energy eﬃciency of computational infrastructures. The novel concept of redesigning inference clusters and modifying the GPT-Neo model oﬀers a signiﬁcant advancement in addressing the computational and environmental challenges associated with AI deployment By developing a novel cluster architecture and implementing strategic architectural and algorithmic changes the research achieved substantial improvements in throughput, latency, and energy consumption. The integration of advanced interconnect technologies, high-bandwidth memory modules, and energy-eﬃcient power management techniques, alongside software optimizations, enabled the redesigned clusters to outperform baseline models signiﬁcantly Empirical evaluations demonstrated superior scalability, robustness, and environmental sustainability, emphasizing the potential for more sustainable AI technologies. The ﬁndings underscore the importance of balancing performance with energy eﬃciency and provide a robust framework for future research and development in AI optimization. The research contributes valuable insights into the design and deployment of more eﬃcient and environmentally responsible AI systems 1 1 Optimizing LLM Inference Clusters for Enhanced Performance and Energy Efficiency Soka Hisaharo*, Yuki Nishimura, and Aoi Takahashi Abstract—The growing demand for efficient and scalable AI solutions has driven research into optimizing the performance and energy efficiency of computational infrastructures. The novel concept of redesigning inference clusters and modifying the GPT-Neo model offers a significant advancement in addressing the computational and environmental challenges associated with AI deployment. By developing a novel cluster architecture and implementing strategic architectural and algorithmic changes the research achieved substantial improvements in throughput latency, and energy consumption. The integration of advanced interconnect technologies, high-bandwidth memory modules, and energy-efficient power management techniques, alongside soft ware optimizations, enabled the redesigned clusters to out perform baseline models significantly. Empirical evaluations demonstrated superior scalability, robustness, and environmental sustainability, emphasizing the potential for more sustainable AI technologies. The findings underscore the importance of balancing performance with energy efficiency and provide a robust framework for future research and development in AI optimization. The research contributes valuable insights into the design and deployment of more efficient and environmentally responsible AI systems.\n",
            "\n",
            "\n",
            "--- Abstract from paper8.pdf ---\n",
            "Reproducible science requires easy access to data, especially with the rise of data-driven and increasingly complex models used within energy research. Too often however, the data to reconstruct and verify purported solutions in publications is hidden due to some combination of commercial, legal, and sensitivity issues. This early work presents our initial efforts to leverage the recent advance ments in Large Language Models (LLMs) to create usable and share able energy datasets. In particular, we’re utilising their mimicry of human behaviors, with the goal of extracting and exploring syn thetic energy data through the simulation of LLM agents capable of interacting with and executing actions in controlled environments We also analyse and visualise publicly available data in an attempt to create realistic but not quite exact copies of the originals. Our early results show some promise, with outputs that resemble the twin peak curves for household energy consumption. The hope is that our generalised approach can be used to easily replicate usable and realistic copies of otherwise secret or sensitive data CCS CONCEPTS Computing methodologies →Multi-agent systems; Natural language generation; • Security and privacy →Social aspects of security and privacy; • Information systems →Data ana lytics KEYWORDS Synthetic Data, Generative AI, Large Language Models, Household Electricity Consumption ACM Reference Format Mahathir Almashor, Yusuke Miyashita, Sam West, and Thi Van Dai Dong 2024. Can Private LLM Agents Synthesize Household Energy Consumption Data?. In The 15th ACM International Conference on Future and Sustainable Commonwealth Science and Industrial Research Organisation (CSIRO) Energy This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike International 4.0 License E-Energy ’24, June 04–07, 2024, Singapore, Singapore 2024 Copyright held by the owner/author(s ACM ISBN 979-8-4007-0480-2/24/06 https://doi.org/10.1145/3632775.3661993 Energy Systems (E-Energy ’24), June 04–07, 2024, Singapore, Singapore. ACM New York, NY, USA, 5 pages. https://doi.org/10.1145/3632775.3661993 1 INTRODUCTION The energy community is working tirelessly on the transition to renewables and there is a pressing need to share data across research organisations. However, the sharing of useful and contemporary data in many domains is often an exercise fraught with hurdles and competing motives. The barriers to effective sharing range from privacy and cyber-security concerns, through to competing commercial interests. Given the urgency and enormity of the task at hand, we need a way for partners and collaborators across the globe to effectively evaluate their solutions against agreed datasets It is in this spirit that we present this very early work exploring the generation of synthetic yet realistic data within the energy domain. Our aim is arrive at method to generate raw household energy consumption data that (i) reflects the demographics and behaviours of a particular geographic region; (ii) be realistic and emergent in a way that accounts for the seemingly stochastic nature of human actions; and (iii) be free of the encumbrances that typically prevent the wider dissemination of data between institutions To this end, we propose the use of private Large Language Mod els (LLMs) [3] in the synthesis of such data. We see a range of possibilities in the abilities of current models to drive interactions within multi-agent simulations, and want to leverage their unique so-called hallucinations to arrive at realistic behavioural patterns Put simply, we want our LLM-powered agents to “dream” about their day-to-day actions and organically arrive at energy consump tion patterns that mirror real-life. The reasons are as follows As was seen in [11], emergent behaviours were seen when the authors simulated a small town of 25 agents powered by ChatGPT 10]. This included an unscripted event where the agents created a mayoral election and then proceeded to realistically interact with one another about it. This is the same phenomena we wish to capture, where the agents can naturally vary their behaviours and activities during the day to form a better mimicry of real-life humans. This includes variations that account for their specific characterisations (i.e., identity), occupations, and interactions with immediate family members We use the term Private LLM both in terms of their localised nature, as well as their inherent privacy benefits. That is, rather than depend on cloud-based and costly implementations such as 664 E-Energy ’24, June 04–07, 2024, Singapore, Singapore Authors et al OpenAI’s ChatGPT, we strove for smaller models that can be run within any organisation’s infrastructure. While the performance of these local models may not be on par with their more famous brethren, better accessibility and lower costs presents a good counterbalance. In this way, the barrier to entry is lowered for most institutions when replicating our methods and results Private also refers to the ability to keep an organisation’s valuable intellectual property safer and only within its confines. There is no need to share text prompts, techniques and potential seed data with any third-party service as everything is run on-premise or within that organisation’s own cloud infrastructure There is another dimension which delves further into privacy and is the reason why we chose household energy consumption at the first attempt. Simply put, it is difficult to obtain current, usable and customisable datasets because it invariably impacts real-life privacy concerns. We cannot effectively measure the daily consump tion of a human household, let alone the hundreds or thousands within a geographical region. In commercial settings, there are legal constraints on the collection of consumption data [13], given the in trusiveness and sensitivities such gathering would entail. We often have to rely on observations, household demographics, presence of appliances, and wider energy usage that are entirely self-reported 1], with all the data quality issues that entails. In any case, electrical load is typically only measured at the meter, requiring heuristics or additional hardware to ascertain the presence and consumption of individual appliances. It is impractical to measure consumption of each appliance and outlet for an entire city, not to mention how in vasive and time-consuming this would be for any given household Our contributions may be summarised as follows A novel addition to an LLM-enabled simulation engine to gen erate emergent daily routines of multiple agents, and the subse quent extraction of corresponding energy patterns The customisation of an existing simulation engine with a pri vate LLM implementation called Mistral that can be run within localised infrastructure and without costly ongoing access fees Experimentation with our approach that exhibits promising abil ity to replicate the consumption patterns seen in publicly avail able datasets that were discovered and analysed Ethical Considerations No personally identifiable information (PII) or other sensitivities were found in the household electricity consumption data used beyond anonymised age-brackets, demographics and post-codes 2 BACKGROUND & RELATED WORK 2.1 Synthetic Energy Data The use of GANs for generating energy time series data has recently gained prominence, fueled by the increased use of time series data across various domains. One objective in generating time series is to accurately capture temporal dynamics. Some earlier approaches such as C-RNN-GAN [9] and RGAN/RCGAN [4] have been de veloped to learn the temporal variations of data. However, these approaches require real data for model train, which in turn poses the risk of leaking sensitive information in any generated outputs 00:00 01:00 02:00 03:00 04:00 05:00 06:00 07:00 08:00 09:00 10:00 11:00 12:00 13:00 14:00 15:00 16:00 17:00 18:00 19:00 20:00 21:00 22:00 23:00 Time 0.2 0.4 0.6 0.8 1.0 1.2 1.4 Energy Consumption (Kwh Daily Energy Consumption Random Household Aggregated Mean of All Household Figure 1: Snapshots of daily and aggregated mean of typical household energy consumption Our approach attempts to overcome this issue by conducting household activities within an LLM-powered simulation world, and subsequently extracting energy data from those activities. This essentially avoids the use of any real data altogether and thus minimising any privacy or sensitivity concerns. In other words there is little to no risk of inadvertently replicating a household’s exact consumption patterns, as the genesis of the data creation is entirely independent from any real-life source 2.2 LLM and LLM Agents Large Language Models, epitomised by groundbreaking models like OpenAI’s GPT-3[2], FaceBook’s Llama2[14] and Mistral[6] AI’s eponymous model have showcased strong abilities to interpret, gen erate, and simulate human-like text. It would be an understatement to say that LLM technologies have gained popularity recently, with various research looking at everything from teasing out hidden meanings in speech [5] to using their generation capabilities in software programming tasks [7 Beyond the obvious chat-bot category of applications, there is also a significant amount of research in LLM agents aimed at imitating human behavior. A primary example of this is Simulacra 11], which introduces a fusion between LLM and computationally interactive agents in a sandbox environment to enable believable simulations of human behavior. Similarly, MemGPT [8] uses LLM as an operating system, allowing it to think, reflect, and produce actions to interact with external devices. Here, we utilized the Mistral-7B private LLM to power the Simulacra agents instead of the original ChatGPT backend, due to cost and accessibility reasons 2.3 Benchmark Datasets The Smart Grid Smart City Customer (SGSC) [1] data was collected between 2010 and 2014 as part of a joint industry and government initiative. It was one of only a few openly available datasets on household energy consumption that we discovered. It contains 30-minute interval readings of electricity usage and generation measured in kWh) for 78,720 participating customers, of which we only had access to a subset of 13,735 households In addition, we also discovered the Solar Cities dataset [12 which contained energy consumption and generation information 665 Can Private LLM Agents Synthesize Household Energy Consumption Data E-Energy ’24, June 04–07, 2024, Singapore, Singapore for almost 38,000 homes in seven Australian cities, recorded at 30-minute intervals from 2005 to 2013. After an extensive pre processing step, we focused on only 4,332 households due to various data quality and accessibility issues. This data was collected mainly as part of a governmental initiative design to measure the impact of direct interventions on household usage patterns In much the same way, there is the possibility to design for and simulate interventions to consumption patterns within our pro posed approach. In [12], consumption was measured for households before and after an intervention such as the installation of solar panels, in an attempt to gauge their impacts to electricity demands Similarly, we can use our LLM-enabled approach to simulate the same objective, by introducing interventions and appropriate pric ing dynamics, and watching how the agents respond. There is also the ability to model the impact of incentives on the agents, and measure the organic spread of interventions introduced gradually Figure 1 shows the daily energy consumption for randomly cho sen household (in blue) as well as aggregated mean of all house holds in our dataset (in red). Daily energy consumption varies for each household, but we see the trend of two peaks in the morning around 8am and evening around 7pm. We see the typical morning and evening peaks of energy usage in the aggregated mean, which is a well-known phenomena observed in individual households and up to the wider grid. On the other hand we witness the expected variability in energy usage for a single household on any given day Figure 2: Example simulation step of two agents in their daily activities, with one using an electrical appliance (their TV 3 APPROACH The approach involves two stages, with the first stage consisting of running the simulacra, and the second involving the extraction of household energy data from the simulation using a variety of methods to perform this extraction. For each step of the simulation description of each persona’s actions and objects they are interact ing with is generated. They are in the format of \"Persona A is doing Action B at Location C\". From these, appliances which consumes energy are identified. The advantage of this two-stage approach is its applicability onto any other LLM simulator. That is, simply allow the LLM to record its actions at each step, and then proceed with extracting useful information from it 3.1 Private LLM in Simulacra Utilizing ChatGPT or any other API-based LLM can offer conve nience, reliability, and strong performance. However, this approach may entail transferring sensitive data and routing it through ex ternal services. In contrast, the deployment of Private/Local LLM enables the execution of the entire simulation within closed envi ronments, ensuring the security and privacy of the data We utilized Mistral-7B as it is one of the highest performing smaller models which are relatively fast when running iterative experiments, and more importantly, can be fit into more modest compute infrastructures. “7B” here refers to 7 Billion parameters with some LLM models reaching 65B and more. Suffice it to say for now that the smaller the number of parameters, the more manage able it is to run within an organisation’s infrastructure This smaller Mistral model is then plugged into the Simulacra platform [11] as its LLM engine. The authors recommend a ro bust LLM for the agents, ideally as proficient as or superior to ChatGPT-3.5 Turbo as using smaller models may degrade the sim ulation quality. However, we’ve found that they still exhibit the ability to generate reasonable plans and actions, resulting in a plau sible human-like energy patterns. For simplicity, we’ve reused the prompts within the original paper to condition our LLM agents 3.2 Energy Data Extraction 3.2.1 String Match. In this straightforward approach, we employ string matching to determine whether the descriptions of daily activities from the simulations include energy appliances. This baseline is perhaps the most accurate reflection of energy data extraction from agent activities. For example in Figure 2, the simu lation step is described as “Maria Lopez is watching TV@common room sofa, Klaus Mueller is having dinner@kitchen sink”, the ap pliance TV is easily matched and included as a proxy for energy use The identified list of appliances in the simulations are as follows TV, shower, refrigerator, toaster, cooking area, microphone, piano game console, computer desk, and computer So, for every mention of an appliance in each time-step’s de scription, we mark that as an active use of that appliance. A simple counting of all the appliances used in the household is performed for each time-step in the simulated day. This allows us to get a mea sure of energy consumption purely from the activities of agents with regards to appliances, as they go about their day 3.2.2 Other Approaches. In our quest to extract more accurate en ergy usage, we also experimented with Semantic Embedding, which employs a text encoder to match the description of LLM generated interactions against appliances based on semantic meaning. One advantage with this method is its ability to capture synonyms or phrasings describing the same appliance. For instance, when the de scription states “Maria is streaming on Twitch”, we can surmise that this would involve the use of either a game console or a computer This semantic embedding method may better capture such cases compared to basic text matching. However, setting a threshold for matching top-K text embeddings was found to be challenging and resulted in inaccuracies with the extraction of energy data We also experimented with using the LLM engine itself to in fer appliance usage. LLMs have the capability to extract potential appliance use even if they are not present in the environment, or if the LLM agents are not explicitly described as interacting with them. Thus, with a line like “Maria reads a book while listening to 666 E-Energy ’24, June 04–07, 2024, Singapore, Singapore Authors et al 0 5 10 15 20 Hours 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 State of Appliances Isabella House and Cafe Daily Usage 0 5 10 15 20 25 Hours 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 State of Appliances Maria and Klaus Sharehouse Daily Usage 0 5 10 15 20 25 Hours 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 State of Appliances Lin Household Daily Usage 0 5 10 15 20 25 Hours 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 State of Appliances Moreno Household Daily Usage Figure 3: Single day energy usage for four simulated households music”, the LLM may infer that an appliance like a radio or TV is also turned on for the purposes of that background music. However the challenge here is the occurrence of hallucinations, making it difficult to precisely control their outputs. After extensive analysis both Semantic and LLM methods were abandoned in favour of the basic but most accurate string matching technique 4 RESULTS The status of each appliance at every step is binary, indicating whether it is on or off. This information is consolidated across all appliances to generate discrete values for energy usage. Further more, we implemented a rolling mean with a 1-hour window to smooth out our data, aiming to alleviate the instantaneous rises and drops in energy usage, given that energy usage is treated in binary states. Figure 3 depicts the daily energy usage patterns of our four simulated households, which include (clockwise from top-left Isabella, Lin, Moreno, and Maria and Klaus. This is represented against the rolling mean of appliance states at each time-step As can be seen, the daily energy data for each household varies according to their routines, occupations, and lifestyles. This is ex actly the variety in energy usage patterns we were aiming for, which resembles the variances seen in recorded single-day usage seen in Figure 1. The energy usage reflects the simulated activities of the LLM agents. For example, the agent Isabella wakes up around 6 am opens her cafe at 8 am, works at the counter until 8 pm, closes the cafe, and goes to bed around 11 pm. These activities correspond to actions such as turning on the lights in the cafe, serving customers using the refrigerator, toaster, and cooking area, which collectively contributes to the overall energy consumption pattern Similarly, these variances are observed for the Lin and Moreno households. The unique shapes of each household’s curve indicates that both the prompt of the LLMs and the simulation environment influence their activities, resulting in their distinct energy consump tion curves. This suggests that by conditioning the prompt, one can generate tunable synthetic energy data for the desired household within the limits of the simulated environment Furthermore, to determine the average daily energy usage per person, the mean is calculated from the combined data by summing all the households’ consumption at each time-step. In diagram 4 we illustrate two peaks near morning and evening times, capturing the demand periods often seen on weekdays in many aggregated energy datasets. The fluctuations and small peaks during the day may arise from the nature of the simulation environments, such as Isabella running a cafe (which is counted against her household’s data) and LLM agents returning home for lunch breaks 5 CONCLUSION The proposed approach uses private LLMs to synthesize daily house hold energy consumption patterns. This is done with privacy in mind, and by using the emergent properties of LLMs to arrive at realistic datasets that can be then be freely shared amongst the energy community. The focus is also on using less computationally intensive and costly LLMs that allows for much easier adoption 5.1 Limitations & Future Work The simulation results heavily depend on the capabilities of the LLM as more advanced LLMs would lead to more realistic simulations and extracted energy data. While early results somewhat resembles the real data, the LLM simulation outputs only binary states of appliances, which doesn’t capture continuously varying loads well and cannot simulate the majority of detailed activities in the cities from which the real data was measured. The translation from state to actual usage remains a potential area for future research Figure 4: Aggregated mean of energy usage from all four households containing a total of eight agents 667 Can Private LLM Agents Synthesize Household Energy Consumption Data E-Energy ’24, June 04–07, 2024, Singapore, Singapore Generating more detailed energy data will require integrating more detailed interactions with appliances, simulating many more agents and will likely require integrating additional factors like climate, weather, traffic, seasons, demographics, and industries. Fur thermore, the simulation does not assess climate control systems e.g. air conditioning), lighting, or transport, despite their signifi cant electricity consumption at home. We posit that they could be treated as constant factors used while occupants are at home, thus not affecting the dynamic usage of appliances. Doubtlessly, there remains a host of further experimentation and rigorous analysis to prove the effectiveness of this approach. The hope is that the wider energy research community will see its value, and collaborate to generate publicly available realistic synthetic datasets ACKNOWLEDGMENTS This work was supported by resources provided by the Pawsey Supercomputing Centre with funding from the Australian Govern ment and the Government of Western Australia REFERENCES 1] Energy Australian Government Department of Climate Change. 2014. Smart-Grid Smart-City Customer Trial Data. https://www.data.gov.au/data/dataset/smart grid-smart-city-customer-trial-data Last Modified: 2022-04-11T01:46:18.101034 2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners arXiv:2005.14165 [cs.CL 3] Vinton G. Cerf. 2023. Large Language Models. Commun. ACM 66, 8 (July 2023 7. https://doi.org/10.1145/3606337 4] Cristóbal Esteban, Stephanie L Hyland, and Gunnar Rätsch. 2017. Real-valued medical) time series generation with recurrent conditional gans 5] Fan Huang, Haewoon Kwak, and Jisun An. 2023. Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech. In Companion Proceedings of the ACM Web Conference 2023 (WWW ’23 Companion). Association for Computing Machinery, New York, NY, USA, 294–297 https://doi.org/10.1145/3543873.3587368 6] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De vendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix and William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL 7] Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma, Barbara J. Ericson, David Weintrop, and Tovi Grossman. 2023. Studying the effect of AI Code Generators on Supporting Novice Learners in Introductory Programming. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI ’23 Association for Computing Machinery, New York, NY, USA, 1–23. https://doi org/10.1145/3544548.3580919 8] MemGPT Community. 2024. Introduction. https://memgpt.readme.io/docs/index 9] Olof Mogren. 2016. C-RNN-GAN: Continuous recurrent neural networks with adversarial training 10] OpenAI Blog. 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt 11] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simu lacra of Human Behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST ’23). Association for Computing Machinery, New York, NY, USA, 1–22. https://doi.org/10.1145/3586183.3606763 12] Saad Sayeef, Sam West, Stephen Lindsay, Brad Sparkes, and Kate Cavanagh 2013. Solar Cities Data Analysis Final Report https://publications.csiro.au rpr/pub?list=SEA&pid=csiro:EP137924&sb=RECENT&expert=false&n=5&rpp 25&page=1&tr=6&q=Solar%20Cities%20Data%20Analysis&dr=all Publisher Department of Resources, Energy and Tourism 13] René Schwermer, Jonas Buchberger, Ruben Mayer, and Hans-Arno Jacobsen 2022. Federated office plug-load identification for building management systems In Proceedings of the Thirteenth ACM International Conference on Future Energy Systems (e-Energy ’22). Association for Computing Machinery, New York, NY USA, 114–126. https://doi.org/10.1145/3538637.3538845 14] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos ale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucu rull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Ro driguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2 Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL 668\n",
            "\n",
            "\n",
            "--- Abstract from paper1.pdf ---\n",
            "With the ubiquitous use of modern large language models (LLMs) across industries, the inference serving for these models is ever expanding. Given the high compute and memory requirements of modern LLMs, more and more top-of-the line GPUs are being deployed to serve these models. Energy availability has come to the forefront as the biggest challenge for data center expansion to serve these models. In this paper, we present the trade-offs brought up by making energy efficiency the primary goal of LLM serving under performance SLOs We show that depending on the inputs, the model, and the service-level agreements, there are several knobs available to the LLM inference provider to use for being energy efficient We characterize the impact of these knobs on the latency throughput, as well as the energy. By exploring these trade offs, we offer valuable insights into optimizing energy usage without compromising on performance, thereby paving the way for sustainable and cost-effective LLM deployment in data center environments.\n",
            "\n",
            "\n",
            "--- Abstract from paper4.pdf ---\n",
            "Abstract not found.\n",
            "\n",
            "\n",
            "--- Abstract from paper7.pdf ---\n",
            "Both the training and use of Large Language Models (LLMs) require large amounts of energy. Their increasing popularity, therefore raises critical concerns regarding the energy efficiency and sus tainability of data centers that host them. This paper addresses the challenge of reducing energy consumption in data centers running LLMs. We propose a hybrid data center model that uses a cost-based scheduling framework to dynamically allocate LLM tasks across hardware accelerators that differ in their energy efficiencies and computational capabilities. Specifically, our workload-aware strat egy determines whether tasks are processed on energy-efficient processors or high-performance GPUs based on the number of in put and output tokens in a query. Our analysis of a representative LLM dataset, finds that this hybrid strategy can reduce CPU+GPU energy consumption by 7.5% compared to a workload-unaware baseline CCS CONCEPTS Computer systems organization →Heterogeneous (hybrid systems; • Hardware →Impact on the environment KEYWORDS sustainable computing, heterogeneous computing, large language models, artificial intelligence ACM Reference Format Grant Wilkins, Srinivasan Keshav, and Richard Mortier. 2024. Hybrid Het erogeneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads. In The 15th ACM International Conference on Future and Sustain able Energy Systems (E-Energy ’24), June 04–07, 2024, Singapore, Singapore ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3632775.3662830 1 INTRODUCTION Large Language Models (LLMs) such as OpenAI’s GPT-4 [24] and Google’s PaLM [4] have become emblematic of the AI revolution driving significant advancements not only in natural language un derstanding, generation, and translation but also in summarizing and contextualizing large volumes of textual data. Characterized by their extensive scale and depth, their deployment demands substan tial computational resources and hence poses significant challenges This work is licensed under a Creative Commons Attribution International 4.0 License E-Energy ’24, June 04–07, 2024, Singapore, Singapore 2024 Copyright held by the owner/author(s ACM ISBN 979-8-4007-0480-2/24/06 https://doi.org/10.1145/3632775.3662830 in terms of energy consumption and operational efficiency [38 The increasing application of LLMs across diverse sectors further compounds these challenges, because datacenters, which are re sponsible for a considerable portion of global electricity consump tion, must balance performance targets for LLM tasks running on heterogeneous hardware with the need for energy efficiency [7, 21 Increasing the energy efficiency of LLMs thus emerges as both a technical challenge and an environmental imperative [22 Traditional data center designs often struggle to best exploit the capabilities of heterogeneous hardware-based LLMs, particularly when trying to minimize energy consumption without sacrific ing output quality and latency [6]. However, this challenge also presents an opportunity to innovate in datacenter architecture and management. We show that by rethinking how GPU resources are allocated and managed, there is potential to significantly reduce the energy footprint of LLM deployments while maintaining or even enhancing computational performance We find that a dynamic task-scheduling model that assigns LLM tasks to GPUs based on the resulting energy efficiency can reduce overall energy. Moreover, implementing a workload-aware system for input and output token processing can further reduce energy usage. Thus, a hybrid datacenter task allocation model, which al locates different tasks to different hardware accelerators based on their system demands, can reduce the overall energy consumption of LLM inference compared to a workload-unaware baseline Our contributions are as follows 1) We analyze the energy consumption and runtime of several 7B-parameter LLMs’ across various hardware configurations 2) We propose and evaluate a workload-aware scheduler for LLMs that optimizes energy efficiency based on the size of input and output token loads, demonstrating a 7.5% decrease in energy consumption over non-workload-aware baselines 3) We release a comprehensive dataset and benchmark suite for evaluating the energy efficiency of LLM inference, enabling researchers and practitioners to assess the impact of their design choices Through these contributions, we hope to support more sustain able and cost-effective AI inference deployments The remainder of this paper is as follows: Section 2 provides background information on LLM inference and energy consump tion in AI systems. Section 3 formulates the problem and introduces our cost function. Section 4 details the methods used for bench marking LLM inference on diverse systems. Section 5 presents the performance results of LLM inference across multiple hardware con figurations. Section 6 proposes and evaluates our energy-optimal hybrid data center design. Finally, Section 7 discusses related works and Section 8 summarizes the conclusions of the paper 506 E-Energy ’24, June 04–07, 2024, Singapore, Singapore Grant Wilkins, Srinivasan Keshav, and Richard Mortier 2 BACKGROUND 2.1 Inference Using Large Language Models Transformer-based neural network architectures have led to im pressive gains in the performance of LLMs for language under standing and generation [5]. LLMs such as OpenAI’s GPT-4 [24 and Google’s Gemini [32] have demonstrated human-level profi ciency on many language benchmarks while requiring billions of parameters and massive datasets for training. The inference phase of LLMs involves utilizing a trained model to make predictions based on new, unseen data. Unlike the training phase, which is typically a one-time, compute-intensive process that occurs offline inference is an ongoing, real-time process that directly impacts end-user experiences [7]. This phase is critical as it represents the point at which AI capabilities become accessible to users Inference in LLMs can be computationally expensive due to sev eral factors: (1) Model Size: The sheer size of these models, often billions of parameters, necessitates significant computational power to process each query [38]. (2) Latency Expectations: Many appli cations based on LLMs, such as digital assistants, automated writing aids, and real-time translators, require low-latency responses [35 3) Scalability: The ability to scale inference operations to accom modate varying user demands without degradation in response times is crucial 2.2 Energy Consumption in AI Systems Recent reports have found that the computational requirements for state-of-the-art AI entail massive energy consumption and carbon emissions [7, 21, 26, 29, 38]. The energy intensity of AI systems can be broadly divided into the energy required for training versus inference after models are deployed [13]. Training complex models on massive datasets is an energy-intensive process, with estimates finding that training GPT-3 required 1,287 megawatt-hours of en ergy [26]. LLMs can also have huge emissions depending on deploy ment scale and hardware efficiency [29]. For example, over a year of use, inference by LLMs on cloud infrastructure can consume over 25× more energy than training a model [7]. Optimizing software and hardware specifically for AI workloads is thus essential [3 2.3 Heterogeneous Systems for Efficient Computing Modern systems demonstrate a complex interplay between scale architecture, workload behavior and efficiency objectives. The ar chitecture of compute nodes can significantly impact the energy efficiency and processing capabilities of large-scale computing sys tems [18]. Conventional server architectures based on multicore CPUs face energy proportionality and scalability limitations for modern data-intensive workloads [20]. Several researchers have explored heterogeneous server configurations to improve energy ef ficiency [12, 15, 16, 19]. Distributed solutions can translate to lower energy efficiency, as communication overheads dominate [9]. Still specialized clusters like NVIDIA’s DGX show 4x better performance per watt over conventional servers [30 3 PROBLEM FORMULATION To model the operational demands of a hybrid, heterogeneous data center hosting LLMs, we define a cost function to reflect the work load distribution across different systems. We define a cost function 𝑈(𝑚,𝑛,𝑠) that accounts for both energy consumption and runtime 𝑈(𝑚,𝑛,𝑠) = 𝜆𝐸(𝑚,𝑛,𝑠) + (1 −𝜆)𝑅(𝑚,𝑛,𝑠 where 𝑚and 𝑛denote the number of input and output tokens respectively. 𝜆∈[0, 1] is a tunable parameter that balances the weight of energy efficiency versus speed. 𝐸(𝑚,𝑛,𝑠) is the energy consumed by system 𝑠to process 𝑚input tokens and generate 𝑛 output tokens, measured in joules. 𝑅(𝑚,𝑛,𝑠) is the time required to process these tokens on system 𝑠, measured in seconds Our objective is to minimize the total cost across all tasks and systems min 𝑄𝑠}𝑠∈𝑆 𝑠∈𝑆 𝑚,𝑛)∈𝑄𝑠 𝑈(𝑚,𝑛,𝑠 1 s.t Ø 𝑠∈𝑆 𝑄𝑠= 𝑄 2 𝑠: 𝑄𝑠∩𝑄𝑠′ = ∅for 𝑠≠𝑠 3 where 𝑆is the set of all systems, 𝑄is the total set of queries, 𝑄𝑠is the subset of queries assigned to system 𝑠 This model ensures that each query is processed exactly once optimizing for energy efficiency or quick response times, depending on the operational needs, as parameterized by 𝜆. We note, however that certain systems may be better suited to specific tasks, based on the workload characteristics, such as the need for rapid response times. Adjustments in 𝜆allow the datacenter to shift its focus be tween minimizing energy consumption and reducing runtime as operational priorities change 4 METHODS Here, we describe the methods and tools we use to benchmark LLM inference. In all cases, we use Huggingface’s Accelerate [11] to stan dardize hardware optimization for inference across all platforms. T his library takes advantage of the available accelerator resources and shards models accordingly to minimize intermediate commu nication and maximize the distributed capabilities for computation across the devices 4.1 Model Selection Our study employs three 7B-parameter, open-source LLMs for their capabilities and ability to run on diverse hardware efficiently: (1 Falcon [2], (2) Llama-2 [33], and (3) Mistral [17]. These models were selected to represent a spectrum of architectures and training corpora. We subject each model to a series of standardized NLP tasks to evaluate their energy consumption during inference 4.1.1 Falcon. The Falcon (7B) [2] model utilizes multi-query atten tion, significantly reducing memory requirements and increasing processing speed. The model’s training on the bilingual RefinedWeb dataset enhances its applicability across diverse linguistic contexts 4.1.2 Llama-2. We select Llama-2 (7B) for its optimization in di alogue tasks and its improvements in safety and helpfulness. The 507 Hybrid Heterogeneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads E-Energy ’24, June 04–07, 2024, Singapore, Singapore model’s unique pretraining methodologies and advanced architec tural features, such as grouped-query attention, make it an ideal candidate for analyzing energy efficiency in complex language tasks 4.1.3 Mistral. We include Mistral (7B) [17] for its grouped-query attention and sliding window attention mechanisms, contributing to fast and efficient inference. Its superior performance in vari ous benchmarks, especially in reasoning, mathematics, and code generation, makes it an essential model for our analysis 4.2 Energy Profiling of Diverse Systems Depending on the platform, we profile each system’s energy con sumption during inference using customized setups that capture runtime and energy or power metrics. Here, we describe how we monitor the energy usage of NVIDIA GPUs, Apple Silicon CPU/GPU, Intel CPUs, and AMD CPUs 4.2.1 NVIDIA GPUs. We use PyJoules [27], a Python-based en ergy measurement library, to quantify the energy consumption associated with inference on NVIDIA GPUs. PyJoules provides an interface to NVML [23], providing a software-defined energy usage assessment for targeted NVIDIA devices. This tool offers real-time energy consumption of GPUs for a given tracked process, which is a critical component of our analysis given the GPU-heavy compu tation involved in LLM inference 4.2.2 Apple Silicon CPU/GPU. No standard energy measurement tools are available for profiling energy and power usage for Ap ple Silicon through an API like PyJoules or RAPL. Therefore, we employ a daemon-based approach to poll macOS’ powermetrics utility, providing a detailed view of the energy usage during model inference. To capture the energy consumption of the M1 GPU, we execute the powermetrics command through a Python subprocess This command returns the percentage of the CPU power each CPU top process uses and the total CPU and GPU power consumption in 200ms intervals. This interval was chosen after testing to find the finest granularity measurement without incurring a significant CPU overhead for the I/O of buffering the large powermetrics output into memory The energy monitoring is conducted concurrently with the LLM inference. A separate thread is dedicated to running the powermetrics command, ensuring real-time data collection. Post-inference, the collected data is processed to extract the recorded power data and then find the energy consumption through integration over the runtime. The GPU energy consumption, 𝐸𝑇𝑜𝑡𝑎𝑙,𝐺𝑃𝑈, is straightfor ward to calculate for each recorded power value, 𝑃𝐺𝑃𝑈,𝑖, at each timestep Δ𝑡𝑖 𝐸𝑇𝑜𝑡𝑎𝑙,𝐺𝑃𝑈 𝑖 𝑃𝐺𝑃𝑈,𝑖Δ𝑡𝑖 The CPU power draw data is less clear, as many processes run on the CPU. However, an \"energy impact factor\" through powermetrics allows us to infer how much power our Python inference process uses. Therefore, we calculate the CPU energy, 𝐸𝑇𝑜𝑡𝑎𝑙,𝐶𝑃𝑈, by mul tiplying 𝑃𝐶𝑃𝑈,𝑖by the \"energy impact factor,\" which we denote as 𝛼𝑖, at each timestep 𝐸𝑇𝑜𝑡𝑎𝑙,𝐶𝑃𝑈 𝑖 𝛼𝑖𝑃𝐶𝑃𝑈,𝑖)Δ𝑡𝑖 4.2.3 Intel CPUs. For Intel CPUs, we leverage PyJoules, a Python based energy measurement library similar to our approach for NVIDIA GPUs. This tool supports RAPL (Running Average Power Limit) interfaces, enabling us to obtain fine-grained energy con sumption data [36]. We focus on two primary RAPL domains: Pack age 0 and Package 1, which correspond to the entire CPU package’s energy consumption, including all cores in the package PyJoules allows us to capture the energy usage of these domains in real time, enabling us to profile the energy consumption specif ically during model inference tasks. To account for base energy consumption unrelated to our inference process, we conduct a pre analysis phase to measure the CPU’s average idle power draw. This idle measurement is then subtracted from the total energy con sumption during inference to accurately determine the net energy expenditure attributable to the inference process We instrument our code to query the RAPL readings at the start and end of the inference task, calculating the energy consumption as follows 𝐸𝑇𝑜𝑡𝑎𝑙,𝐶𝑃𝑈 𝑖 𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−0,𝑖−𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−0,𝐼𝑑𝑙𝑒 𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−1,𝑖−𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−1,𝐼𝑑𝑙𝑒 Δ𝑡𝑖 where 𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−0,𝑖and 𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−1,𝑖, represent the power draw from Package 0 and Package 1, respectively, and 𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−0,𝐼𝑑𝑙𝑒 and 𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−1,𝐼𝑑𝑙𝑒represent the average idle power draw of the CPU packages, respectively 4.2.4 AMD CPUs. We adopt a different strategy for AMD CPUs due to the absence of a Python API. Instead, we utilize AMD𝜇Prof’s timechart feature, which provides detailed power draw metrics for every core on the chip at fine-grained intervals. By polling AMD𝜇Prof at 100ms intervals, we can capture the power draw of each physical core throughout the model inference process To ensure we accurately attribute the energy consumption to our inference task, we monitor the CPU core residency through psutil This information allows us to identify and record the specific cores actively engaged in the inference process at each time step. The total energy consumption for the inference task is then calculated by summing the power usage across all active cores and summing over the product of the power usage and time of inference, as follows 𝐸𝑇𝑜𝑡𝑎𝑙,𝐶𝑃𝑈 𝑐𝑜𝑟𝑒 𝑖 𝑃𝑐𝑜𝑟𝑒,𝑖Δ𝑡𝑖 where 𝑃𝑐𝑜𝑟𝑒,𝑖represents the power draw of an individual core at each time step, 𝑖 5 LLM INFERENCE PERFORMANCE ON DIVERSE CLUSTERS 5.1 Hardware and Software Versions The systems we profile are shown in Table 1. We consider these sys tems as they demonstrate three prominent CPU manufactures and different generations of GPUs. We utilize PyTorch v2.0.1, Torchvi sion v0.15.2, Numpy v1.26.0, Huggingface v0.20.2, and Accelerate v0.26.1 508 E-Energy ’24, June 04–07, 2024, Singapore, Singapore Grant Wilkins, Srinivasan Keshav, and Richard Mortier System Name CPU GPU(s) per Node DRAM per Node VRAM per GPU Macbook Pro 10-core M1 Pro 14-core M1 Pro 32GB Swing AMD+A100 2×64-core AMD EPYC 7742 8×NVIDIA A100 1TB 40GB Palmetto Intel+V100 40-Core Intel Xeon 6148G 2×NVIDIA V100 376GB 16GB Table 1: Our System Configurations We note that the M1-Pro results only include the Llama-2 (7B and Mistral (7B) results, as Falcon (7B) generally did not complete tasks in less than two orders of magnitude greater runtime 5.2 Experimental Strategy To comprehensively evaluate the performance of different system configurations across various models, we conducted a series of controlled experiments. We systematically varied the number of input and output tokens to measure their effects on runtime and energy consumption under two main experimental conditions. In each experiment we do not allow for key-value caches to be re-used to ensure our testing environment is standardized 5.2.1 Vary Input Tokens. For the first experimental condition, we executed inference requests with increasing input token sizes, rang ing from 8 to 2048 tokens, while maintaining a fixed output token size of 32. This setup allowed us to isolate the impact of input size on the system’s performance and energy efficiency 5.2.2 Vary Output Tokens. In the second set of experiments, we varied the output token limit from 8 to 4096 tokens, keeping the input token size constant at 32. This approach helped us understand how increasing output demands affect the runtime and energy consumption of the systems tested 5.2.3 Randomization and Stopping Criteria. Each experiment was conducted in a randomized order to mitigate any potential bias introduced by the sequence of tests. To ensure the reliability of our results, we adhered to strict criteria for statistical confidence. Each configuration was tested repeatedly until either of two conditions was met: (1) The measured runtime had to be within 0.5 seconds of the actual mean runtime with 95% confidence. (2) A maximum of 25 trials were conducted for each setting if the first condition could not be met 5.3 Input Token Analysis Here, we present the impacts on runtime, energy consumption per token, and throughput for LLMs across different hardware config urations while varying the number of input tokens. We perform these experiments using the suite of systems outlined in Table 1 with the models outlined in Section 4.1. In our experiments on the Palmetto Intel+V100 system, the V100 GPU had an out-of-memory error beyond 1024 output tokens for Falcon (7B Our runtime measurements show a significant increase as in put tokens grow. As depicted in Figure 1(a), all systems exhibit a nonlinear escalation in runtime with increasing token counts, with the M1-Pro system showing the most significant magnitude. This trend highlights the computational burden imposed by larger input sizes, particularly on smaller systems that are not as well designed to handle extensive workloads For all systems, we notice that throughput follows a “roofline model\" with increasing input tokens [37]. Figure 1(b) illustrates these dynamics, indicating an increase in throughput for all systems until a certain point where inference becomes bound by compute and not by the overhead of the software, as described by roofline performance models [37 Energy efficiency varies markedly across different systems. The M1-Pro demonstrates consistently low energy consumption per to ken, particularly for smaller input sizes, as shown in Figure 1(c). This efficiency reflects the M1-Pro’s design optimization for low-power operations. In contrast, the Swing AMD+A100, while capable of handling more significant token inputs more efficiently, consumed more energy per token for small workloads yet became more en ergy efficient at larger input token sizes, underscoring a trade-off between workload size and energy efficiency 5.4 Output Token Analysis Here we examine the performance trends associated with increasing the number of output tokens for our LLMs and systems of interest specifically focusing on runtime, energy consumption per token and throughput. In our experiments, the M1-Pro also could not generate more than 512 output tokens without significant runtime penalties. For the Palmetto Intel+V100 system, the V100 GPU had an OOM error beyond 1024 output tokens for Falcon (7B) and for all models beyond 2048 tokens Runtime significantly increases with the number of output to kens across all systems. As illustrated in Figure 2(a), the escala tion in runtime is pronounced, particularly as the output token count reaches higher magnitudes. This increase is indicative of the substantial computational effort required by LLMs to generate successive tokens In Figure 2(b), we observe a decrease in throughput across all systems as the number of output tokens increases. This trend high lights the inherent computational complexity involved in generat ing larger sequences of tokens in LLM tasks. As the output token count grows, the system must process each additional token, re calculating the context and updating internal model states [34 This not only increases the total computation per query but also leads to a greater accumulation of processing time per token, which consequently lowers the overall throughput Energy consumption per token also shows an increasing trend as the number of output tokens grows. Displayed in Figure 2(c this trend underscores the energy-intensive nature of producing larger outputs. Systems such as the M1-Pro, while generally more energy-efficient, begin to consume more energy per token as output demands increase, reflecting the intensive processing involved in output generation 509 Hybrid Heterogeneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads E-Energy ’24, June 04–07, 2024, Singapore, Singapore 2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 Number of Input Tokens 10 1 10 0 10 1 10 2 10 3 Runtime (s a) Runtime 2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 Number of Input Tokens 10 0 10 1 10 2 10 3 Throughput (tokens/s b) Throughput 2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 Number of Input Tokens 10 0 10 1 10 2 Energy per Token (J/tokens System Swing AMD+A100 Palmetto Intel+V100 M1-Pro Model Falcon (7B Llama-2 (7B Mistral (7B c) Energy per Token Figure 1: Performance of Various Systems and Models for Processing Variable Input Tokens–Due to the low variance in the data, error bars are too small to be visible 2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 2 12 Number of Output Tokens 10 1 10 0 10 1 10 2 10 3 10 4 Runtime (s a) Runtime 2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 2 12 Number of Output Tokens 10 1 10 0 10 1 10 2 10 3 Throughput (tokens/s b) Throughput 2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 2 12 Number of Output Tokens 10 1 10 0 10 1 10 2 10 3 Energy per Token (J/tokens System Swing AMD+A100 Palmetto Intel+V100 M1-Pro Model Falcon (7B Llama-2 (7B Mistral (7B c) Energy per Token Figure 2: Performance of Various Systems and Models for Processing Variable Output Tokens–Missing data points in M1-Pro and Palmetto Intel+V100 are due to CUDA out of memory errors. Due to the low variance in the data, error bars are too small to be visible 5.5 Comparing the Input and Output Analyses When comparing Figure 1(a) and Figure 2(a), we observe that in creases in the number of output tokens result in a more considerable increase in runtime than increases in input tokens. The computa tional complexity of processing input tokens primarily involves encoding the input context, which occurs once per input sequence and follows a more linear computational trajectory. In contrast generating output tokens is inherently more complex and iterative Each new output token requires the model to run through all its layers to predict the next token based on an ever-expanding context which includes both the initial input and all previously generated tokens [34]. This ongoing computation involves recalculating atten tion across an increasing number of tokens, updating hidden states and generating a probability distribution over the vocabulary for each new token. Consequently, as the number of output tokens grows, the computational load increases significantly, leading to more significant runtime increases than processing input tokens The impacts on runtime also translate to the throughput, de picted in Figure 1(b) and Figure 2(b). There is a noticeable decline in throughput as output tokens increase, more so than input to kens. The decrease in throughput for output tokens is primarily due to the heightened computational requirements for generating subsequent tokens, where each token’s generation slows down as the sequence lengthens. Furthermore, the energy per token also increases as output tokens grow, as shown in our analysis. The energy required to generate each output token becomes significant due to longer passes through the transformer network. We contrast this with the energy consumption when processing input tokens which, despite increasing, does so at a less steep rate 6 ENERGY-OPTIMAL HYBRID DATACENTER FOR LLM INFERENCE Considering the performance results we collect from LLM inference across multiple systems, we notice that there is an energy-optimal way to construct a hybrid datacenter with a combination of M1 Pro’s and A100s. The intuition behind this is that the energy expended per token for the M1 Pro is lower than that of the A100 up to a certain point in the number of input and output tokens as seen in Figures 1(c) and 2(c). However, the energy efficiency characteristics are different when varying the number of input and output tokens and therefore, we will proceed with separate analyses 6.1 Number of Input Tokens Analysis Suppose we have a hybrid data center with M1-Pros and A100s Then, we have some workload for an LLM, a set of queries with some outputs. In such a configuration, we implement a scheduling heuristic based on a cutoff threshold, 𝑇𝑖𝑛, for input token length 510 E-Energy ’24, June 04–07, 2024, Singapore, Singapore Grant Wilkins, Srinivasan Keshav, and Richard Mortier This heuristic dictates that queries with 𝑛≤𝑇𝑖𝑛tokens are pro cessed on M1 Pro systems, which we have shown have good energy efficiency with handling smaller computational loads. Conversely queries with 𝑛> 𝑇𝑖𝑛tokens leverage the greater computational abil ity of A100 GPUs, which offer greater energy-per-token advantages for larger tasks despite their higher power usage. We point out that this is the same method mentioned in the problem formulation in Eqn. 1, where our queries 𝑄are partitioned into 𝑄𝑀1 and 𝑄𝐴100 strictly on input and output size To find an optimal threshold 𝑇𝑖𝑛empirically, we analyze the to ken distribution in prompts from the Alpaca [31] dataset, a bench mark dataset frequently used in model fine-tuning. This dataset comprises 52K prompts, offering a diverse range of lengths akin to a typical workload in systems like GPT-4 [24]. The distribution of input tokens, visualized in our analysis (see Fig. 3(a)), serves as a proxy for understanding the variegated nature of LLM workloads 0 20 40 60 80 100 Number of Input Tokens 0 2000 4000 6000 8000 Frequency a) Input Tokens 0 200 400 600 Number of Output Tokens 0 2000 4000 6000 8000 Frequency b) Output Tokens Figure 3: Distribution of Token Counts for Alpaca [31 The energy component of our cost function, split over the token threshold, is as follows 𝐸𝑇𝑜𝑡𝑎𝑙,𝑖𝑛 𝑇𝑖𝑛 𝑚=1 𝑚𝑓𝑖𝑛(𝑚)𝐸𝑀1,𝑖𝑛(𝑚 𝑀 𝑚=𝑇𝑖𝑛+1 𝑚𝑓𝑖𝑛(𝑚)𝐸𝐴100,𝑖𝑛(𝑚 where 𝐸𝑇𝑜𝑡𝑎𝑙,𝑖𝑛represents the total energy consumption for a given dataset of input lengths 𝑚with corresponding frequencies 𝑓𝑖𝑛(𝑚 and 𝐸𝑀1,𝑖𝑛(𝑚) and 𝐸𝐴100,𝑖𝑛(𝑚) denote the mean energy per token for varying the input token size for the M1-Pro and A100 systems respectively. Utilizing this model with our dataset enables the ap proximation of total energy consumption for various threshold settings, offering insights into the energy dynamics of hybrid dat acenter operation. In Figure 4, we show the energy and runtime simulation results of performing inference for the input token sizes from the Alpaca dataset Our findings indicate that a threshold of 32 tokens strikes an optimal balance, significantly reducing energy consumption by relegating the inference of shorter queries to the more energy efficient M1 Pro systems. This policy not only capitalizes on the inherent energy efficiency of the M1 Pro for smaller tasks but also reserves the computational might of the A100 for queries that necessitate its robust capabilities. However, it’s important to note that this energy optimization comes at the cost of increased runtime 6.2 Number of Output Tokens Analysis We want to use the same scheduling heuristic and performance model to determine a threshold 𝑇𝑜𝑢𝑡for the number of output 2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 Threshold 0.76 0.78 0.80 Total Energy (kWh M1-Pro Only Swing AMD+A100 Only Hybrid System a) Energy Consumption for Changing 𝑇𝑖𝑛 2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 Threshold 0.5 1.0 Runtime (s 1e7 M1-Pro Only Swing AMD+A100 Only Hybrid System b) Runtime for Changing 𝑇𝑖𝑛 Figure 4: Performance of Hybrid Datacenter for Input Tokens Processing Alpaca–Dashed line shows the value for using only one kind of hardware for inference tokens. Except this time, we have different frequencies 𝑓𝑜𝑢𝑡(𝑛 for the 𝑛output tokens and different mean energy per token for varying the output token size, 𝐸𝑀1,𝑜𝑢𝑡(𝑛) and 𝐸𝐴100,𝑜𝑢𝑡(𝑛). We also utilize the distribution of the number of output tokens in the Alpaca dataset (see Fig. 3(b)). We revise our performance model as follows 𝐸𝑇𝑜𝑡𝑎𝑙,𝑜𝑢𝑡 𝑇𝑜𝑢𝑡 𝑛=1 𝑛𝑓𝑜𝑢𝑡(𝑛)𝐸𝑀1,𝑜𝑢𝑡(𝑛 𝑁 𝑛=𝑇𝑜𝑢𝑡+1 𝑛𝑓𝑜𝑢𝑡(𝑛)𝐸𝐴100,𝑜𝑢𝑡(𝑛 As the M1 Pro could only generate up to 512 tokens of a response we only test𝑇𝑜𝑢𝑡up until this point. In Figure 5, we show the energy and runtime simulation results of performing inference for the input token sizes from the Alpaca dataset Fig. 5(b) and Fig. 2(c) assess the energy consumption and runtime implications of various threshold settings for output generation Our findings suggest that although higher thresholds may leverage the M1 Pro’s energy efficiency for smaller outputs, there is an opti mal point at 32 output tokens that minimizes energy consumption 6.3 Balancing Energy Efficiency and Runtime Performance Our analysis of both input and output token processing within a hybrid, heterogeneous datacenter framework has led to the identifi cation that with certain thresholds at 𝑇𝑖𝑛𝑝𝑢𝑡= 32 and 𝑇𝑜𝑢𝑡𝑝𝑢𝑡= 32 511 Hybrid Heterogeneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads E-Energy ’24, June 04–07, 2024, Singapore, Singapore 2 3 2 4 2 5 2 6 2 7 2 8 2 9 Threshold 0.66 0.68 0.70 Total Energy (kWh M1-Pro Only Swing AMD+A100 Only Hybrid System a) Energy Consumption for Changing 𝑇𝑜𝑢𝑡 2 3 2 4 2 5 2 6 2 7 2 8 2 9 Threshold 0.5 1.0 1.5 Runtime (s 1e7 M1-Pro Only Swing AMD+A100 Only Hybrid System b) Runtime for Changing 𝑇𝑜𝑢𝑡 Figure 5: Performance of Hybrid Datacenter for Output Tokens Processing Alpaca – Dashed line shows the value for using only one kind of hardware for inference we can strategically allocate tasks to M1 Pro systems or A100 GPUs based on token count, optimizing for energy efficiency Shifting the token distribution leverages the M1 Pro’s superior energy efficiency for input and output tasks up to the threshold beyond which we utilize the A100’s computational power. This policy saves energy as smaller-token tasks are handled by the more efficient M1 Pro for outputs up to the threshold. However, this energy optimization comes at the expense of increased runtime which is particularly noticeable in output token generation where the M1 Pro, despite its efficiency, does not match the A100’s speed The energy-runtime trade-off presents a favorable scenario for applications that have low runtime sensitivity. For instance, batch processing of LLM tasks, such as overnight data analyses or non time-critical computations, can benefit significantly from this energy efficient configuration. Similarly, free or not directly monetized services, where the cost of computation impacts operational sus tainability, stand to gain from minimizing energy expenditures even at the cost of longer processing times This approach also opens discussions on Quality of Service (QoS for LLMs, an area that still needs to be explored [1, 35]. Traditional QoS metrics often prioritize speed and reliability, but energy effi ciency may also become a critical QoS dimension for LLM applica tions, particularly in energy-constrained or cost-sensitive scenarios 7 RELATED WORK 7.1 Hybrid and Energy Efficient Heterogeneous Data Centers Recent studies in optimizing data center architectures for deep learn ing have highlighted the necessity of energy-efficient scheduling and task allocation across diverse hardware. Gu et al. [10] explore GPU clusters’ energy-efficient scheduling, revealing substantial im provements in power utilization without considering diverse GPU types for different task requirements. This work highlights a gap in understanding how various GPU configurations could enhance energy efficiency further. Similarly, Patel et al. [25] demonstrate the benefits of hybrid computing environments, emphasizing FPGA over GPU diversity. This focus leaves room to explore the specific impacts of different GPU classes in such settings In the realm of LLMs, Zhao et al. [39] introduce strategies like phase-aware partitioning and adaptive quantization in heteroge neous clusters but do not integrate energy considerations into their analysis, which is crucial for understanding the real-world appli cability of these models in power-sensitive environments. On the other hand, Radovanović et al. [28] and Chien et al. [7] discuss broader aspects of carbon-aware computing and reducing the car bon impact of AI inference, respectively. These works emphasize the importance of node/device-level energy metrics, often over looked in typical LLM deployment strategies, thus underscoring the need for detailed energy consumption profiling across different models and hardware types 7.2 LLM Inference as a Service Further focusing on energy consumption, Hu et al. [14] analyze deep learning workloads in GPU datacenters, offering insights into energy conservation strategies through workload scheduling. This research aligns with our objectives by confirming the critical role of scheduling in reducing energy footprints. Anderson et al. [3 propose carbon-aware datacenter software that could complement physical hardware adjustments by making energy and carbon met rics visible to application developers, encouraging more energy efficient coding practices Addressing service quality, Wang et al. [35] study the efficiency and reliability of LLM serving, highlighting the challenges of main taining high-quality service while managing computational loads effectively. This perspective is pertinent as it underscores the trade off between performance and energy efficiency, which is central to our study. Lastly, Desislavov et al. [8] provide a timely examination of trends in AI inference energy consumption, arguing that while performance has increased dramatically, energy consumption has not escalated at the same pace, thanks to hardware optimizations and algorithmic innovations. This outlook is necessary as it sug gests the potential for further optimizations in LLM inference tasks which are typically energy-intensive 8 CONCLUSIONS AND FUTURE WORK Future work will explore minimizing the energy and runtime and maximizing the accuracy of serving differently-sized LLMs. Larger models are generally more accurate but come at the expense of requiring more hardware accelerators and often greater runtime therefore, exploring this trade-off is highly relevant. Also, we plan to make our solution for energy-optimal routing of incoming queries an online decision-making heuristic to increase its efficacy. Simi larly, we aim to extend our energy model to reflect carbon awareness and water consumption to decrease the environmental impact of LLM inference further 512 E-Energy ’24, June 04–07, 2024, Singapore, Singapore Grant Wilkins, Srinivasan Keshav, and Richard Mortier By carefully analyzing the energy and runtime of heterogeneous compute hardware to host LLMs, we show that a hybrid, hetero geneous datacenter and a cost-based scheduling framework can allocate LLM tasks to accelerators that are best suited to run them in terms of energy efficiency and computational performance. This decision is based simply on the size of input and output tokens making the decision process easy to integrate into existing work loads ACKNOWLEDGMENTS We gratefully acknowledge the computing resources provided on Swing and Palmetto, both high-performance computing clusters operated by the Laboratory Computing Resource Center at Argonne National Laboratory and Clemson University, respectively. During this work GW was supported by a Churchill Scholarship REFERENCES 1] Megha Agarwal, Asfandyar Qureshi, Linden Li Nikhil Sardana, Julian Quevedo and Daya Khudia. 2023 LLM Inference Performance Engineering: Best Practices https://www.databricks.com/blog/llm-inference-performance engineering-best-practices 2] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, et al. 2023. The Falcon Series of Language Models: Towards Open Frontier Models. (2023 3] Thomas Anderson, Adam Belay, Mosharaf Chowdhury, Asaf Cidon, and Irene Zhang. 2023. Treehouse: A Case For Carbon-Aware Datacenter Software. SIGEN ERGY Energy Inform. Rev. 3, 3 (oct 2023), 64–70. https://doi.org/10.1145/3630614 3630626 4] Rohan Anil, Andrew M. Dai, Orhan Firat, et al. 2023. PaLM 2 Technical Report arXiv:2305.10403 [cs.CL 5] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, and et al. 2022. On the Oppor tunities and Risks of Foundation Models. arXiv:2108.07258 [cs.LG 6] Le Chen, Nesreen K. Ahmed, Akash Dutta, et al. 2024. The Landscape and Challenges of HPC Research and LLMs. arXiv:2402.02018 [cs.LG 7] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. 2023. Reducing the Carbon Impact of Generative AI Inference (Today and in 2035). In Proceedings of the 2nd Workshop on Sustainable Computer Systems (Boston, MA, USA) (HotCarbon ’23). Association for Computing Machinery, New York, NY, USA, Article 11, 7 pages https://doi.org/10.1145 3604930.3605705 8] Radosvet Desislavov, Fernando Martínez-Plumed, and José Hernández-Orallo 2023. Trends in AI inference energy consumption: Beyond the performance-vs parameter laws of deep learning. Sustainable Computing: Informatics and Systems 38 (2023), 100857. https://doi.org/10.1016/j.suscom.2023.100857 9] Yiannis Georgiou, David Glesser, and Denis Trystram. 2015. Adaptive Resource and Job Management for Limited Power Consumption. In Proceedings of the 2015 IEEE International Parallel and Distributed Processing Symposium Workshop IPDPSW ’15). IEEE Computer Society, USA, 863–870. https://doi.org/10.1109 IPDPSW.2015.118 10] Diandian Gu, Xintong Xie, Gang Huang, Xin Jin, and Xuanzhe Liu. 2023. Energy Efficient GPU Clusters Scheduling for Deep Learning. arXiv:2304.06381 [cs.DC 11] Sylvain Gugger, Lysandre Debut, Thomas Wolf, et al. 2022. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com huggingface/accelerate 12] Yuxiong He and Sameh Elnikety. 2011. Position paper: embracing heterogeneity improving energy efficiency for interactive services. In Proceedings of the 8th AAAI Conference on AI for Data Center Management and Cloud Computing (AAAIWS’11 08). AAAI Press, New York, NY, 11–14 13] Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. 2020. Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning. J. Mach. Learn. Res. 21, 1, Article 248 (jan 2020 43 pages 14] Qinghao Hu, Peng Sun, Shengen Yan, Yonggang Wen, and Tianwei Zhang 2021. Characterization and prediction of deep learning workloads in large scale GPU datacenters. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (<conf-loc>, <city>St Louis</city>, <state>Missouri</state>, </conf-loc>) (SC ’21). Association for Computing Machinery, New York, NY, USA, Article 104, 15 pages https doi.org/10.1145/3458817.3476223 15] Xiaoxuan Hu, Peng Li, and Yanfei Sun. 2021. Minimizing energy cost for green data center by exploring heterogeneous energy resource. Journal of Modern Power Systems and Clean Energy 9, 1 (2021), 148–159 16] Mehboob Hussain, Lian-Fu Wei, Abdullah Lakhan, Samad Wali, Soragga Ali and Abid Hussain. 2021. Energy and performance-efficient task scheduling in heterogeneous virtualized cloud computing. Sustainable Computing: Informatics and Systems 30 (2021), 100517 17] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, and et al. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL 18] Willis Lang, Jignesh M. Patel, and Srinath Shankar. 2010. Wimpy node clusters what about non-wimpy workloads?. In Proceedings of the Sixth International Workshop on Data Management on New Hardware (Indianapolis, Indiana) (DaMoN 10). Association for Computing Machinery, New York, NY, USA, 47–55. https doi.org/10.1145/1869389.1869396 19] Wenyu Liu, Yuejun Yan, Yimeng Sun, Hongju Mao, Ming Cheng, Peng Wang and Zhaohao Ding. 2023. Online job scheduling scheme for low-carbon data center operation: An information and energy nexus perspective. Applied Energy 338 (2023), 120918 20] David Lo, Liqun Cheng, Rama Govindaraju, et al. 2015 Heracles: Improv ing resource efficiency at scale. In 2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA). ACM, New York, NY, 450–462 https://doi.org/10.1145/2749469.2749475 21] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Es timating the Carbon Footprint of BLOOM, a 176B Parameter Language Model arXiv:2211.02001 [cs.LG 22] David Mytton and Masa¯o Ashtine. 2022. Sources of data center energy estimates A comprehensive review. Joule 6, 9 (2022), 2032–2056. https://doi.org/10.1016/j joule.2022.07.011 23] NVIDIA. Accessed 2024. NVIDIA-NVML. https://docs.nvidia.com/deploy/nvml api/index.html. Available online 24] OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, et al. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL 25] Pratyush Patel, Katie Lim, Kushal Jhunjhunwalla, Ashlie Martinez, Max Demoulin Jacob Nelson, Irene Zhang, and Thomas Anderson. 2023. Hybrid Computing for Interactive Datacenter Applications. arXiv:2304.04488 [cs.DC 26] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon Emissions and Large Neural Network Training. arXiv:2104.10350 [cs.LG 27] powerapi ng. 2024. PyJoules: Python-based energy measurement library for various domains including NVIDIA GPUs. https://github.com/powerapi-ng pyJoules. Accessed: 2024-01-10 28] Ana Radovanović, Ross Koningstein, Ian Schneider, Bokan Chen, Alexandre Duarte, Binz Roy, Diyue Xiao, Maya Haridasan, Patrick Hung, Nick Care, et al 2022. Carbon-aware computing for datacenters. IEEE Transactions on Power Systems 38, 2 (2022), 1270–1280 29] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. 2023. From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference. arXiv:2310.03003 [cs.CL 30] Matej Špeťko, Ondřej Vysocký, Branislav Jansík, and Lubomír Říha. 2021. DGX A100 Face to Face DGX-2—Performance, Power and Thermal Behavior Evaluation Energies 14, 2 (2021). https://doi.org/10.3390/en14020376 31] R. Taori, I. Gulrajani, T. Zhang, and et al. 2024. Stanford alpaca: An instruction following llama model. https://github.com/tatsu-lab/stanford_alpaca. Accessed 2024-01-15 32] Google Gemini Team. 2024. Gemini: A Family of Highly Capable Multimodal Models. arXiv:2312.11805 [cs.CL 33] Hugo Touvron, Louis Martin, Kevin Stone, and et al. 2023 Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL 34] Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS’17). Curran Associates Inc., Red Hook, NY, USA, 6000–6010 35] Yuxin Wang, Yuhan Chen, Zeyu Li, Zhenheng Tang, Rui Guo, Xin Wang, Qiang Wang, Amelie Chi Zhou, and Xiaowen Chu. 2024. Towards Efficient and Reliable LLM Serving: A Real-World Workload Study. arXiv:2401.17644 [cs.DC 36] Vincent M. Weaver, Matt Johnson, Kiran Kasichayanula, James Ralph, Piotr Luszczek, Dan Terpstra, and Shirley Moore. 2012. Measuring Energy and Power with PAPI. In 2012 41st International Conference on Parallel Processing Workshops 262–268. https://doi.org/10.1109/ICPPW.2012.39 37] Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: an insightful visual performance model for multicore architectures. Commun. ACM 52, 4 (apr 2009), 65–76. https://doi.org/10.1145/1498765.1498785 38] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, and et al. 2022. Sustainable ai: Environmental implications, challenges and opportunities. Proceedings of Machine Learning and Systems 4 (2022), 795–813 39] Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. LLM PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization. arXiv preprint arXiv:2403.01136 (2024 513\n",
            "\n",
            "\n",
            "--- Abstract from paper5.pdf ---\n",
            "The rapid evolution and widespread adoption of generative large language models (LLMs) have made them a pivotal workload in various applications. Today, LLM inference clusters receive a large number of queries with strict Service Level Objectives (SLOs). To achieve the desired performance these models execute on power-hungry GPUs causing the in ference clusters to consume large amount of energy and, conse quently, result in excessive carbon emissions. Fortunately, we find that there is a great opportunity to exploit the heterogeneity in inference compute properties and fluctuations in inference work loads, to significantly improve energy-efficiency. However, such a diverse and dynamic environment creates a large search-space where different system configurations (e.g., number of instances model parallelism, and GPU frequency) translate into different energy-performance trade-offs. To address these challenges, we propose DynamoLLM, the first energy-management framework for LLM inference environments. DynamoLLM automatically and dynamically reconfigures the inference cluster to optimize for energy and cost of LLM serving under the service’s performance SLOs. We show that at a service-level, DynamoLLM conserves 53% energy and 38% operational carbon emissions, and reduces 61% cost to the customer, while meeting the latency SLOs.\n",
            "\n",
            "\n",
            "--- Abstract from paper2.pdf ---\n",
            "The rapid adoption of large language models (LLMs) has led to significant advances in natural language processing and text gener ation. However, the energy consumed through LLM model infer ence remains a major challenge for sustainable AI deployment. To address this problem, we model the workload-dependent energy consumption and runtime of LLM inference tasks on heterogeneous GPU-CPU systems. By conducting an extensive characterization study of several state-of-the-art LLMs and analyzing their energy and runtime behavior across different magnitudes of input prompts and output text, we develop accurate (𝑅2 > 0.96) energy and run time models for each LLM. We employ these models to explore an offline, energy-optimal LLM workload scheduling framework Through a case study, we demonstrate the advantages of energy and accuracy aware scheduling compared to existing best practices CCS CONCEPTS Computer systems organization →Heterogeneous (hybrid systems; • Hardware →Impact on the environment KEYWORDS Sustainable computing, Heterogeneous computing, Large Language Models, Artificial Intelligence ACM Reference Format Grant Wilkins, Srinivasan Keshav, and Richard Mortier. 2024. Offline Energy Optimal LLM Serving: Workload-Based Energy Models for LLM Inference on Heterogeneous Systems. In The 3rd ACM HotCarbon Workshop on Sustainable Computer System (HotCarbon ’24), 9 July 2024, Santa Cruz, CA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn 1 INTRODUCTION Rapid advancements in large language models (LLMs) have rev olutionized natural language processing, enabling AI systems to achieve human-level performance on a wide range of language tasks [3, 26, 40]. However, the computational resources and energy consumption associated with deploying these models present signif icant challenges to not only energy systems but also sustainability goals [20, 21, 29]. As LLMs become increasingly integrated into Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored For all other uses, contact the owner/author(s HotCarbon’24, 9 July 2024, Santa Cruz, CA 2024 Copyright held by the owner/author(s ACM ISBN 978-x-xxxx-xxxx-x/YY/MM https://doi.org/10.1145/nnnnnnn.nnnnnnn real-world applications, optimizing their energy efficiency during inference is crucial for sustainable AI development [45 Inference, the process of using a trained model to make predic tions on new data, is a critical phase in LLM deployment as it is the point at which AI capabilities become accessible to users. Unlike the one-time training process, inference is an ongoing, real-time pro cess that directly impacts end-user experience. Inference in LLMs can be computationally expensive due to model size [45] and quality of service/latency expectations [42]. Scaling LLMs up across large data centers is challenging due to power [27] and communication overheads [28 The energy intensity of LLM inference can be substantial even when compared to training [4]. Decarbonizing the energy sources for data centers can be challenging due to both sporadic demand and regional inefficiencies in adopting renewables. Higher energy con sumption of an application approximately correlates with greater carbon intensity [32]. It is thus crucial to find energy-efficient meth ods to mitigate the environmental costs of LLM inference [1, 18 To address this issue, we propose a workload-based model of energy consumption for LLM inference to let system operators navigate the trade-off between accuracy and energy usage Our contributions are as follows 1) We characterize the energy consumption and runtime be havior of several state-of-the-art LLMs on a heterogeneous GPU-CPU system (§4 2) We develop workload-based energy and runtime models that accurately capture the relationship between the number of input and output tokens and the energy and runtime characteristics of each LLM (§5 3) We demonstrate the effectiveness of our approach through a case study, showcasing a tunable trade-off between energy and accuracy (§6 Our profiling framework and datasets are openly available.1 2 RELATED WORK 2.1 Energy Consumption in AI Systems Recent reports have found that the computation required by state of-the-art AI systems entail massive energy consumption and car bon emissions [4, 22, 29, 35, 45]. The energy intensity of AI systems can be broadly split between the energy required for training and that required for inference after models are deployed [10]. Training complex models on massive datasets is an energy-intensive process with estimates finding that training GPT-3 required 1,287 megawatt hours of energy [29]. Even with this huge amount of energy, a year of inference by an LLM on cloud infrastructure can consume over 1https://github.com/grantwilkins/energy-inference.git arXiv:2407.04014v1  [cs.DC]  4 Jul 2024 HotCarbon’24, 9 July 2024, Santa Cruz, CA Grant Wilkins, Srinivasan Keshav, and Richard Mortier 25× more energy than training that same model [4]. Some of these issues and emissions of course depend on the deployment scale and hardware efficiency [35], however, the trend remains that energy consumption in inference is a large issue. Optimizing software and hardware specifically for AI workloads is thus essential [1 Desislavov et al. [5] provide an examination of trends in AI in ference energy consumption, arguing that while performance has increased dramatically, energy consumption has not escalated at the same pace, thanks to hardware optimizations and algorithmic innovations. Chien et al. [4] discuss larger trends in LLM inference energy consumption and do not focus on device-level energy mod eling benefits. Samsi et al. [35] explore the energy consumption of Meta’s Llama LLMs for different batch sizes and numbers of GPUs showing the potential energy reductions obtainable by tuning these parameters. Stojcovik et al. [37] discuss the impacts of GPU fre quency scaling on the energy efficiency of serving LLMs; however at this point, this work is only a characterization and not an applied analysis Our work extends these studies with a thorough CPU+GPU energy measurements across multiple model families and sizes producing one of the most comprehensive datasets of its kind 2.2 Energy-Aware Data Center Scheduling A large body of work that focuses on energy-aware scheduling [6 13, 19, 24, 34, 38], but none of these focus on the unique challenge of developing workload-aware models for LLM inference towards this goal. Hu et al. [12] analyze deep learning workloads in GPU data centers, offering insights into energy conservation strategies through workload scheduling. This research aligns with our ob jectives by confirming the critical role of scheduling in reducing energy footprints Li et al. [17] introduce Clover, which promises to minimize car bon emissions for serving AI inference. Unlike our study, this work does not explicitly consider LLMs or a per-model function to capture energy and runtime, instead focusing on carbon-emission patterns for a data center Gu et al. [8] presents PowerFlow, a tool that uses clock-frequency data from GPUs to minimize energy consumption as a scheduling decision. However, their study does not consider LLMs and is not necessarily workload-aware Patel et al. introduce POLCA [27], which can provide a way to automatically power-cap based on existing workload traces. Li et al. [18] focuses on delivering a geographic load balancing perspec tive for AI inference, optimizing environmental equity. However their model considers large-scale workload traces, not device-level energy and runtime data Our work aims to fill the niche with energy-aware LLM inference scheduling using measurements from state-of-the-art open-source LLMs leading to an applied analysis using offline optimization. The results of our findings can be used by system operators to accurately predict and schedule based on the amount of energy and runtime for inference 3 METHODS For our LLM inference engine we use Hugging Face’s Accelerate [9 This library uses all available GPUs, and divides a model among the available GPUs in a tensor parallel fashion to minimize inter mediate communication and maximize the distributed capabilities for computation across the devices. We disable KV-caching [41] to ensure that our measurements are consistent between runs and do not require a warm-start phase 3.1 LLM Selection We study several open-source LLMs, summarized in Table 1. By profiling different LLMs we are able to explore the effects of diverse architectures and parameter values on runtime, energy consump tion, and accuracy. For each model, we conduct a series of standard ized text generation prompts to evaluate their energy consumption during inference Numerous benchmarks have sought to quantify LLM accuracy e.g., the MMLU [11] and HellaSwag [46]. To avoid the inadequacies introduced by individual tests for accuracy [23], we use the Hugging Face Leaderboard’s [2] average accuracy, denoted 𝐴𝐾, that averages the performance of a model, 𝐾, on a large repository of datasets and tests Table 1: LLM Energy Consumption and Runtime LLM (# Params vRAM Size (GB A100s 𝑨𝑲(%) [2 Falcon (7B 14.48 1 44.17 Falcon (40B 83.66 3 58.07 Llama-2 (7B 13.48 1 50.97 Llama-2 (13B 26.03 1 55.69 Llama-2 (70B 137.98 4 64.52 Mistral (7B 15.00 1 60.97 Mixtral (8x7B 93.37 3 68.47 3.2 Energy Profiling of Our Cluster We perform all experiments the Swing cluster at Argonne National Lab using a single node with 8×Nvidia A100 (40GB) GPUs, 2×AMD Epyc 7742 64-core processors, and 1TB of DDR4 RAM. We use only the minimum number of GPUs as shown in Table 1. We profile the system’s energy consumption during inference using tools that capture Nvidia GPU energy and AMD CPU power while timing the operation. Our methods utilize the known relationship that 𝐸= 𝑃𝑡 where 𝐸represents energy, 𝑃is average power, and 𝑡is runtime 3.2.1 NVIDIA GPUs. We use PyJoules [31], a Python-based en ergy measurement library, to quantify the energy consumption associated with inference on NVIDIA GPUs. PyJoules provides an interface to NVML [25], providing a software-defined energy usage assessment for targeted NVIDIA devices. This tool offers GPUs real-time energy consumption for a given tracked process 3.2.2 AMD CPUs. We adopt a different strategy for AMD CPUs due to the absence of a Python API. Instead, we utilize AMD𝜇Prof’s timechart feature, which provides detailed power draw metrics for every core on the chip at fine-grained intervals. By polling AMD𝜇Prof at 100ms intervals, we can capture the power draw of each physical CPU core throughout the model inference process To ensure we accurately attribute the energy consumption to our inference task, we monitor the CPU core residency through psutil Offline Energy-Optimal LLM Serving: Workload-Based Energy Models for LLM Inference on Heterogeneous Systems HotCarbon’24, 9 July 2024, Santa Cruz, CA This information allows us to identify and record the specific cores actively engaged in the inference process at each time step. The total energy consumption for the inference task is then calculated by summing the power usage across all active cores and summing over the product of the power usage and time of inference, as follows 𝐸𝑇𝑜𝑡𝑎𝑙,𝐶𝑃𝑈 𝑐𝑜𝑟𝑒 𝑖 𝑃𝑐𝑜𝑟𝑒,𝑖Δ𝑡𝑖 where 𝑃𝑐𝑜𝑟𝑒,𝑖represents the power draw of an individual core at each time step, 𝑖, with Δ𝑡𝑖being the time step size 4 PROBLEM FORMULATION The purpose of developing workload-based models of LLM infer ence is to create a framework that allows a data center operator to navigate the trade-off between model accuracy and energy con sumption. To do so, we formalize an optimization problem below Consider a data center that hosts K = {1, . . . , 𝐾} distinct LLM models. Assume that a fraction 𝛾𝐾of the inference workload is assigned to each model 𝐾, where𝛾𝐾∈[0, 1], ∀𝐾and Í 𝐾∈K 𝛾𝐾= 1 We denote a query 𝑞by its count of input and output tokens, 𝑞 𝜏𝑖𝑛,𝜏𝑜𝑢𝑡). A workload with𝑚queries is then a multiset𝑄∈\u0000N2\u0001𝑚 As our goal is to perform scheduling of each query, we must create a disjoint partition of our set 𝑄. We say that each 𝑄𝐾∈\u0000N2\u0001𝑚𝐾 has 𝑚𝐾prompts and is composed of a set of lengths of input and output tokens 𝑄𝐾 𝜏𝑖𝑛,1,𝜏𝑜𝑢𝑡,1), . . . , (𝜏𝑖𝑛,𝑖,𝜏𝑜𝑢𝑡,𝑖 Since this is an offline setting we assume we have perfect knowl edge of our system, including the number of output tokens that a given input prompt will produce. In reality, this is not known ab initio though work by Zheng et al. [47] has shown that the number of output tokens can be reasonably well estimated by analyzing past input-output pairs For optimization purposes, we must define a function based on the constant 𝐴𝐾from Table 1. We propose 𝑎𝐾: N2 →[0, ∞), a monotonically increasing function based on the number of input and output tokens that a model 𝐾ingests and produces. Therefore for a model 𝐾processing tokens (𝜏𝑖𝑛,𝜏𝑜𝑢𝑡) we have 𝑎𝐾(𝜏𝑖𝑛,𝜏𝑜𝑢𝑡) = 𝐴𝐾𝜏𝑖𝑛+ 𝐴𝐾𝜏𝑜𝑢𝑡 1 As we will later derive, we denote a model for energy consumption for a given number of input and output tokens as 𝑒𝐾(𝜏𝑖𝑛,𝑖,𝜏𝑜𝑢𝑡,𝑖 N2 →[0 Both of these functions have a normalized counterpart c 𝑒𝐾, c 𝑎𝐾 N2 →[0, 1] that scales the cost associated with these values [0, 1 to make these different metrics comparable. We normalize by di viding by the largest known value of energy and accuracy prior to optimization Finally, let 𝜁∈[0, 1] denote a tuning parameter that lets a data center operator trade off energy for accuracy. Let |𝑄| represent the total number of queries in our workload, and |𝑄𝐾| represent the total number of queries each model 𝐾processes We now formulate our workload assignment problem as min 𝑄𝐾∈𝑄 𝐾∈K 𝜏𝑖𝑛,𝜏𝑜𝑢𝑡)∈𝑄𝐾 𝜁c 𝑒𝐾(𝜏𝑖𝑛,𝜏𝑜𝑢𝑡) −(1 −𝜁)c 𝑎𝐾(𝜏𝑖𝑛,𝜏𝑜𝑢𝑡 2 s.t., 0 < |𝑄𝐾 𝑄| < 1 3 𝑄 Ø 𝐾∈K 𝑄𝐾 4 𝑄𝐼∩𝑄𝐽= ∅, 𝐼≠𝐽, ∀𝐼, 𝐽∈K 5 where Equations 4 and 5 define the partition coverage of the work load, and Equation 3 ensures we give each LLM some queries. In our implementation, we dynamically normalize our energy and accuracy measures across all the queries to allow us to adjust the scale of costs across different models and query combinations. This problem is computationally intensive to solve as it is an example of a general assignment problem which are known to be NP-hard [7 5 LLM INFERENCE PERFORMANCE All hardware information we state in Section 3.2. We use Ubuntu 20.04 with Python 3.12.0, PyTorch v2.0.1, Torchvision v0.15.2, Numpy v1.26.0, Hugging Face v0.20.2, and Accelerate v0.26.1 5.1 Experimental Strategy We conduct an experimental campaign to evaluate the performance of differing workloads across various models. We systematically varied the number of input and output tokens to measure their effects on runtime and energy consumption under two main ex perimental conditions. In each experiment we do not allow for key-value caches [41] to be re-used to ensure our measurements are standard between iterations. We fix the batch size at 32 5.1.1 Vary Input Tokens. For the first experimental condition, we executed inference requests with increasing the number of input tokens, ranging from 8 to 2048 tokens, while maintaining a fixed output token size of 32. This setup allowed us to isolate the impact of input size on the system’s performance and energy efficiency 5.1.2 Vary Output Tokens. In the second set of experiments, we varied the output token limit from 8 to 4096 tokens, keeping the number of input tokens constant at 32. This approach helped us understand how increasing output demands affect the runtime and energy consumption of the systems tested 5.1.3 Randomization and Stopping Criteria. Each experiment was conducted in a randomized order to mitigate any potential bias introduced by the sequence of tests. Also, we repeated trials until either of two conditions was met: (i) the measured runtime was within 0.5 seconds of the actual mean runtime with 95% confidence and (ii) a maximum of 25 trials were conducted for each setting if the first condition could not be met 5.2 Input Token Effects Figure 1 presents the impact of varying numbers of input tokens on the runtime, throughput, and energy per token for various LLMs The results depict a clear trend: as the number of input tokens increases, the runtime tends to increase, while the throughput HotCarbon’24, 9 July 2024, Santa Cruz, CA Grant Wilkins, Srinivasan Keshav, and Richard Mortier 23 24 25 26 27 28 29 210 211 Number of Input Tokens 100 101 102 Runtime (s a) Runtime 23 24 25 26 27 28 29 210 211 Number of Input Tokens 101 102 103 Throughput (tokens/s b) Throughput 23 24 25 26 27 28 29 210 211 Number of Input Tokens 100 101 102 Energy per Token (J/tokens Falcon (7B Falcon (40B Llama-2 (7B Llama-2 (13B Llama-2 (70B Mistral (7B Mixtral (8x7B c) Energy per Token Figure 1: Model performance against number of input tokens. Low variance renders error bars invisible 23 24 25 26 27 28 29 210 211 212 Number of Output Tokens 10−1 100 101 102 103 104 Runtime (s a) Runtime 23 24 25 26 27 28 29 210 211 212 Number of Output Tokens 10−1 100 101 102 103 Throughput (tokens/s b) Throughput 23 24 25 26 27 28 29 210 211 212 Number of Output Tokens 10−1 100 101 102 103 Energy per Token (J/tokens Falcon (7B Falcon (40B Llama-2 (7B Llama-2 (13B Llama-2 (70B Mistral (7B Mixtral (8x7B c) Energy per Token Figure 2: Model performance against number of output tokens. Low variance renders error bars invisible plateaus, in accordance with a roofline model [44]. Specifically, the runtime increase is most pronounced for larger models like Llama 2 (70B) and Falcon (40B), likely due to the higher computational burden these models sustain as they process more extensive input sequences. The energy consumption per token demonstrates similar trends, with smaller models exhibiting lower energy per token compared to larger models An outlier to all of these cases is Mixtral (8x7B), which has a higher throughput and energy efficiency compared to other large models at larger token input sizes. This LLM’s sparse mixture of-experts architecture (SMoE) [14, 33] allows it to activate just 12B parameters on average by selecting two expert sub-models This classification phase comes with an added runtime and energy overhead, however, on larger prompts it regains its performance capabilities. Therefore, for SMoE one gets the accuracy advantages of a large model for less energy and lower runtime than its denser counterparts 5.3 Output Token Effects Figure 2 illustrates how changes in the number of output tokens affect runtime, throughput, and energy consumption per token across different LLMs. Notably, the runtime exhibits a steep in crease with larger output token sizes, which is consistent across all models but is especially significant for the high-parameter models such as Falcon (40B) and Llama-2 (70B). Throughput, decreases as the number of output tokens increases. This inverse relationship highlights the additional time required to generate each additional token, which involves more extensive interaction between model layers and successive passes through the LLM to generate each token [41]. Energy per token also increases with the number of out put tokens and number of parameters. This increase is particularly sharp in higher-parameter models like Falcon (40B Again, Mixtral (8x7B) demonstrates greater energy efficiency compared to its large parameter counterparts. Even in cases of high output token generation, an SMoE architecture can yield improve ments in energy efficiency 6 WORKLOAD-BASED MODEL FITTING From the experimental results in Section 5 we can see that each LLM has a unique runtime and energy consumption characteristic that is a function of the given workload. In this section we develop and apply these models to optimizing energy and runtime of serving LLMs Offline Energy-Optimal LLM Serving: Workload-Based Energy Models for LLM Inference on Heterogeneous Systems HotCarbon’24, 9 July 2024, Santa Cruz, CA 6.1 Independence of Input and Output Tokens From observing our results, we explored whether the number of input and output tokens are independent in their effect on the energy consumption and runtime. The following table presents the ANOVA results for assessing the effects of the number of input tokens, the number of output tokens, and their interaction on the total energy consumption and runtime for LLM inference. To collect this data we perform a grid search from 8 to 2048, in increments of powers of two, for the space of input and output tokens to eliminate the bias of holding the input or output size constant. This analysis includes data aggregated across all models in Table 1 Table 2: ANOVA Results for LLM Energy Consumption and Runtime Metric Variable Sum of Squares F-statistic 𝑝-value Energy (J Input Tokens 5.17 × 1010 15.86 3.79 × 10−17 Output Tokens 4.13 × 1011 126.63 1.22 × 10−65 Interaction 1.18 × 1011 4.53 4.67 × 10−15 Runtime (s Input Tokens 3.43 × 105 12.97 2.34 × 10−14 Output Tokens 2.78 × 106 104.98 4.56 × 10−60 Interaction 8.21 × 105 3.88 1.92 × 10−12 The number of input tokens and number of output tokens both individually have a substantial impact on energy consumption and runtime, with output tokens having a larger effect size as indicated by the higher 𝐹statistic. Also, the interaction term shows that the input and output tokens depend on each other while impacting en ergy consumption and runtime. The high 𝐹-statistics and extremely low 𝑝-values for these effects confirm their significance. Therefore we conclude that there is dependence between the number of input and output tokens for energy consumption and runtime 6.2 Modeling Energy and Runtime We use the results in Table 2 to guide the creation of models to predict the energy consumption and runtime of LLMs for use in optimization problems such as those discussed in Section 4 For accurate models based on the number of input and output tokens there needs to be an interaction term that combines them. We therefore propose a model to describe the total energy consumption for a model 𝐾as a function of input and output tokens, 𝜏𝑖𝑛and 𝜏𝑜𝑢𝑡 respectively 𝑒𝐾(𝜏𝑖𝑛,𝜏𝑜𝑢𝑡) = 𝛼𝐾,0𝜏𝑖𝑛+ 𝛼𝐾,1𝜏𝑜𝑢𝑡+ 𝛼𝐾,2𝜏𝑖𝑛𝜏𝑜𝑢𝑡 6 where 𝛼𝐾,0, 𝛼𝐾,1, 𝛼𝐾,2 are parameters determined through ordinary least squares (OLS) regression for each model and system combina tion Similarly, we propose the following model to describe the total runtime for a model 𝐾as a function of input and output tokens, 𝜏𝑖𝑛 and 𝜏𝑜𝑢𝑡, respectively 𝑟𝐾(𝜏𝑖𝑛,𝜏𝑜𝑢𝑡) = 𝛽𝐾,0𝜏𝑖𝑛+ 𝛽𝐾,1𝜏𝑜𝑢𝑡+ 𝛽𝐾,2𝜏𝑖𝑛𝜏𝑜𝑢𝑡 7 where 𝛽𝐾,0, 𝛽𝐾,1, 𝛽𝐾,2 are also unique to each model 𝐾 Using the statsmodel (v0.14.2) Python package and its OLS API we can determine the values of 𝛼𝐾,𝑖and 𝛽𝐾,𝑗that best fit Equa tions 6 and 7 for each LLM, 𝐾. A summary of the quality of these fits are included in Table 3. As we can see, this model has high explainability for the effect of input and output tokens on energy and runtime for inference of these different LLMs Table 3: Summary of OLS Regression Results Across Models LLM (# Params Energy Model (𝑒𝐾 Runtime Model (𝑟𝐾 𝑹2 F-statistic 𝑝-value 𝑹2 F-statistic 𝑝-value Falcon (7B 0.964 681.2 2.53e-55 0.962 651.1 1.35e-54 Falcon (40B 0.972 904.5 1.78e-60 0.976 1073.0 2.74e-63 Llama-2 (7B 0.973 942.3 3.76e-61 0.972 1032.0 1.19e-62 Llama-2 (13B 0.972 887.8 3.60e-60 0.972 907.0 1.60e-60 Llama-2 (70B 0.976 1022.0 6.66e-62 0.980 1230.0 6.23e-65 Mistral (7B 0.975 997.0 1.70e-61 0.976 1039.0 3.62e-62 Mixtral (8x7B 0.980 1238.0 4.97e-65 0.992 3139.0 2.23e-80 6.3 Applying Our Models to Workload Routing We can now use our runtime and energy consumption models to solve the workload-aware routing problem outlined in Section 4 Using PuLP (v.2.8.0), a Python package designed for solving opti mization problems like that we formulate in Equation 2, we can encode a workload of input and output tokens with a set of binary variables that indicate which model will process that pair of tokens Then, we convert the given constraints in Equations 3–5 using this format and effectively route our workload to different models As we show in Table 1 and Figures 1 and 2, an LLM with a larger parameter count has greater accuracy but also greater runtime and energy consumption for each input and output token. It is reasonable to host differently sized models to allow us to serve inference requests more runtime and energy efficiently with a trade off of slightly lower accuracy For this example, we consider a data center serving the three Llama-2 models of 7B, 13B, and 70B parameters. Assume that our set K = {1, 2, 3} enumerates those models, respectively. A tunable parameter that affects our optimization problem is the data center partition 𝛾𝑖. In our evaluation, we choose 𝛾1 = 0.05,𝛾2 = 0.2, and 𝛾3 = 0.75 With this, we can use the model for energy consumption of each LLM, 𝐾, in Equation 6 and our function to capture accuracy from Equation 1 to calculate the costs associated with each query and model as shown in Equation 2. For our sample workload, we use a subset of 500 queries from the Alpaca dataset [39], as it is a collection of 52002 queries with answers from GPT-4 [26 Figure 3 shows the trade-offs in energy consumption, runtime and accuracy by varying the operational parameter 𝜁while routing queries to different models. We represent as constants (straight lines) methods that do not use 𝜁, preferring to pick either a single LLM or to use a simple query-independent mechanism to route a query to an LLM. The remaining non-constant line represents the trade-off our offline scheduler makes as it adjusts to changes in 𝜁 In Figure 3(a), we see that energy consumption is high when 𝜁is low because the system prioritizes accuracy over energy efficiency Higher 𝜁values lead to more energy-efficient routing decisions sacrificing accuracy for energy savings. Similarly, Figure 3(b) shows that the mean runtime per query decreases with increasing 𝜁. A low 𝜁value results in longer runtimes as the system routes queries to models that provide higher accuracy but are less efficient in time and energy. Conversely, higher 𝜁values result in shorter runtimes HotCarbon’24, 9 July 2024, Santa Cruz, CA Grant Wilkins, Srinivasan Keshav, and Richard Mortier 0.00 0.25 0.50 0.75 1.00 휁 0 2 4 6 8 10 Total Energy (kWh a) Energy Consumption 0.00 0.25 0.50 0.75 1.00 휁 0 25 50 75 100 125 Mean Runtime (s b) Mean Runtime 0.00 0.25 0.50 0.75 1.00 휁 50.0 52.5 55.0 57.5 60.0 62.5 65.0 Accuracy Llama-2 (7B Llama-2 (13B Llama-2 (70B Round Robin Random Oﬀline c) Accuracy Figure 3: Behavior under offline simulation as 𝜁varies. Round-robin and Random query assignment are indistinguishable as the system favors more energy and time-efficient models over the most accurate ones. Figure 3(c) demonstrates the accuracy-cost trade-off, with small increases in accuracy requiring significant increases in runtime and energy consumption Our solution allows data center operators use 𝜁to navigate the trade-off space by, e.g., providing higher accuracy when energy prices are lower, or delivering lower latency and lower energy responses during times of peak load albeit with slightly reduced accuracy. This flexibility is important for adapting to different op erational scenarios 7 CONCLUSIONS In this paper, we have examined the significant energy expenditure of LLM inference. We show that modeling and optimizing the en ergy consumption of LLM inference for a system is straightforward We also showed that SMoE LLMs exhibit very promising energy efficiency characteristics. Through our models of energy and run time we contribute to the ongoing efforts towards sustainable AI by providing a tunable optimization framework that allows for system operators to trade-off energy and accuracy. We confirm our hypothesis that there is potential for energy optimization using models of energy and accuracy Of course, as many others have done [4, 10, 16, 20, 21, 36] we have used energy consumption as a proxy for carbon footprint. As pointed out by Kannan and Kremer [15], improving carbon effi ciency and energy efficiency are distinct goals, yet they are related and energy metrics can assist in understanding the magnitude of emissions for a given application [1]. Our measurements of energy consumption are also based on a single node in an HPC setting and so we cannot capture the runtime and energy overheads introduced by faults, networking, and communications that would pertain at data center scale. We also disabled key-value caching [30] to estab lish a performance baseline; future work should explore the impact of this and other optimizations. Finally, our workload-models are specific primarily to an NVIDIA A100 (40GB), as pointed out in other studies there are large variations for the same inference task across hardware [35, 43 We hope that our energy models can be used in real-time systems to reduce energy consumption dynamically. By integrating these models into online scheduling algorithms, data centers can make energy-aware decisions based on the current workload and system state. This real-time optimization approach has the potential to significantly improve the energy efficiency of LLM inference in production environments. Similarly, including externalities like energy pricing and availability of sustainable energy into our model would bring systems closer to meeting sustainability goals ACKNOWLEDGMENTS We gratefully acknowledge the computing resources provided on Swing, a high-performance computing cluster operated by the Labo ratory Computing Resource Center at Argonne National Laboratory During this work GW was supported by a Churchill Scholarship We would like to thank the reviewers for their valuable feedback to help improve our work REFERENCES 1] Thomas Anderson, Adam Belay, Mosharaf Chowdhury, Asaf Cidon, and Irene Zhang. 2023. Treehouse: A Case For Carbon-Aware Datacenter Software. SIGEN ERGY Energy Inform. Rev. 3, 3 (oct 2023), 64–70. https://doi.org/10.1145/3630614 3630626 2] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lam bert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023 Open LLM Leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_ llm_leaderboard 3] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, and et al. 2022. On the Oppor tunities and Risks of Foundation Models. arXiv:2108.07258 [cs.LG 4] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. 2023. Reducing the Carbon Impact of Generative AI Inference (Today and in 2035). In Proceedings of the 2nd Workshop on Sustainable Computer Systems (Boston, MA, USA) (HotCarbon ’23). Association for Computing Machinery, New York, NY, USA, Article 11, 7 pages https://doi.org/10.1145 3604930.3605705 5] Radosvet Desislavov, Fernando Martínez-Plumed, and José Hernández-Orallo 2023. Trends in AI inference energy consumption: Beyond the performance-vs parameter laws of deep learning. Sustainable Computing: Informatics and Systems 38 (2023), 100857. https://doi.org/10.1016/j.suscom.2023.100857 6] Kaijie Fan, Marco D’Antonio, Lorenzo Carpentieri, Biagio Cosenza, Federico Ficarelli, and Daniele Cesarini. 2023. SYnergy: Fine-grained Energy-Efficient Heterogeneous Computing for Scalable Energy Saving. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 1–13 7] Marshall L Fisher, Ramchandran Jaikumar, and Luk N Van Wassenhove. 1986. A multiplier adjustment method for the generalized assignment problem. Manage ment science 32, 9 (1986), 1095–1103 8] Diandian Gu, Xintong Xie, Gang Huang, Xin Jin, and Xuanzhe Liu. 2023. Energy Efficient GPU Clusters Scheduling for Deep Learning. arXiv:2304.06381 [cs.DC 9] Sylvain Gugger, Lysandre Debut, Thomas Wolf, et al. 2022. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com huggingface/accelerate Offline Energy-Optimal LLM Serving: Workload-Based Energy Models for LLM Inference on Heterogeneous Systems HotCarbon’24, 9 July 2024, Santa Cruz, CA 10] Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. 2020. Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning. J. Mach. Learn. Res. 21, 1, Article 248 (jan 2020 43 pages 11] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika Dawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Lan guage Understanding. In International Conference on Learning Representations https://openreview.net/forum?id=d7KBjmI3GmQ 12] Qinghao Hu, Peng Sun, Shengen Yan, Yonggang Wen, and Tianwei Zhang. 2021 Characterization and prediction of deep learning workloads in large-scale GPU datacenters. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC ’21). Association for Computing Machinery, New York, NY, USA, Article 104, 15 pages. https://doi.org/10.1145 3458817.3476223 13] Hongpeng Huo, Chongchong Sheng, Xinming Hu, and Baifeng Wu. 2012. An en ergy efficient task scheduling scheme for heterogeneous GPU-enhanced clusters In 2012 International Conference on Systems and Informatics (ICSAI2012). IEEE 623–627 14] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024 Mixtral of Experts. arXiv:2401.04088 [cs.LG 15] Sudarsun Kannan and Ulrich Kremer. 2023 Towards Application Centric Carbon Emission Management. In Proceedings of the 2nd Workshop on Sus tainable Computer Systems (Boston, MA, USA) (HotCarbon ’23). Association for Computing Machinery, New York, NY, USA, Article 5, 7 pages https doi.org/10.1145/3604930.3605725 16] Baolin Li, Yankai Jiang, Vijay Gadepally, and Devesh Tiwari. 2024. Toward Sus tainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference. arXiv:2403.12900 [cs.DC] https://arxiv.org/abs/2403.12900 17] Baolin Li, Siddharth Samsi, Vijay Gadepally, and Devesh Tiwari. 2023. Clover Toward Sustainable AI with Carbon-Aware Machine Learning Inference Service In Proceedings of the International Conference for High Performance Computing Networking, Storage and Analysis (SC ’23). Association for Computing Machinery New York, NY, USA, Article 20, 15 pages. https://doi.org/10.1145/3581784.3607034 18] Pengfei Li, Jianyi Yang, Adam Wierman, and Shaolei Ren. 2024. Towards En vironmentally Equitable AI via Geographical Load Balancing. In Proceedings of the 15th ACM International Conference on Future and Sustainable Energy Systems Singapore, Singapore) (e-Energy ’24). Association for Computing Machinery New York, NY, USA, 291–307. https://doi.org/10.1145/3632775.3661938 19] Qianlin Liang, Walid A Hanafy, Ahmed Ali-Eldin, and Prashant Shenoy. 2023 Model-driven cluster resource management for ai workloads in edge clouds. ACM Transactions on Autonomous and Adaptive Systems 18, 1 (2023), 1–26 20] Liuzixuan Lin and Andrew A Chien. 2023. Adapting Datacenter Capacity for Greener Datacenters and Grid. In Proceedings of the 14th ACM International Conference on Future Energy Systems (Orlando, FL, USA) (e-Energy ’23). As sociation for Computing Machinery, New York, NY, USA, 200–213 https doi.org/10.1145/3575813.3595197 21] Liuzixuan Lin, Rajini Wijayawardana, Varsha Rao, Hai Nguyen, Emmanuel Wedan GNIBGA, and Andrew A. Chien. 2024. Exploding AI Power Use: an Opportunity to Rethink Grid Planning and Management. In Proceedings of the 15th ACM International Conference on Future and Sustainable Energy Systems (Singapore Singapore) (e-Energy ’24). Association for Computing Machinery, New York, NY USA, 434–441. https://doi.org/10.1145/3632775.3661959 22] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2023. Es timating the Carbon Footprint of BLOOM, a 176B Parameter Language Model Journal of Machine Learning Research 24, 253 (2023), 1–15. http://jmlr.org/papers v24/23-0069.html 23] Timothy R. McIntosh, Teo Susnjak, Tong Liu, Paul Watters, and Malka N. Halga muge. 2024. Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence. arXiv:2402.09880 [cs.AI 24] Xinxin Mei, Xiaowen Chu, Hai Liu, Yiu-Wing Leung, and Zongpeng Li. 2017 Energy efficient real-time task scheduling on CPU-GPU hybrid clusters. In IEEE INFOCOM 2017-IEEE Conference on Computer Communications. IEEE, 1–9 25] NVIDIA. Accessed 2024. NVIDIA-NVML. https://docs.nvidia.com/deploy/nvml api/index.html. Available online 26] OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, et al. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL 27] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Brijesh Warrier Nithish Mahalingam, and Ricardo Bianchini. 2024. Characterizing Power Man agement Opportunities for LLMs in the Cloud. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 (ASPLOS ’24). Association for Computing Machinery New York, NY, USA, 207–222. https://doi.org/10.1145/3620666.3651329 28] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri Saeed Maleki, and Ricardo Bianchini. 2024 Splitwise: Efficient generative LLM inference using phase splitting. In ISCA. https://www.microsoft.com/en us/research/publication/splitwise-efficient-generative-llm-inference-using phase-splitting 29] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon Emissions and Large Neural Network Training. arXiv:2104.10350 [cs.LG 30] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Brad bury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2023. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems 5 2023), 606–624 31] PowerAPI. 2024. PyJoules: Python-based energy measurement library for various domains including NVIDIA GPUs. https://github.com/powerapi-ng/pyJoules Accessed: 2024-01-10 32] Ana Radovanović, Ross Koningstein, Ian Schneider, Bokan Chen, Alexandre Duarte, Binz Roy, Diyue Xiao, Maya Haridasan, Patrick Hung, Nick Care, et al 2022. Carbon-aware computing for datacenters. IEEE Transactions on Power Systems 38, 2 (2022), 1270–1280 33] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. 2022. DeepSpeed MoE: Advancing Mixture-of-Experts Inference and Training to Power Next Generation AI Scale. In Proceedings of the 39th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaud huri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato Eds.). PMLR, 18332–18346. https://proceedings.mlr.press/v162/rajbhandari22a html 34] Lavanya Ramapantulu, Bogdan Marius Tudor, Dumitrel Loghin, Trang Vu, and Yong Meng Teo. 2014. Modeling the energy efficiency of heterogeneous clusters In 2014 43rd International Conference on Parallel Processing. IEEE, 321–330 35] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. 2023. From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference. In 2023 IEEE High Performance Extreme Computing Conference (HPEC). 1–9. https://doi.org/10.1109/HPEC58863.2023.10363447 36] Satveer and Mahendra Singh Aswal. 2016. A comparative study of resource allocation strategies for a green cloud. In 2016 2nd International Conference on Next Generation Computing Technologies (NGCT). 621–625. https://doi.org/10 1109/NGCT.2016.7877487 37] Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo Goiri, and Josep Torrellas 2024. Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference. arXiv:2403.20306 [cs.AI 38] Xiaoyong Tang and Zhuojun Fu. 2020. CPU–GPU utilization aware energy efficient scheduling algorithm on heterogeneous computing systems. IEEE Access 8 (2020), 58948–58958 39] R. Taori, I. Gulrajani, T. Zhang, and et al. 2024. Stanford alpaca: An instruction following llama model. https://github.com/tatsu-lab/stanford_alpaca. Accessed 2024-01-15 40] Google Gemini Team. 2024. Gemini: A Family of Highly Capable Multimodal Models. arXiv:2312.11805 [cs.CL 41] Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS’17). Curran Associates Inc., Red Hook, NY, USA, 6000–6010 42] Yuxin Wang, Yuhan Chen, Zeyu Li, Zhenheng Tang, Rui Guo, Xin Wang, Qiang Wang, Amelie Chi Zhou, and Xiaowen Chu. 2024. Towards Efficient and Reliable LLM Serving: A Real-World Workload Study. arXiv:2401.17644 [cs.DC 43] Grant Wilkins, Srinivasan Keshav, and Richard Mortier. 2024. Hybrid Heteroge neous Clusters Can Lower the Energy Consumption of LLM Inference Workloads In Proceedings of the 15th ACM International Conference on Future and Sustainable Energy Systems (e-Energy ’24). Association for Computing Machinery, New York NY, USA, 506–513. https://doi.org/10.1145/3632775.3662830 44] Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: an insightful visual performance model for multicore architectures. Commun. ACM 52, 4 (apr 2009), 65–76. https://doi.org/10.1145/1498765.1498785 45] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, and et al. 2022. Sustainable ai: Environmental implications, challenges and opportunities. Proceedings of Machine Learning and Systems 4 (2022), 795–813 46] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019 HellaSwag: Can a Machine Really Finish Your Sentence? arXiv:1905.07830 [cs.CL 47] Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang You. 2023. Response Length Perception and Sequence Scheduling: An LLM Empowered LLM Inference Pipeline. In Thirty-seventh Conference on Neural In formation Processing Systems. https://openreview.net/forum?id=eW233GDOpm\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#import the Bert pretrained model from the transformers library\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "7xfiSGzV5uuA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "kaLWnvSYu3nQ",
        "outputId": "eab4e22c-88c1-4fb1-e77f-4e2334da4e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10800 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (10800) must match the size of tensor b (512) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1514720626.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtokenized_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mlast_hidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokenized_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mabstract_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    937\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"absolute\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10800) must match the size of tensor b (512) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#initialization of the dictionary of abstracts. Substitute this with the abstracts of the 10 papers considered as sources for RAG\n",
        "#(we could use functions to read the PDFs to \"cut\" the abstracts from the papers. For simplicity reasons, we will copy and paste them)\n",
        "\n",
        "\n",
        "#the text for rag is used as an input to the BERT model\n",
        "\n",
        "#The tokenized inputs are passed to the BERT model for processing.\n",
        "#(#remember padding=True: Ensures that all inputs are padded to the same length, allowing batch processing.)\n",
        "#The model outputs a tensor (last_hidden_state), where each input token is represented by a high-dimensional vector.\n",
        "#last_hidden_state is of shape (batch_size, sequence_length, hidden_size), where:\n",
        "#batch_size: Number of input texts.\n",
        "#sequence_length: Length of each tokenized text (after padding).\n",
        "#hidden_size: Dimensionality of the vector representation for each token (default 768 for bert-base-uncased).\n",
        "\n",
        "#last_hidden_state[:, 0]: Selects the representation of the [CLS] token for each input text. The [CLS] token is a special token added at the start of each input and is often used as the aggregate representation for the entire sequence.\n",
        "abstract_vectors = {}\n",
        "\n",
        "def chunking_text(text):\n",
        "    l\n",
        "\n",
        "for filename, abstract in abstracts_dict.items():\n",
        "\n",
        "    tokenized_inputs = tokenizer(abstract, return_tensors=\"pt\", padding=True)\n",
        "    print(tokenized_inputs)\n",
        "    with torch.no_grad():\n",
        "        last_hidden_state = model(**tokenized_inputs).last_hidden_state\n",
        "        abstract_vectors[filename] = last_hidden_state[:, 0].numpy()\n",
        "\n",
        "\n",
        "#abstract_vectors is a tensor of shape (batch_size, hidden_size) (e.g., (3, 768) in this case), representing each text as a single 768-dimensional vector.\n",
        "\n",
        "print(abstract_vectors.values()[0].shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOJBtgjUu3nQ"
      },
      "source": [
        "### Search\n",
        "\n",
        "With our text data vectorized and indexed, we can now perform searches. We will define a function to search the index for the most relevant documents based on a query.\n",
        "\n",
        "To perform the search, we need a function (search documents) where we perform the cosine similarity between the query vector and all the abstract vectors. This function will give our the top-k indexes. Once we find the top-k indexes, with another function, we can collect the full text of the documents from the paper dictionary.\n",
        "\n",
        "To compute cosine similarity, refer to the following formula\n",
        "\n",
        "```cs = cosine_similarity(vector_a.detach().numpy(), vector_b.detach().numpy())```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz1Axh_mu3nQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_top_k_similar_indices(query_vector, abstract_vectors, k):\n",
        "\n",
        "    #Computes the top k indices of the most similar abstracts to the query based on cosine similarity.\n",
        "\n",
        "    #Parameters:\n",
        "    #- query_vector: A tensor of shape (1, hidden_size) representing the query vector.\n",
        "    #- abstract_vectors: A tensor of shape (batch_size, hidden_size) representing the abstract vectors.\n",
        "    #- k: The number of top indices to return.\n",
        "\n",
        "    #Returns:\n",
        "    #- sorted_indices: A numpy array of shape (1, k) containing the indices of the top k most similar abstracts.\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def retrieve_documents(indices, documents_dict):\n",
        "\n",
        "    #Retrieves the documents corresponding to the given indices and concatenates them into a single string.\n",
        "\n",
        "    #Parameters:\n",
        "    #- indices: A numpy array or list of top-k indices of the most similar documents.\n",
        "    #- documents_dict: A dictionary where keys are document indices (integers) and values are the document texts (strings).\n",
        "\n",
        "    #Returns:\n",
        "    #- concatenated_documents: A string containing the concatenated texts of the retrieved documents.\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "\n",
        "#now I create a vector also for my query\n",
        "\n",
        "query = \"\"\n",
        "\n",
        "query_vector = \"\"\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1GdnrBCu3nQ"
      },
      "source": [
        "### A function to perform Retrieval Augmented Generation\n",
        "\n",
        "In this step, we’ll combine the context retrieved from our documents with LLAMA to generate responses. The context will provide the necessary information to the model to produce more accurate and relevant answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW2PR9Yyu3nR",
        "outputId": "c0f40d3d-4d88-4a23-e1f6-18521039a19e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#now we put it all together\n",
        "\n",
        "def generate_augmented_response(query, documents):\n",
        "\n",
        "    system = \"\"             #TODO: define system prompt\n",
        "\n",
        "    context = \"\"               #TODO: concatenate here all the search results\n",
        "\n",
        "\n",
        "    prompt = \"\"                 #TODO: create the prompt for LLAMA (system + context + query)\n",
        "\n",
        "    response = \"\"\n",
        "\n",
        "    #perform a query with LLAMA in the usual way\n",
        "\n",
        "    #return the response\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "# TODO: generate the queries!\n",
        "query = \"\"\n",
        "response = generate_augmented_response(query)\n",
        "print(response)\n",
        "\n",
        "#TODO: now compare the results with a prompt without RAG. What are the results?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6860ea42986f413c8a5b93527bff4fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eacdb2d59149466fade7a2d3fba63d67",
              "IPY_MODEL_8012e0d489a54bf79122b6cdb26b08e9",
              "IPY_MODEL_d027f2d320754793800c54a0cb423c37"
            ],
            "layout": "IPY_MODEL_eb686a80e527485f9c6fb29b52fe895e"
          }
        },
        "eacdb2d59149466fade7a2d3fba63d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3069f4cf4d34353b421ec054c356e3c",
            "placeholder": "​",
            "style": "IPY_MODEL_26fa74a01b5e4161b9131e91e2e131d1",
            "value": "config.json: 100%"
          }
        },
        "8012e0d489a54bf79122b6cdb26b08e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_536d443e03cf487c8a64d466111adef5",
            "max": 843,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a61ccc15ce24a04b8ca1282d7a5173e",
            "value": 843
          }
        },
        "d027f2d320754793800c54a0cb423c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a05c8e9689254c198ade062316f973dc",
            "placeholder": "​",
            "style": "IPY_MODEL_f4ba8faf11f8449a97da39555203944d",
            "value": " 843/843 [00:00&lt;00:00, 92.0kB/s]"
          }
        },
        "eb686a80e527485f9c6fb29b52fe895e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3069f4cf4d34353b421ec054c356e3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26fa74a01b5e4161b9131e91e2e131d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "536d443e03cf487c8a64d466111adef5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a61ccc15ce24a04b8ca1282d7a5173e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a05c8e9689254c198ade062316f973dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4ba8faf11f8449a97da39555203944d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffbf91d8e91c4c279183430209fd1bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9369d314368b4e16b2a68df3e0c4eb7f",
              "IPY_MODEL_fc001ba4a1954e2bb96c70a3aa71ce89",
              "IPY_MODEL_dca3fa0d7b5a485c8346a1ae45c06fe9"
            ],
            "layout": "IPY_MODEL_e7eba09b70d74f72b924e52f798fbc7f"
          }
        },
        "9369d314368b4e16b2a68df3e0c4eb7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c95287884cde46869063b050cf8fe756",
            "placeholder": "​",
            "style": "IPY_MODEL_42f8220280e740c4b48d7c0de4063bbf",
            "value": "model.safetensors: 100%"
          }
        },
        "fc001ba4a1954e2bb96c70a3aa71ce89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b879fdefefd247cc8dc609a1833382da",
            "max": 2471645608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdc282f8d7814007a193c2fc1e3d3920",
            "value": 2471645608
          }
        },
        "dca3fa0d7b5a485c8346a1ae45c06fe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06a812d974c24ed5aaef7427347ccbd3",
            "placeholder": "​",
            "style": "IPY_MODEL_3c2f840876234229b1b7be06d2323835",
            "value": " 2.47G/2.47G [00:30&lt;00:00, 210MB/s]"
          }
        },
        "e7eba09b70d74f72b924e52f798fbc7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c95287884cde46869063b050cf8fe756": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42f8220280e740c4b48d7c0de4063bbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b879fdefefd247cc8dc609a1833382da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdc282f8d7814007a193c2fc1e3d3920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06a812d974c24ed5aaef7427347ccbd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c2f840876234229b1b7be06d2323835": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2498c8a6fc694e3a941101cb9914ac5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdc9e721b69d4a7f82940773a1ec9bd4",
              "IPY_MODEL_8b2845493b60465eb74248ac0b843e62",
              "IPY_MODEL_883ca4415f274ca0ae47a8576bdc93fc"
            ],
            "layout": "IPY_MODEL_e83b81d3db0f4adab79daea9d2ab7af9"
          }
        },
        "cdc9e721b69d4a7f82940773a1ec9bd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_564c8eb9160e46afb895cb29778ce657",
            "placeholder": "​",
            "style": "IPY_MODEL_78370d2507124e20ab1b8e5d38d08d1b",
            "value": "generation_config.json: 100%"
          }
        },
        "8b2845493b60465eb74248ac0b843e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9411c360a26c4e91b8a8289b14549f08",
            "max": 185,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2023172a76614616b0c07be2355b6fb5",
            "value": 185
          }
        },
        "883ca4415f274ca0ae47a8576bdc93fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_274500f519a34775b461638a90001d60",
            "placeholder": "​",
            "style": "IPY_MODEL_3844a75cdbaf4d9c9310138611f49422",
            "value": " 185/185 [00:00&lt;00:00, 11.0kB/s]"
          }
        },
        "e83b81d3db0f4adab79daea9d2ab7af9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "564c8eb9160e46afb895cb29778ce657": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78370d2507124e20ab1b8e5d38d08d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9411c360a26c4e91b8a8289b14549f08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2023172a76614616b0c07be2355b6fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "274500f519a34775b461638a90001d60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3844a75cdbaf4d9c9310138611f49422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e564a1e26d3401a8113448ee064af9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a103b5c0cd34c81935722b3dc8f3f5a",
              "IPY_MODEL_219e191ab5ef4ba192b8dc4dbdfc05f6",
              "IPY_MODEL_f5ec43deac6b4051827acf46a8c4bc20"
            ],
            "layout": "IPY_MODEL_ae33c01e553d42d18e10f4a456962b7c"
          }
        },
        "9a103b5c0cd34c81935722b3dc8f3f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c795d4b9c84f4eab82d214bf71e6c615",
            "placeholder": "​",
            "style": "IPY_MODEL_3f05c13c04e344da86230acd72439996",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "219e191ab5ef4ba192b8dc4dbdfc05f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff14f785c1d7436cae5e76f95d51b1b9",
            "max": 50500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66ef8c549722466f8f09de9cae1f8ea5",
            "value": 50500
          }
        },
        "f5ec43deac6b4051827acf46a8c4bc20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0156862e6324271846af87976d97437",
            "placeholder": "​",
            "style": "IPY_MODEL_b5f723174cec4d8cbec2bd6e83f3d9b8",
            "value": " 50.5k/50.5k [00:00&lt;00:00, 5.24MB/s]"
          }
        },
        "ae33c01e553d42d18e10f4a456962b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c795d4b9c84f4eab82d214bf71e6c615": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f05c13c04e344da86230acd72439996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff14f785c1d7436cae5e76f95d51b1b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66ef8c549722466f8f09de9cae1f8ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0156862e6324271846af87976d97437": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5f723174cec4d8cbec2bd6e83f3d9b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68750819911c44e1bc078a5a6e63d6a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_380bac766a1e4d12aacb53565819dbba",
              "IPY_MODEL_77dc678e8aac47cd8be1d16df3585541",
              "IPY_MODEL_f285ac43b6c0464faa6b5bb0f3380b05"
            ],
            "layout": "IPY_MODEL_2551ceeac64742769ed649003e87df3f"
          }
        },
        "380bac766a1e4d12aacb53565819dbba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b9f04d03b4443ea914756a05c30b2af",
            "placeholder": "​",
            "style": "IPY_MODEL_6fa4b183ab2c4748a597a2169ce5d991",
            "value": "tokenizer.json: 100%"
          }
        },
        "77dc678e8aac47cd8be1d16df3585541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09b7e1a74db74a96948604401af8982a",
            "max": 9085657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ee9048a83eb49adb7e2d7b03f65725a",
            "value": 9085657
          }
        },
        "f285ac43b6c0464faa6b5bb0f3380b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ee494d3a4e04abe909cabc5752b0877",
            "placeholder": "​",
            "style": "IPY_MODEL_fd5fb4b5bd5d4e9eaf555fbbd8e8ac79",
            "value": " 9.09M/9.09M [00:00&lt;00:00, 10.6MB/s]"
          }
        },
        "2551ceeac64742769ed649003e87df3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b9f04d03b4443ea914756a05c30b2af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fa4b183ab2c4748a597a2169ce5d991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09b7e1a74db74a96948604401af8982a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ee9048a83eb49adb7e2d7b03f65725a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ee494d3a4e04abe909cabc5752b0877": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd5fb4b5bd5d4e9eaf555fbbd8e8ac79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a11e8e269f345788386a95c3dc26bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7071b56892284b6fbd4a1f6c0393132c",
              "IPY_MODEL_cc539511904d43a38c65484d3b45912a",
              "IPY_MODEL_111cb8d96e7c411ab41dc074580430a6"
            ],
            "layout": "IPY_MODEL_477950761c0a48e596d07b4bb9b1847a"
          }
        },
        "7071b56892284b6fbd4a1f6c0393132c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11b67898018245db9fd0d9625ea640da",
            "placeholder": "​",
            "style": "IPY_MODEL_b4332756b86643e88b4863cb76eef863",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "cc539511904d43a38c65484d3b45912a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f716ff3d5a324a03a722e2f3eecd807a",
            "max": 301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc54e1a5e2aa4c2d97e0363306329d21",
            "value": 301
          }
        },
        "111cb8d96e7c411ab41dc074580430a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10834c496b9b42539348efe0f1e1d53b",
            "placeholder": "​",
            "style": "IPY_MODEL_1a0c83f8df404dcba59b657c7edbf397",
            "value": " 301/301 [00:00&lt;00:00, 34.9kB/s]"
          }
        },
        "477950761c0a48e596d07b4bb9b1847a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b67898018245db9fd0d9625ea640da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4332756b86643e88b4863cb76eef863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f716ff3d5a324a03a722e2f3eecd807a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc54e1a5e2aa4c2d97e0363306329d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10834c496b9b42539348efe0f1e1d53b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a0c83f8df404dcba59b657c7edbf397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}